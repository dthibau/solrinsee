{
    "id": 112,
    "titre": "Deep Learning for Search",
    "sujet" : "The recent success of deep learning to solve computer vision and natural language
    processing tasks has been impressive, and we are seeing deep neural networks being applied
    more and more in different contexts with beautiful results. One of those contexts is search
    engines, often are our primary interface with computers through web search or e-commerce.
    In this book I teach how to apply deep learning techniques to help users turn data into
    knowledge more effectively in search engines.",
    "auteur": [
        "Tomasso Teofili"
    ],
    "date": "2018-01-01T00:00:00Z",
    "mots-clefs" : [
        "IA",
        "Full-text"
    ], 
    "content_type": "application/pdf",
    "nb_pages": 244,
    "language": "en",
    "content" : "MEAP Edition
    Manning Early Access Program
    Deep Learning for Search
    Version 8
    Copyright 2018 Manning Publications
    For more information on this and other Manning titles go to
    www.manning.com
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>welcome
    Thank you for purchasing the MEAP edition of Deep Learning for Search.
    The recent success of deep learning to solve computer vision and natural language
    processing tasks has been impressive, and we are seeing deep neural networks being applied
    more and more in different contexts with beautiful results. One of those contexts is search
    engines, often are our primary interface with computers through web search or e-commerce.
    In this book I teach how to apply deep learning techniques to help users turn data into
    knowledge more effectively in search engines.
    When I first started experimenting on neural networks back in 2011 it was 'just' a
    fascinating journey into trying to understand something that looked magical. Studying,
    learning, and taking on challenges was so engaging that, after a while, I decided to try to test
    those techniques to solve real problems with search engines in my professional life. It turned
    out the results were very good and very interesting. So I decided to share what I've learned
    in the hope that my excitement for this topic reaches you. And, most important, I want this
    book to make the lives of your search engine's users better!
    The book is divided into three parts. Part 1 guides you through the fundamentals of
    search and deep learning, so if you have never read anything about neural networks or
    search engines, these chapters should give you enough introduction to handle the more
    advanced stuff later in the book. Part 2 dives into typical search tasks, such as relevance,
    which can be improved immensely with the use of deep neural networks. In part 3, we'll
    solve more advanced problems, like searching through images or translating user queries
    with the help of deep learning.
    Please let me know your thoughts on what's been written so far and what you'd like to see
    in the rest of the book. I'm not a native English speaker (I'm Italian by the way) so my
    sentences are not perfect yet—but they will be! Your feedback will be invaluable and
    greatly appreciated as I continue to write and improve Deep Learning for Search.
    Thanks again for your interest and for purchasing the MEAP!
    —Tommaso Teofili
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>brief contents
    PART 1: SEARCH MEETS DEEP LEARNING
    1 Neural search
    2 Generating synonyms
    PART 2: THROWING NEURAL NETS AT YOUR SEARCH ENGINE
    3 From plain retrieval to text generation
    4 More sensitive suggestions
    5 Word embeddings based ranking
    6 Document embeddings for ranking and recommendation
    PART 3: ONE STEP BEYOND
    7 Searching across languages
    8 Content based image search
    9 Peaking into performance
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>1
    1
    Neural search
    Suppose you want to learn something about the latest research breakthroughs in
    artificial intelligence; what do you usually do to find it? How much time and work would
    it take to get the information that you are looking for? If we were in a (huge) library we
    could ask the librarian what books exist on the topic and he/she would probably point us
    to a few books he/she knows from various shelves. Ideally, the librarian would suggest
    particular chapters we should browse from each book. That sounds easy enough.
    However, the librarian generally comes from a different context than you do, meaning
    you and the librarian may have different opinions of what’s significant or not. The library
    itself could have books in various languages, or the librarian himself could be speaking a
    different language. His/her information about the topic could be outdated, given that
    latest is a fairly relative point in time, and you don’t know when the librarian last read
    anything about artificial intelligence, or if the library regularly receives publications in
    the field. Additionally he/she may not understand your inquiry properly. The librarian
    may think you’re talking about intelligence from the psychology perspective 1) requiring
    a few iterations back and forth before you understand one another and get to the pieces of
    information you need. Then, after all this, you could discover the library doesn’t have the
    book you need, or the information may be spread among several books and you need to
    read them all. Exhausting!
    Footnote 1mthis happened to me in real life
    Unless you’re a librarian yourself, that’s what often happens nowadays when you
    search for something on the Internet too. Although we can think of the Internet as a
    single huge library, there are actually many different librarians out there to help you find
    the information you need: the search engines. Some search engines are experts in certain
    topics, some know only a subset of a library, or even just a single book. Now imagine
    that someone, let’s call him Robbie, who already knows enough information about both
    the library and its visitors, could help you communicate with the librarian in order to
    better find what you’re looking for. That would surely help you get to your answers
    faster. Robbie could help the librarian understand a visitor’s inquiry by providing
    additional context, e.g. Robbie knows what the visitor usually reads about and skips all
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>2
    books about psychology. Also, having read a lot of the books in the library, Robbie could
    have a better insight into what’s more important in the field of artificial intelligence. It’d
    be nice if we could have such 'advisors' like Robbie to help search engines work better
    and users get more useful information faster.
    This book is about making use of the techniques from a machine learning field called
    deep learning in order to build models and algorithms that can influence the behavior of
    search engines, to make them more effective. Deep learning algorithms will play the role
    of Robbie, helping out the search engine to provide better search experience and more
    precise answers to the end users.
    Figure 1.1 Artificial intelligence, machine learning, deep learning
    You are not expected to know what deep learning is or how it generally works at this
    point; we’ll get to know it by looking into some specific algorithms one by one as they
    become useful for solving particular types of search problems. For now we’ll just
    mention that deep learning is a field of machine learning where computers are capable of
    learning to represent and recognize 'things' incrementally, by using 'deep' neural
    networks, deep refers to the fact that such networks are deep because they have possibly
    many hidden layers.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>3
    Figure 1.2 A deep feed forward neural network with 2 hidden layers
    So DL is a subfield of machine learning, which is part of the broader area of artificial
    intelligence. But what is machine learning?
    Machine learning is an automated approach to solving problems based on algorithms
    that can learn optimal solutions from both data and experience. During a training (or
    learning) phase, a ML algorithm looks at the data in order to find recurring patterns or
    learn what is a good solution for a certain input. Once training has completed, what the
    algorithm has learned is used on unseen data (from similar or the same domain), to
    perform accurate predictions. In practice this means that we have machines that can learn
    to perform predictions on certain tasks based on what they’ve been trained for; for
    example a machine learning algorithm can learn to predict stock market prices based on
    previous stock market fluctuations data. We can ìprovide data about previous stock
    market prices along time for one or more categories of goods and then ask the machine
    learning algorithm how it predicts such market will behave in the short term.
    Machine learning can be divided into supervised and unsupervised learning. In the
    former, a set of training data (or experience) of the form input correct output is given to
    the learning algorithm. On the other hand, in unsupervised learning, the algorithm just
    looks at the raw data (with no information about any expected output) and extracts useful
    patterns and data representations. Supervised and unsupervised methods can be used to
    solve either the same problem or different problems, however in both cases you first need
    to define the problem you want to tackle and what you expect your machine learning
    method to use to solve it. Let’s take the stock market prediction example, we may be
    interested in predicting the price of the stock from a certain company given the previous
    stock price history of the same company and the price of competitors' stocks. The data
    for such task may look like:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>4
    Listing .1 Data for stock price prediction
    date
    | TSMP |
    CSMPs
    ======================================
    2017/06/01 | 150 | 100,120,88,302,10
    2017/06/02 | 148 | 120,110,87,308,3
    The date column would define the point in time in which stock prices reached the
    give values. the TSMP column would contain the value for the Target Stock Market Price
    , that is the price of the stock we want to predict (for a specific target company), the
    CSMPs column would contain stock prices of other companies identified as competitors.
    We could use such data in many different ways: for example we could use the
    competitors prices and the date as input and the target company stock market price as the
    desired output, setting a supervised learning task, with the goal of learning to predict the
    stock prices of a certain company. In real life such a tool could be used to decide when to
    buy or sell stocks from that particular company.
    Alternatively if we have stock prices per company in a table like the one below we
    could instead use an unsupervised machine learning algorithms to predict which
    companies have similar trends.
    Listing .2 Per company stock prices
    date
    | FOO | BAR | LOR | EMI | PSU | MSI
    ===============================================
    2017/06/01 | 150 | 100 | 120 | 88 | 302 | 10
    2017/06/02 | 148 | 120 | 110 | 87 | 308 | 3
    In such scenario, we would not be using any of the values as the desired 'target'
    output, all of them will be just used as inputs to a learning algorithm. In real life having
    such information could be useful to group together similar companies by their stock
    fluctuations, this for example could be useful to see whether your company fluctuate
    together with wealthy companies or more with troubled ones …
    In both supervised and unsupervised scenarios the output of the training phase is a
    model: you can think it as the artifact that captures what the algorithm has learned and is
    then used to perform predictions, like a magic black box.
    Such black boxes are not magic, what looks surprising is how good the results can be
    when using them. Machine learning can help solve a lot of problems, like learning to
    predict stock prices, classify emails as spam or not, grouping similar objects (text,
    images) together, recognize speech and many others. Until early 2000s several different
    techniques were in order to achieve good results when trying to solve such tasks, until
    deep learning became mainstream not just in research labs of universities, but also in the
    industry. The reason for the success of DL is that a lot of machine learning problems got
    better solved with deep learning, in practice this means better spam filters, more accurate
    image search, etc.
    Deep learning algorithms are based on the usage of deep artificial neural networks
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>5
    (ANNs), a set of computational approaches inspired by the way the brain is organized
    into graphs of neurons (although the brain is much more complex than an artificial neural
    network).
    Figure 1.3 An artificial neuron (green) with 3 inputs and 1 output
    We will get to know how this neural networks are composed, how they work and how
    they can be used in practice throughout the book, in the context of search engines.
    Because of that we call Neural Search the field where search problems are solved with
    the help of deep artificial neural networks.
    SIDEBAR
    Neural search
    The term neural search is a less academic form of the term neural
    information retrieval which first appeared during a research workshop at
    SIGIR 2016 conference 2 focused on applying deep neural networks to the
    field of information retrieval.
    Footnote 2mwww.microsoft.com/en-us/research/event/neuir2016/
    You might be wondering why we need neural search: after all, we already have some
    very good search engines on the web, and we often manage to find what we need. So
    what’s the value proposition of neural search ?
    Deep neural networks are very good at :
    providing a representation of textual data that captures word and document semantics,
    allowing a machine to say which words and documents are semantically similar
    generating text that is meaningful in a certain context, that is useful, for example, for
    creating chatbots
    providing a representation of images that doesn’t simply pertain to the pixels but rather to
    their composing objects; this allows you to build very efficient face / object recognition
    systems
    performing machine translation efficiently
    much more: under certain assumptions, they can approximate any function, this means
    that the there’s no theoretical limit to what kind of tasks deep neural networks can
    achieve
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>6
    Now this might sound a bit abstract, so let’s look at how these capabilities turn out to
    be useful for you as a search engineer and / or as a user. If you have ever used a web
    search engine, think of the major struggle points when searching, most likely the you’ll
    be thinking things like:
    I didn’t get good results: I found somewhat related documents but not the one I was
    looking for
    It took me too much time to find the information I was looking for (and then I gave up)
    I had to read through some of the provided results before getting a good understanding of
    the topic I wanted to learn about
    I was looking for a content in my native language, but I could find good search results
    only in English
    I was looking for a certain image I had once seen on a website, but I couldn’t find it
    anymore
    Such problems are quite common and various different solutions exist to mitigate
    each one of them. But the exciting thing is that deep neural networks, if tailored properly,
    can help in all of these cases.
    1.1 What deep learning can do for search
    With the help of deep learning algorithms the search engine will be able to:
    provide more relevant results to its end users, so that the users will be more satisfied by
    the results' quality
    search through binary contents like images the same way we search over text; think of
    this as being able to search for an image with the phrase 'picture of a leopard hunting an
    impala' (and you’re not Google)
    serve content to users speaking in different languages, allowing more users to access the
    data in the search system
    generally become more sensitive to the data it serves, that means less chance to have
    queries which give no results
    If you’ve ever worked on designing, implementing or configuring a search engine,
    you will have surely faced the problem of having a solution that adapts to your data; deep
    learning will help a lot in providing solutions to these problems that are accurately based
    on the data, not on fixed rules or algorithms.
    The quality of search results is crucial for end users, there’s one single thing a search
    engine should do very well, that would be finding out which of the possibly matching
    search results would be most useful for a specific user’s information need. Well-ranked
    search results allow users to find the important results easier and faster; that’s why we
    put a lot of emphasis on the topic of relevant results. In real life this can make a huge
    difference, in fact according to a article published on Forbes magazie 'By providing
    better search results, Netflix estimates that it is avoiding canceled subscriptions that
    would reduce its revenue by $1B annually.' 3
    Footnote 3m
    www.forbes.com/sites/louiscolumbus/2017/07/09/mckinseys-state-of-machine-learning-and-ai-2017/#27fe02c375b6
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>7
    Deep neural networks can help by automatically tweaking the end user query under
    the hood based on past user queries or based on the search engine contents.
    People today are used to work with web search engines to retrieve images. If you
    search for 'pictures of angry lions' on Google, for instance, you’ll get strongly relevant
    images. Before the advent of deep learning, such images had to be decorated with
    metadata (data about data) describing their contents before being put into the search
    engine. And that metadata usually had to be typed by a human. Deep neural networks can
    abstract a representation of an image which captures what’s in there so that no human
    intervention is required to put an image description in the search engine.
    For scenarios like web search (searching over all the websites over the internet) users
    can come from all over the world, so it’s best if they can search in their own native
    languages. Additionally, the search engine could pick user profiles and return them
    results in their own language even if they search in English; this is a very common
    scenario for tech queries, because lots of content is produced in English. A very
    interesting application of deep neural networks is called neural machine translation, a set
    of techniques that leverage deep neural networks to translate a piece of text from a source
    language into another target language.
    Also exciting is the possibility of using deep neural networks to let the search engine
    generate information instead of 'just' retrieving search results 4. You could even
    aggregate all the above ideas and build a search engine serving both text and images
    seamlessly to users from all over the world which, instead of returning search results,
    returns one single piece of text or image you need.
    Footnote 4mDeep Learning Relevance: Creating Relevant Information (as opposed to retrieving it), see
    arxiv.org/pdf/1606.07660.pdf
    The above applications are all examples of what we call neural search, and, as you
    can imagine, they have the potential to revolutionize the way we work and use search
    engines today.
    To take advantage of all the potential of deep neural networks, people interested in
    computer science, especially in the fields of natural language processing, computer
    vision, informational retrieval, will need to know how such artificial neural networks
    work in practice.
    The goal of this book is to enable you readers to use deep learning in the context of
    search engines by teaching some deep learning algorithms applied to search problems,
    even if you’re not going to build the next Google search, you should be able to learn
    enough to use DL techniques also within small / medium sized search engines to provide
    a better experience to your users.
    As you read through the book you’ll learn a lot of deep learning and the required
    search fundamentals, so if you’re a search engineer or a programmer willing to learn
    neural search, this book is for you.
    We’ll run our neural search examples on top of open source software written in Java
    with the help of Apache Lucene 5, an information retrieval library, and Deeplearning4j 6,
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>8
    a deep learning library. However we’ll focus as much as possible on the principles rather
    than on the implementation in order to make sure the techniques explained in this book
    can be applied in with different technologies and / or scenarios too. At the time of
    writing, Deeplearning4j is a widely used framework for deep learning in the enterprise
    communities, it is part of the Eclipse foundation. It also has a good adoption because of
    integrations with popular big data frameworks like Apache Spark. Other deep learning
    frameworks (a lot!) exist though; for example TensorFlow (from Google) is very popular
    among the Python and research communities. Almost every day new tools get invented,
    so we decided to focus on a relatively easy-to-use DL framework that can be easily
    integrated with Lucene, which is one of the most widely adopted search libraries for the
    JVM.
    Footnote 5mlucene.apache.org
    Footnote 6mdeeplearning4j.org
    While planning this book, we decided to present chapters in a kind of ascending level
    of difficulty, so each chapter will teach a certain application of neural networks to search
    problems, supported by a well-known algorithm. This means that while we’ll try to keep
    an eye on the state-of-the-art deep learning algorithms, we’ll also be quietly conscious
    that we cannot cover everything. The aim is to provide good baselines that can be easily
    extended if a new and better neural network based algorithm comes out next week. Key
    things we’ll improve with the help of deep neural networks are: relevance, text
    generation, image search, machine translation, document recommendation. Don’t worry
    if you know none of them, we’ll introduce such tasks as they exist without any deep
    learning technique and then discover when and how DL can help.
    We’ll try to provide also some considerations about accuracy and how to measure the
    final results when we apply neural search. In fact without numbers constantly
    demonstrating what we think is good, we won’t go very far. We need to measure how
    good our systems are, with and without our fancy neural nets…
    In this first chapter, we’ll have a look at the problems search engines try to solve and,
    correspondingly, the most common techniques used. This will allow you to learn the
    basics of how text is analyzed, ingested, and retrieved within a search engine, so that
    you’ll get to know how queries hit search results as well as some fundamentals on
    solving the problem of returning the relevant results first. We’ll also uncover the
    weaknesses inherent to common search techniques. This will set up the basis for the
    discussion on what deep learning can be used for in the context of search. We’ll then
    have a look at which tasks deep learning can help to solve and what are the practical
    implications of its applications in the search field, so that we can get to understand what
    we can expect and also what we can’t expect from neural search in real life scenarios.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>9
    1.2 Retrieving useful information
    In this section we’ll see how to retrieve search results that are relevant to users' needs.
    This will give us the search fundamentals we need to start looking into how deep neural
    networks can help us build innovative search platforms.
    First question: what is a search engine ? It is a system, a program running on a
    computer, that people can use to retrieve information. The main value of a search engine
    is that while it ingests 'data' it is expected to provide 'information', this means that the
    search engine should try to its best to make sense of the data it gets in order to provide
    something that can easily consumed by its users. As users we rarely need lots of data
    about a certain topic, we are often looking for a specific piece of information, probably
    we would just need one answer, not hundreds or thousands of results to inspect.
    When it comes to search engines we probably tend to think to Google, Bing, Baidu,
    etc. as they’re among the biggest and most widely used ones because they provide access
    to huge amounts of information coming from a lot of diverse sources. However some
    smaller search engines focused on contents from a specific domain or topic exist out
    there. These are often called vertical search engines because they only work on a
    constrained set of document types or topics of the whole set of contents that is online
    nowadays. They have a great importance too as they are often able to provide more more
    precise results on 'their' data as they have been taylored on those specific contents. This
    usually allows us to retrieve more fine grained results with a higher accuracy (think of
    searching for an academic article on Google vs using Google Scholar). For now we won’t
    go into the details of what accuracy means: in this case we are talking about the general
    concept of accuracy of an answer to an inquiry, however accuracy is also the name of a
    well defined measure used to evaluate how good and precise an information retrieval
    system results are. We’ll make no distinction at this point on the sizes of data and user
    base as all of the concepts that follow apply to most of the existing search engines.
    Key responsibilities of a search engine usually involve:
    indexing: ingesting and storing data efficiently so that it can be retrieved fast
    querying: providing a retrieval functionality so that search can be performed by an end
    user
    ranking: presenting and ranking the results according to certain metrics as to best satisfy
    users' information needs
    A key point in practice is also efficiency, if it takes us too much time to get the
    information we are looking for, it’s likely we’ll switch to another search engine next
    time.
    But how does a search engine work with pages, books and similar kinds of text they
    are used for? In the next sections we’ll get to know:
    how big chunks of text are split into smaller pieces for the search engine to be able to
    take a given query and quickly retrieve a document
    basics of how to capture the importance and relevance of search results, for a particular
    query
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>10
    Let’s start with the fundamentals of information retrieval (indexing, querying and
    ranking). Before diving into that we’ll have to understand how big streams of texts end
    up in a search engine, that is very important because this topic has an impact on the
    search engine capabilities of searching fast and provide sensitive results.
    1.2.1 Text, tokens, terms and search fundamentals
    If you put yourself in the librarian’s shoes receiving an inquiry for books related to a
    certain topic, how would you be able to say that one book contains information about a
    certain topic ? How would you know if a book contains a certain word ?
    Extracting the categories a certain book belongs to (high level topics like artificial
    intelligence, deep learning, etc.) is different than extracting all the words contained in the
    book. For example, categories make searching for a book about artificial intelligence
    easier for a newbie because no prior knowledge of AI specific techniques or authors is
    required, a user will just go to the search engine website and start browsing between the
    existing categories and look for something that is close enough to the topic of artificial
    intelligence.
    On the other hand for an AI expert knowing whether a book contains the words
    'gradient descent' or 'backpropagation' allows finding results which contain finer
    grained information about certain techniques or problems in the field of artificial
    intelligence.
    Humans generally have a very hard time in remembering all the words contained in
    book, although we can quite easily say a book’s topic by reading a few paragraphs from
    the book or even from looking at the preface or foreword. Computers tend to behave the
    opposite instead: they can easily store large amounts of text and 'remember' all the
    words contained in millions of pages so that they can be used while searching, on the
    other hand they are not so good at extracting information that is implicit, scattered or not
    directly formulated in a given piece of text, like which category a book belongs to. For
    example a book about neural networks may never mention 'artificial intelligence' (while
    it would probably refer to machine learning) however it would still belong to the broad
    category of books about artificial intelligence.
    Let’s first have a look at the task computers can do well already: extracting and
    storing text fragments (also known as terms) from streams of text. You can think of this
    process, called text analysis, as breaking down the text of a book into all its constituent
    words. You can imagine to have a tape where the contents of a book are written in a
    stream one after the other and have a machine (the text analysis algorithm) where you
    insert such tape as input and you receive many pieces of such tape as output, each of
    those output pieces will contain a word, or a sentence or a noun phrase (e.g. artificial
    intelligence); you may realize that some of the words that were written on the input tape
    have been eaten by the machine and not outputted in any form.
    Since the final units to be created by the text analysis algorithm may be words but
    also group of words / sentences or even portions of words, we generally refer such
    fragments as terms. You can think of a term as the fundamental unit that a search engine
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>11
    uses to store the data and, consequently, retrieve it.
    That’s the basis of one of the most fundamental forms of search, keyword search: a
    user types a set of words and expects the search engine to return all the documents that
    contain some or all of such terms. This is how web search started tens of years ago and,
    despite many search engines nowadays are much 'smarter', many users keep composing
    queries based on keywords they expect search results to contain. In fact we’re about to
    learn how the text entered by a user into a search box makes the search engines return
    some results. We call a query this text the user entered to search for something; although
    it’s just text, it conveys and encodes what the user needs and how such user manages to
    turn its possibly abstract need (e.g. I want to learn what artificial intelligence is) in a way
    that is concise but still enough descriptive of its need (e.g. books about artificial
    intelligence).
    So if, as a user, you want to find documents which contain the word 'search' how
    would the search engine return such documents?
    A not-so-smart way of doing that could be to go over each document’s content from
    the beginning and scan it until they find a match. However it would be very expensive to
    perform such text scans for each query, especially with many large documents; in fact:
    a lot of documents may not contain the word 'search' and therefore it would be a waste of
    computation resources to scan through them
    even for the documents containing the word 'search', such word may occur towards the
    end of the document, requiring the search engine to 'read' through all the preceding
    words before finding a match for 'search'
    We have a match or a hit when one or more terms that are part of a query are found in
    a search result.
    We need to find a way to make this retrieval phase get computed fast. One
    fundamental method to accomplish that is to break down sentences like 'I like search
    engines' into smaller units, in this case ['I', 'like', 'search', 'engines'], this is a
    requisite for an efficient storage mechanism called inverted indexes, which we’ll cover in
    the next section. A text analysis program is often organized as a pipeline, a chain of
    components each of which takes the previous component’s output as its input. Such
    pipelines are usually composed by building blocks of two types:
    tokenizers: components that break a stream of text up into words, phrases, symbols, or
    other units called tokens
    token filters: components that accept a stream of tokens (from a tokenizer or another
    filter) and can modify, delete or add new tokens
    The output of such text analysis pipelines is a sequence of consecutive terms, like in
    the image below.
    Figure 1.4 Getting the words of 'I like search engines' using a simple text analysis
    pipeline
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>12
    We have learned that text analysis is useful for performance reason to build fast
    search engines, however another equally important aspect is that they control how
    queries and the text to be put into the index match. Often they are used to filter some
    tokens that are not considered useful or needed for the search engine. For example, a
    common practice is to avoid storing very common terms like articles or prepositions into
    the search engine, because those words exist in most of text documents in languages like
    English and you usually don’t want a query to return everything you have in the search
    engine, because that would not give much value to the user. In such cases you can create
    a token filter responsible for removing tokens like 'the', 'a', 'an', 'of', 'in', etc. while letting
    all the other tokens flow out, as the tokenizer produces them. In this, simplistic, example:
    the tokenizer will split tokens every time it encounters a whitespace character
    the token filter will remove tokens that match a certain blacklist (also known as stopword
    list)
    In real life it’s very common, especially when setting up a search engine for the first
    time, to build several different text analysis algorithms and try them out on the data we
    want to put into the search engine. This allows us to visualize how content will be
    handled by such algorithms, like which tokens get generated, which ones get eventually
    filtered out, etc.. We’ve built our first text analysis chain (also called analyzer) and want
    to make sure it works as expected and filters articles, prepositions, etc. Let’s try to pass a
    first piece of text to our simplistic analyzer and submit the sentence the brown fox
    jumped over the lazy dog to our pipeline; we expect articles to be removed. The
    generated output stream will look like the graph below:
    Figure 1.5 The traversed token graph
    The resulting token stream has the tokens 'the' removed, as expected; you can see that
    from the dotted arrows at the start of the graph and between nodes over and lazy. The
    numbers besides the tokens represent the start and ending positions (in number of
    characters) of each token. The important bit of the above example is that a query for 'the'
    won’t match, because our analyzer has removed all such tokens and therefore they won’t
    end up being part of the search engine contents. In real life text analysis pipelines are
    often more complex, we will see some of them in the next chapters. Now that we have
    learned about text analysis we’ll see how search engines store the text (and terms) to be
    queried by end users.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>13
    INDEXING
    While the search engine needs to split text into terms for the sake of fast retrieval, end
    users expect search results to be in the form of a 'single unit', a document. Think to the
    search results from Google, if you search for 'books', you will receive a list of results,
    each composed by a title, a link, a text snippet of the result, etc. Each of those results
    contains the term 'books' but what is shown is a document which has lot more
    information and context than only the text snippet where the term matched. In practice
    tokens resulting from text analysis are stored with a reference to the original piece of text
    they belong to.
    In fact this link between a term and a document makes it possible to:
    match a keyword / search term from a query
    return the referenced original text as a search result
    This whole process of analyzing the streams of text and storing the resulting terms
    (along with their referenced documents) in the search engine is usually referred to as
    indexing.
    The reason for such wording stands in the fact that terms are stored inside an inverted
    index, a data structure which maps a term into the text which originally contained it.
    Probably the easiest way to look at it is as the analytic index of an actual book, which
    points each word to the pages where it’s mentioned, in the case of the search engine the
    words are the terms and the pages are the original pieces of text.
    From now on we’ll refer to the pieces of text to be indexed (pages, books) as
    documents. In order to visualize how documents end up after being indexed let’s assume
    we have the following two very similar documents:
    1. the brown fox jumped over the lazy dog (document 1)
    2. a quick brown fox jumps over the lazy dog (document 2)
    Assuming we use the text analysis algorithm defined above (white space tokenization
    with stop words 'a', 'an', 'the') the below table shows a good approximation of an inverted
    index containing such documents.
    Table 1.1mInverted index table
    TermDOC Ids
    brown1,2
    fox1,2
    jumped1
    over1,2
    lazy1,2
    quick2
    jumps1
    As you can see there’s no entry for the term 'the' because the stopwords based token
    filter has removed such tokens. In the above table you can find the dictionary of terms in
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>14
    the first column and a posting list, a set of document identifiers, associated with each
    term for each row. With inverted indexes retrieval of documents that contain a given term
    is very fast: the search engine picks the inverted index, looks for an entry for the search
    term and eventually retrieves the documents contained in the posting list. With the above
    index, if you search for the term 'quick', the inverted index will return document no. 2 by
    looking into the posting list corresponding to the term 'quick'. We’ve just gone through a
    quick example of indexing text into a search engine.
    Let’s think of the task of indexing books; books are composed by pages, their core
    content, but they also have a title, an author, an editor, a publication year, etc. We cannot
    use the same text analysis pipeline for everything, you would not want to remove 'the' or
    'an' from a book title. A user knowing a book’s title should be able to find it by exact
    matching it! If the text analysis chain removes in from the book title Tika in action, a
    query for 'Tika in action' won’t find it. On the other hand you may want to avoid
    keeping such tokens for the book contents so you have a text analysis pipeline that is
    more aggressive in filtering unwanted terms. If the text analysis chain removes in and the
    from the book inception Living in the information age it should not be problematic
    because it’s very unlikely that a user will search for 'Living in the information age',
    while he / she may search for 'information age.' In this case, there’s no or very little loss
    of information there, but we get the benefits of having to store smaller texts and, more
    important, improving relevance (we’ll talk about this in the next section). A very
    common approach in real life is to have multiple inverted indexes that address indexing
    of different parts of a document, all within the same search engine.
    SEARCHING
    Now that we have some content indexed in our search engine, we will look into
    searching. Historically the first search engines allowed users to search with specific
    terms, also known as keywords, and eventually some boolean operators which let the
    users determine which terms must match, must not match or can match in the search
    results. Most commonly a term in a query should match, but is not mandatory, if you
    want search results which must contain such a term you must add the relevant operator,
    e.g. a + in front of the term. A query like +deep learning for search_ would require
    results that both contain _deep_ and _learning_ and optionally contain _for_ and
    _search_. It's very common also to allow users to specify they need entire phrases to
    match, instead of single terms. That allows users to search for an exact sequence of
    words instead of single terms. The previous query could be rephrased in _'deep
    learning' for search, in order to return search results that must contain the sequence deep
    learning and optionally the terms for and search.
    It may sound surprising but text analysis is also very important during the search, or
    retrieval, phase. Suppose you want to search for our book Deep learning for search on
    top of the data we just indexed; assuming we have a web interface, you would probably
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>15
    type a query like 'deep learning for search'. The challenge for us in this retrieval phase
    is to make it possible to retrieve the right book. The first thing that sits between a user
    and a classic search engine UI is a query parser.
    A query parser is responsible for transforming the text of the search query entered by
    the used in a set of clauses which indicate which terms the search engine should look for
    and how to use them when looking for a match in the inverted indexes. In the previous
    query examples the query parser would be responsible for making sense of the + and '
    symbols. Another widespread syntax allows you to put boolean operators among query
    terms: deep AND learning. In this case, the query parser will give a special meaning to
    the AND operator: terms at the left and right of it are mandatory. A query parser can be
    thought as a function that takes some text and outputs a set of constraints to apply to the
    underlying inverted index(es) in order to find results.
    During indexing a text analysis pipeline is used to split the input text into terms to be
    stored in the index; this is also called index time text analysis. Similarly text analysis can
    be applied during search on the query in order to break the query string into terms; this is
    therefore called search time text analysis.
    A document is retrieved by the search engine when the search time terms match a
    term in the inverted index referenced by that doc.
    Figure 1.6 Index, search time analysis and term matching
    The diagram above shows an index time analysis on the left, which is used to split a
    document text into terms. These end up in the index, all referencing doc 1. The index
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>16
    time analysis is composed by a whitespace tokenizer and two token filters: the former is
    used to remove unwanted stopwords (like the) while the latter is used to convert all the
    terms into lowercase (e.g. Fox gets converted to fox). On the top right, the query lazy
    foxes is passed to the search time analysis which splits tokens using a whitespace
    tokenizer but filters using a lowercase filter and a stemming filter. A stemming filter
    transforms terms by reducing inflected or derived words into their root form, this means
    removing plural suffixes, ing form in verbs, etc., in this case foxes is transformed into fox
    .
    A common way to verify that indexing and search text analysis pipelines work as
    expected is to follow these steps:
    take sample content
    pass the content to the index time text analysis chain
    take sample query
    pass the query to the search time text analysis chain
    check the produced terms match
    For example, it’s common to have stop word filters at indexing time because
    performing the filtering then won’t have any performance impact on the retrieval phase.
    However it may be possible to have other filters within either the indexing or search
    phases. With index and search time text analysis chains and query parsing in place, we
    can have a look at how the process of retrieving search results works.
    We’ve learned one of the basic techniques at the core of every search engine: text
    analysis (tokenization and filtering) allows the system to break down text into the terms
    we expect our users to type at query time and place them into a data structure called an
    inverted index, which allows efficient storage (space wise) and retrieval (time wise).
    As users, however, we do not want to look into all the search results so we need the
    search engine to tell us which ones are supposed to be the best. Now, you could be
    wondering: what does the best mean? Is there a measure of how good a piece of
    information is, given our queries? The answer is yes, we call such a measure relevance.
    Ranking search results in an accurate way is one of the most important tasks a search
    engine has to accomplish. In the next section we’ll have a brief look at how to address the
    problem of relevance.
    1.2.2 Relevance first
    You now know how search engines retrieve a document, given a query. In this section
    you’ll learn how search engines rank the search results so that the most important results
    are returned first. This will give you a solid understanding of how common search
    engines work.
    Relevance is a key concept in search; it’s a measure of how much important a
    document is with respect to a certain search query. As humans it’s often easy for us to tell
    why certain documents are more relevant than others with respect to a query. So, in
    theory, we could even try to extract a set of rules to represent our knowledge about
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>17
    ranking the importance of a document. But in practice such an exercise would probably
    fail because:
    the amount of information we have doesn’t allow us to extract a set of rules that is
    applicable to most of the documents
    documents in the search engine change a lot over time and it’s a huge effort to keep
    adjusting the rules accordingly
    documents in the search engine can belong to very diverse domains (e.g. in web search)
    and it’s not possible to find a good set of rules which works for all types of information
    One of the central themes in the field of information retrieval is to define a model that
    doesn’t require a search engineer to extract such rules. Such a retrieval model should
    capture the notion of relevance as accurately as possible. Given a set of search results, a
    retrieval model will rank each of them: the more relevant the result, the higher its score.
    Most of the time, as a search engineer, you simply won’t get perfect results by just
    choosing a retrieval model; relevance is a capricious beast! In real life scenarios, you
    may have to continuously adjust your text analysis pipelines, as well as the retrieval
    model and possibly make some fine-grain tuning to the search engine internals. However,
    retrieval models help a lot by providing a solid baseline to obtain a good relevance.
    Probably one of the most commonly used information retrieval models is the Vector
    Space Model 7. In this model, each document and query is represented as a vector. You
    can think of a vector as an arrow in a graph; each arrow in VSM can represent a query or
    a document. The closer two arrows are the more similar they are; each arrow’s direction
    is defined by the terms which compose the query / document. In such vector
    representation, each term has a weight (usually between 0 and 1), which tells how much
    important that term is in that document / query with respect to the rest of the documents
    in the search engine. Such weights can be calculated in various ways. At this point we
    won’t go too deep into the details of how these weights are calculated; we’ll just mention
    that the most common algorithm is called TF-IDF (term frequency inverted document
    frequency). The basic idea behind TF-IDF is that the more frequently a term appears in a
    single document (term frequency, or TF) the more important it is. At the same time, it
    states that the more common a term is among all the documents, the less important it is
    (inverted document frequency, or IDF). So in VSM, search results are ranked with
    respect to the query vector, so documents appear higher in the results list (get a higher
    rank/score) if they’re closer to such query vector.
    Footnote 7mdl.acm.org/citation.cfm?doid=361219.361220
    While VSM is an information retrieval model based on linear algebra, over the years
    alternate approaches based on probabilistic relevance models have emerged. Instead of
    calculating how near a document and a query vector are, a probabilistic model ranks
    search results on an estimate of the probability that a document is relevant to a certain
    query. One of the most common ranking functions for such models is called Okapi BM25
    . We won’t dive into its details, however it has shown good results especially on not very
    long texts.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>18
    We’ll be looking into how neural search can help with addressing relevance in the
    next chapters, but first we need to be able to measure it! A standard way of measuring
    how well an information retrieval system is doing is to calculate its precision and recall.
    Precision is the fraction of retrieved documents that are relevant. If your system has good
    precision, your users will mostly find the best results they’re looking for at the top of the
    list of search results. Recall is the fraction of relevant documents that are retrieved. If
    your system has a good recall, your users will find all results that are relevant for them in
    the search results, although they might not all be among the top ones.
    As you may have noticed, measuring precision and recall requires someone to judge
    how relevant the search results are. In small scenarios that’s an addressable task, however
    the effort that is required makes it hardly doable for huge collections of documents. An
    option to measure the effectiveness of your search engines is to use publicly available
    datasets for information retrieval, like the ones from TREC 8 which contain lots of
    ranked queries to be used for testing precision and recall.
    Footnote 8mtrec.nist.gov/data.html
    1.3 Unsolved problems
    We’ve had a closer look at how a search engine works, in particular how it strives to
    retrieve information that is relevant to the end user’s needs. But let’s take a step back and
    try to look at the problem from the perspective of how, as users, we use search engines
    every day. We’ll examine some of the problems that remain unresolved in many search
    scenarios, in order to better understand which issues we can hope to solve with the help
    of deep learning.
    Filling a knowledge gap, as opposed to simply retrieving information, is a slightly
    more complex topic. Let’s again take the example of going to a library because you want
    to know about interesting recent research in the field of artificial intelligence. As soon as
    you meet the librarian you have a first problem: how do you get the librarian to
    accurately understand what you need and what would really be useful to you?
    Although this sounds simple, the usefulness of a piece of information is hardly
    objective, but instead is rather subjective based on contexts and opinions. You may
    assume that a librarian has enough knowledge and experience and so you simply accept
    that what is given to you is good. In real life you would probably go and talk to the
    librarian in order to introduce yourself and share some information about your
    background and the reason why you need something; that would allow the librarian to
    use such context in order to:
    exclude some books before even trying to search
    discard some books after having found them
    explicitly look into one or more sections which have a closer relation to your context (e.g.
    coming from academia vs industry sources) while searching
    You’ll be able to give a feedback on the books given to you by the librarian mostly
    afterwards, although sometimes you could express concerns based on past experiences
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>19
    (e.g. you don’t like books written by a certain author, so you advise the librarian to
    explicitly not consider them). Both contexts and opinions can float very much and
    consequently have influence on the relevance of a certain information over time and
    among different persons. How does a librarian cope with this mismatch ?
    You as a user may not know the librarian, or at least not well enough to understand
    his/her context. The librarian’s background and opinions are important because they
    influence the results you get. Therefore the better you understand the librarian, the faster
    you’ll get your information. So you need to know your librarian in order to get good
    results!
    What if the librarian gives you a book about deep learning techniques in response to
    your first enquiry about artificial intelligence ? If you don’t know the subject, you need
    to make a second enquiry about an introduction to what deep learning is and if there’s
    any good book about it in the library. This process can go on a number of times; the key
    thing to understand here is that information is flowing incrementally; you just don’t
    upload stuff into your brain like in The Matrix movie. Instead, if you want to know
    something about AI, you may realise you need to know a bit of deep learning first, and
    for that you discover you need to read something about calculus and linear algebra, and
    so on. In other words, you don’t entirely know what you need when you start asking.
    To sum up, getting the information you are looking for from the librarian has some
    flaws, caused by these situations:
    the librarian doesn’t know you
    you don’t know the librarian
    you may need a few iterations in order to get everything you need
    It’s important to identify these issues here, because we want to use deep neural
    networks to help us build better search engines that can be more easily leveraged by end
    users and we’d like deep learning to help us fix those problems. Understanding these
    problems is the first step toward resolving them.
    1.3.1 Opening the search engine black box
    Now let’s try to understand what the users can see of what the search engine is doing.
    A crucial issue in creating effective search queries is which query language you use.
    Some years ago, you entered one or more keywords in a search box to perform a query.
    Nowadays, technology has evolved to the point where it allows you to type queries in
    natural language. Some search engines index documents in multiple languages (e.g. for
    web search) and allow querying consequently. If you try to search for the very same thing
    expressing it with slightly different queries on Google, you’ll observe surprisingly
    different results. Let’s try to google for latest breakthroughs in artificial intelligence and
    some variants:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>20
    Table 1.2mComparing similar queries
    Query1st Result Title
    latest breakthroughs in artificial intelligenceAcademic papers for 'latest breakthroughs in artificial intelligence'
    (Google Scholar)
    latest boosts in artificial intelligenceGoogle boosts artificial intelligence push with 2 top hires
    latest boosts on artificial intelligenceImages related to 'latest boosts on artificial intelligence' (Google
    Images)
    latest breakthroughs in AIArtificial Intelligence News—ScienceDaily
    più recenti sviluppi di ricerca sull’intelligenza Intelligenza Artificiale (Wikipedia)
    artificiale
    While the first result of the first query is not surprising, changing the term
    'breakthroughs' with one of its synonym ('boosts') produces a different result, which
    seems to suggest that the search engine has a different understanding of the information
    need: we were not looking into how Google is boosting AI! The third query gives a
    surprising result: images! We have no real explanation for this. Changing 'artificial
    intelligence' with its acronym AI led to a different, though still relevant, result. And when
    we use the Italian translation of the original query, we get a completely different result of
    the original query: a Wikipedia page about Artificial Intelligence, that seems quite
    generic, given the fact that Google Scholar indexes papers in different languages.
    Search engine ranking can float significantly, much like user opinions; although a
    search engineer could optimize it to respond to a set of given queries, it’s very hard to
    adjust scoring for the possibly tens or hundreds of similar queries. Often, performing
    search is a trial and error process: you issue a first query and get too many results, you
    issue a second query and still get too many, a third query might return trivial results
    you’re not interested in. Expressing an information need using a search query is not a
    trivial task. You often end up performing a bunch of queries just to get an high-level
    understanding of what you think the search engine can do with them. It’s like trying to
    look into a black box: you see just about nothing but try to make assumptions about what
    happens inside. What does this quick search experiment tells us ?
    In most of the cases users do not have the chance to understand what the search
    engine is doing and, even worse, things change a lot depending on the way the user
    expresses his/her information need.
    Now that you understand how a search engine generally works (Section 1.2), and
    learned about some important problems not completely solved yet in the search field
    (Section 1.3), it’s time to meet deep learning and discover how it can help to solve or at
    least mitigate these problems. To show how deep learning actually works, we’ll start with
    a high-level overview of the capabilities of deep neural networks.
    1.4 Deep learning to the rescue
    So far we’ve gone through information retrieval themes that were needed to prepare our
    journey through neural search. We’ll now start learning about deep learning (or DL)
    which can help us create smarter search engines. This section will introduce you to basic
    DL concepts.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>21
    In the past, a key difficulty in computer vision (a field in computer science that deals
    with processing and understanding visual data like pictures or videos), when working
    with images, was that it was hardly possible to come with an image representation
    containing information about the enclosed objects and visual structures. Deep learning
    helped to fix this difficulty with the creation of a special type of deep neural network that
    could learn image representation incrementally, one abstraction at a time, as shown in the
    figure below.
    Figure 1.7 Learning image abstractions incrementally
    Deep learning is a subfield of machine learning which focuses on learning deep
    representations of text, images or data in general by learning successive abstractions of
    increasingly meaningful representations. It does that by using deep neural networks (see
    a deep neural network with 3 hidden layers in the picture below).
    Figure 1.8 A deep feed forward neural network with 3 hidden layers
    At each step (or layer of the network), such deep neural networks are able to capture
    increasingly more complex structures in the data. It is not by chance that computer vision
    is one of the fields that fostered the development and research of representation-learning
    algorithms for images.
    Researchers have discovered that it makes sense to use such deep networks especially
    on data that is highly compositional 9, which means that they can help immensely when
    you can think of something as being formed by smaller parts of similar constituents.
    Images and text are very good examples of compositional data, as one can divide them
    into smaller units incrementally (e.g. text paragraphs sentences words).
    Footnote 9mcbmm.mit.edu/publications/when-and-why-are-deep-networks-better-shallow-ones
    Though there are many different ways a neural network can be architectured, neural
    networks are commonly composed by:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>22
    a set of neurons
    a set of connections between all or some neurons
    a weight (a real number) for each directed connection between two neurons
    one or more functions that map how each neuron receives and propagates signals
    towards its outgoing connections
    optionally, a set of layers that group sets of neurons having similar connectivity in the
    neural network
    In the above figure we can identify 20 neurons organized in a set of 5 layers. Each
    neuron within each layer is connected with all the neurons in the layers nearby (both the
    previous and the following layers), except for the first (blue) and last (green) layers.
    Conventionally, information starts flowing within the network from left to right, so that
    the first layer which receives the inputs is called the input layer as opposed to the last
    layer, called the output layer, which outputs the results of the neural network. The (red)
    layers in between are called hidden layers.
    Imagine that we could apply the same approach on text to learn representations of
    documents that capture increasingly higher abstractions within a document. Deep
    learning-based techniques exist for such tasks, and over time these algorithms are
    becoming smarter and we can use them to extract word, sentence, paragraph, document
    representations that can capture surprisingly interesting semantics. In fact, when using a
    neural network algorithm to learn word representations within a certain set of text
    documents, you’ll be able to see that closely related words lie near each other in the
    vector space. Think about creating a point on a two dimensional plot for each word
    contained in a piece of text and see how similar or closely related words lie close to one
    another, that’s achieved by using a neural network algorithm for learning word
    representations called word2vec.
    Figure 1.9 Word vectors derived from papers on word2vec
    You can notice that the words Information and Retrieval lie very close, similarly
    word2vec and Skip-gram, terms that both relate to (shallow) neural network algorithms
    used to extract word vectors, are near each other too.
    One of the key ideas of neural search is to leverage such representations in order to
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>23
    improve the effectiveness of search engines. It would be very nice if we could have a
    retrieval model that relies on word and document vectors (also called embeddings) with
    the above capabilities, so we could calculate and leverage document and word
    similarities very efficiently by looking at the 'nearest neighbours'.
    Figure 1.10 A neural search application: using word representations generated by a deep
    neural network to provide more relevant results
    In this picture we can see that we use a deep neural network to create word
    representations of the words contained in the indexed documents and we put them back
    into the search engine as we’ll use them to adjust the search results.
    In the previous section we’ve analyzed the importance of the context when compared
    to the complexity of expressing and understanding information needs via text queries.
    Deep representations of text are often built by using the context in which a certain word /
    sentence / document appears in order to infer an appropriate representation.
    Let’s look at the above example to briefly explain how deep learning algorithms can
    help in getting better results with relevance. Let’s get back to the table comparing queries
    and corresponding search results from the previous section and take the two queries latest
    breakthroughs in artificial intelligence and latest breakthroughs in AI, assuming we’re
    using the vector space model. In such models the similarity between queries and
    documents can vary a lot based on the text analysis chain. However this problem doesn’t
    affect vector representations of text generated with some recent neural network based
    algorithm. So while we could have artificial intelligence and AI lie very far in a vector
    space model, they will likely be placed very close when plotted using word
    representations generated by neural nets. With such simple change, we added some
    relevance boost to the search engine via more semantically grounded representations of
    words.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>24
    SIDEBAR
    Deep learning vs deep neural networks
    An important distinction has to be made: deep learning is mostly about
    learning representation of word, text, documents, images in a deep fashion.
    Deep neural networks, however, have a wider usage and are used, for
    example, in language modelling, machine translation, and many others. In
    this book we will make it clear when we use deep neural nets to learn
    representations and when we’re using it for other purposes. Besides learning
    representations, there are a number of information retrieval tasks that deep
    neural networks can help to solve.
    Before diving deeper into neural search applications, let’s a have a look at some
    practical implications of making search engines and neural networks work together.
    1.5 Index please meet Neuron
    An artificial neural network can learn to predict outputs based on a training set
    (supervised learning, in such collections each input is accompanied by its corresponding
    correct output) or perform unsupervised learning (no information about correct outputs is
    given) in order to extract patterns and / or learn representations. A search engine typical
    workflow involves indexing and searching content, notably such tasks can happen in
    parallel. Although this may sound like a technicality at this point, the way we integrate a
    search engine with a machine learning algorithm and output model is important in
    principle because it impacts neural search design effectiveness and performance. We can
    have a super accurate system, but if it’s slow no one will care to use it !
    1.5.1 Neural network training
    In order to use its powerful capabilities of learning we need to train a neural net.
    Training a network like the one shown in the previous section via supervised learning
    means providing some inputs to the network input layer, comparing the network
    (predicted) outputs with the known (target) outputs and letting the network learn from the
    discrepancies between predicted and target outputs. Neural networks can easily represent
    many interesting mathematical functions; that’s one of the reasons why they can have
    very high accuracy. Such mathematical functions are governed by the connections'
    weights and neurons' activation functions. A neural network learning algorithm takes the
    discrepancies between desired and actual outputs and adjusts each layer’s weights in
    order to reduce the output error in the future. If you feed enough data to the network it
    will be able to reach a very small error rate and therefore perform really well. Activation
    functions have an impact on the neural network capability to perform predictions and on
    how quickly they learn.
    The most famous neural network learning algorithm is called backpropagation. Given
    desired and actual outputs, the algorithm backpropagates each neuron’s error contribution
    and consequently adjusts its internal state on each neuron connections, one layer at a
    time, from output to input. This is a very high level description of how backpropagation
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>25
    algorithm works, we’ll have a closer look at it when we are more familiar with neural
    networks as we use them in the next chapters.
    The states for each layer together with its activation functions define a machine
    learning model for neural networks.
    Now that we have a first understanding of how neural nets learn, we need to decide
    how to plug into the search engine. Although design decisions can vary depending on the
    purpose neural networks are used for, training usually needs:
    a non trivial amount of time
    a lot of data
    For the these reasons we can identify a few high level solutions.
    Search engines can receive data to be indexed continuously; since new content is
    added, existing content is updated or even deleted. While it’s relatively easy and quick to
    support this within the search engine, sometimes machine learning algorithms create
    models that can’t be adapted quickly as data changes. Therefore it may be required to
    perform training again from scratch for the model to stay consistent with the new data in
    the search engine.
    A good choice in such scenarios is to look for online learning algorithms that can
    cope with data that changes without requiring training from scratch. When this is not
    possible we can mitigate the inconsistency between indexed data and the neural network
    model if we :
    'unplug' the model from the prediction phase
    'discount' the outputs of the neural network prediction by a certain rate, depending on
    how much data we expect to be unconsistent
    MODEL STORAGE
    Search engine central data structures are inverted indexes; they contain: posting lists,
    term dictionaries, information about term positions and other such data. A neural network
    model can be composed by a matrix of hundreds of thousands rows/columns. If we don’t
    store such a model within the index or somewhere on a disk we will need to retrain it
    upon restart of the system; given the high cost of training this is usually discouraged.
    In some cases, like in the previous example of using a neural network for learning to
    rank, it’s very hard to find a correlation between any entity stored in the inverted index
    and the learned model. On the other hand, for example, when learning representation of
    words, the neural network model is usually composed of multiple sets of weights, so one
    of these sets outlines a matrix where each row can be mapped to a word. So it makes
    sense to relate word vectors to terms in the inverted index. That means we may decide to
    'split' the machine learning model so that we store the word embedding together with its
    term within the index.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>26
    Table 1.3mInverted index table with word embeddings
    TermDOC Idsword embedding
    brown1,2[0.1,0.3,0.4]
    fox1,2[0.9,0.3,0.1]
    jumped1[0.2,0.2,0.4]
    over1,2[0.4,0.2,0.4]
    lazy1,2[0.5,0.1,0.1]
    quick2[0.7,0.4,0.3]
    jumps2[0.9,0.1,0.1]
    Such 'tricks' allow for efficient storage and retrieval of portions of the model on
    demand and do not require additional infrastructures to maintain it.
    1.5.2 The promises of neural search
    Neural search is about integrating deep learning and deep neural networks into search at
    different stages. Deep learning’s capability of capturing deep semantics within the
    generated representations will allow us to obtain relevance models and ranking functions
    that adapt well to underlying data. We’ll be able to learn image representations which
    will give us surprising results in image search. Simple similarity measures like cosine
    distance can be applied to learned representations to capture semantically similar words,
    sentences, paragraphs, etc.; this has a number of applications like in the text analysis
    phase or in recommending similar documents. At the same time, deep neural networks
    can do more than 'just' learning representations; they can learn to generate or translate
    text or learn how to optimize search engine performance.
    All the above sounds awesome, but beware that that you can’t just throw neural
    networks at your search engine and expect it to be automagically perfect. Every decision
    has to be taken in context and neural networks have some limitations too, including the
    cost of training, upgrading models, and more. But applying neural search to your search
    engine is a great way to make it better for your users. It also makes for a fascinating
    journey for the search engineers, who get to explore the beauty of neural networks.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>27
    1.6 Summary
    Search is a hard problem: common approaches to information retrieval come with some
    limitations and disadvantages, and both users and search engineers can have a hard time
    making things work as expected.
    Text analysis is an important task in search, for both indexing and search phases, because
    it prepares the data to be stored in the inverted indexes and has a high influence on the
    effectiveness of a search engine.
    Relevance is the fundamental measure of how well the search engine responds to users'
    information needs. Some information retrieval models can give a standardized measure
    of the importance of results with respect to queries, but there is no silver bullet. Context
    and opinions can float significantly among users, and therefore measuring relevance
    needs to be a continuous focus for a search engineer
    Deep learning is a field in machine learning that makes use of deep neural networks to
    learn (deep) representations of content (text like words, sentences, paragraphs, but also
    images) that can capture semantically relevant similarity measures.
    Neural search stands as a bridge between search and deep neural networks with the goal
    of using deep learning to help improve different tasks related to search.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>28
    2
    Generating synonyms
    In the previous chapter we had a high level overview of what kind of possibilities open
    up when deep learning gets applied to search problems. Those possibilities include using
    deep neural networks to search for images via a text query based on its content or
    generating text in natural language to better handle the user queries. We also learned
    about the basics of search engines, so now we are ready to start applying deep neural
    networks to solve search problems. In information retrieval, a common technique to
    improve the number of relevant results for a certain query is to use synonyms. In fact,
    synonyms allow you to expand the number of potential ways a query or a piece of
    indexed document is expressed. For example, you can express the sentence 'I like living
    in Rome' also as 'I enjoy living in the Eternal City,' the terms living and enjoying as
    well as Rome and the Eternal City are synonyms, the information conveyed by both
    sentences is the same though. If you think to the problems we’ve discussed in chapter 1,
    this technique may help a lot in allowing people to express the same concept in different
    ways while still retrieving the same search results!
    In this chapter we’ll get our hands on working with synonyms in the context of
    search, with the help of one of the most common neural network based algorithms for
    learning word representations, called word2vec. Learning about word2vec will give us
    the chance to look more closely at how neural networks work in practice. First, we’ll
    introduce feed-forward neural networks; next we’ll learn about the Skip-Gram and
    Continuous Bag of Words architectures. Then we’ll see how to apply these last two
    models to improve the search engine recall.
    We’ll also work on measuring how much the search engine can be enhanced this way
    and which tradeoffs we’ll need to consider for production systems. This is important
    when deciding when and where apply the techniques we’re going to learn to real life
    scenarios.
    In the previous chapter we have seen how important it is to have good algorithms for
    performing text analysis: in fact they specify the way text is broken into smaller
    fragments (terms). When it comes to executing a query, terms generated at indexing time
    need to match the ones extracted from the query for a document to be found and appear
    in the search results.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>29
    Put yourself in the shoes of the search engineer as you design your text analysis
    algorithms. Your ultimate goal is to facilitate retrieval over the data in your search
    engine. Let’s assume you can pick a query and adjust relevant documents so that they
    match the query. Or, you can do the opposite, and adjust the query in order to match
    some relevant documents which wouldn’t otherwise match. This is not the way to address
    the problem at scale, though. Since you can’t look at each and every document and then
    tweak your text analysis pipelines accordingly, it’s a good practice for someone working
    on a search engine to look at the data first. A lot of decisions can be made (and time
    saved) by simply browsing through the documents before indexing them.
    One of the most frequent hurdles that prevents matching is the fact that people can
    express a certain concept in at least two different ways. For example 'going for a walk in
    the mountains' can be also expressed using the word 'hiking', or with other words with
    similar meanings like 'tramping' or 'trekking'. It is quite common for the author of the
    text to be indexed, on the one hand, and the user performing searches, on the other, to use
    different ways of expressing the same concept respectively. For this reason, we need to
    make the search engine aware of such synonyms.
    Now, we’ll see how we can make use of a technique called synonym expansion to
    make it possible to express the same information need in more ways. While synonym
    expansion is a popular technique, it comes with some limitations. The biggest limitation
    is the need to maintain a dictionary of synonyms which will likely change over time and
    which is often not perfectly suited for the data to be indexed (e.g. obtained from publicly
    available data). We’ll see how we can make use of algorithms like word2vec to learn
    word representations that help generating synonyms in a very accurate way based on the
    data that needs to be indexed.
    By the end of the chapter, we will have a search engine which can leverage a neural
    network to generate synonyms that can be used to decorate the text to be indexed. Figure
    2.1 shows what we will end up with.
    Figure 2.1 Synonym expansion at search time, with a neural network
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>30
    2.1 Why synonyms
    A synonym of a certain word is another word which has different spelling but the same or
    very close meaning. For example aircraft and airplane are both synonyms of the word
    plane. In information retrieval it is very common to use synonyms to decorate a certain
    piece of text in order to make it more probable for an appropriate query to match. Yes,
    we’re talking about probability here, because we cannot anticipate all of the possible
    ways of expressing an information need, so this technique is not a silver bullet that will
    make it possible to understand all of your user queries, however it will help a lot to lower
    the number of queries that give too few or zero results.
    Let’s look at an example where this technique can be very useful: imagine you have a
    system which records song lyrics and you want your users to be able to search through it.
    This has probably happened to you already: you vaguely remember only a short piece of
    a certain song, or you remember something about the meaning of a lyric, but not the
    exact wording from the song you have in mind. Let’s say you liked a lot a song whose
    chorus sang along the lines of 'music is my … something'. What was it ? A car ? A boat
    ? A plane ? If you have synonym expansion enabled in the search engine searching for
    music is my plane, you will find the song you’re looking for: music is my aeroplane! The
    key effect of the use of synonyms in this context is that an end user is able to find a
    relevant document (the song Aeroplane by Red Hot Chili Peppers) which would have not
    been possible to retrieve with the mentioned queries (music is my boat, music is my
    plane, music is my car) with synonym expansion disabled. This is considered an
    improvement in recall. As briefly mentioned in chapter 1, the recall is a number between
    0 and 1 equal to the number of documents that are retrieved and relevant, divided by the
    number of retrieved documents. So if none of the retrieved documents is relevant, recall
    is equals to 0, if all of the retrieved documents are also relevant, recall is equals to 1.
    Suppose that without synonym expansion the query music is my plane returns 3
    documents, 2 of which are also relevant, giving a recall of 2/3 = 0.66. If we then enable
    synonym expansion we have one more document retrieved (the song Aeroplane) that is
    also relevant, the resulting recall is then 3/4 = 0.75.
    The overall idea of synonym expansion is that when the search engine receives a
    certain stream of terms, either at search or indexing time, it can enrich them by adding
    their synonyms, if they exist, at the very same position. In the Aeroplane example,
    synonyms of the query terms have been expanded, so that they got silently decorated
    with the word aeroplane at the same position of plane in the stream of text.
    Figure 2.2 Synonym expansion graph
    We can also apply the same technique during indexing of the Aeroplane lyrics.
    Expanding synonyms at indexing time will make indexing slightly slower (because of the
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>31
    calls to word2vec) and the index will inevitably be bigger (because it will contain more
    terms to store). On the plus side, searching will be faster because the word2vec call won’t
    happen during search. Deciding whether to do index or search time synonym expansion
    might have a noticeable impact on the performance of the system as its size and load
    grows.
    Now that we have seen why synonyms are useful in the context of search, let’s look
    at how to implement synonym expansion, first by using common techniques, and then by
    using word2vec. This will allow us to appreciate the advantages of using the latter over
    the former.
    2.2 Vocabulary based synonym matching
    Now we’ll dive into how to implement a search engine with synonym expansion enabled
    at indexing time. The most common and simple approach for implementing synonyms is
    based on feeding the search engine with a vocabulary which contains the mapping
    between all the words and their related synonyms. Such a vocabulary can look like a
    table where each key is a word and the corresponding values are its synonyms:
    Listing 2.1 A vocabulary of synonyms
    aeroplane -> plane, airplane, aircraft
    boat -> ship, vessel
    car -> automobile
    ...
    So imagine that we feed the lyrics for Aeroplane song into the search engine for
    indexing and we use synonym expansion with the above vocabulary. Let’s pick the
    chorus of the song: music is my aeroplane and see how synonym expansion would handle
    it. We have a simple text analysis pipeline composed of a tokenizer, which creates a
    token every time it encounters a white space, simply resulting in creating a token for each
    of the words in the sentence. The index time text analysis pipeline will therefore create
    such tokens. Then we’ll use a token filter for synonym expansion: for each received
    token it will look into the vocabulary of synonyms and see if any of the keys (e.g.
    aeroplane, boat, car) is equal to the token text. The posting list for the fragment music is
    my aeroplane (sorted in ascending alphabetical order) will look like:
    Table 2.1mPosting list for 'music is my aeroplane' fragment
    TermDOC(POS)
    aeroplane1(12,17)
    aircraft1(12,17)
    airplane1(12,17)
    is1(6,8)
    music1(0,5)
    my1(9,11)
    plane1(12,17)
    This particular posting list also records information about the position of the
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>32
    occurrence of a certain term in a specific document. This information helps us visualize
    the fact that the terms plane, airplane and aircraft, which were not included in the
    original text fragment, got added to the index with the same position information attached
    to the original term (aeroplane).
    We can in fact record the positions of the terms in an inverted index in order to
    reconstruct the order in which a certain term appears in the text of a certain document. If
    we look into the inverted index table above and pick the terms that have the lower
    positions in ascending order, we’ll get music is my aeroplane/aircraft/airplane/plane.
    The synonyms can be seamlessly replaced with one another, so in our index we can
    imagine having four different pieces of text: music is my aeroplane, music is my aircraft,
    music is my airplane. It’s very important to emphasize here that while we came out with
    four different forms in which the above sentence can be indexed and searched, if any of
    them match only one document will be returned by a search engine, as they all reference
    doc 1 in the posting list.
    Now that we have an understanding of how synonyms can be indexed into the search
    engine, we’ll get to try things out and build our first Apache Lucene 10 based search
    engine, indexing some lyrics and setting up proper text analysis with synonym expansion
    at indexing time.
    Footnote 10mgoing forward we’ll use Lucene and Apache Lucene names interchangeably, but the proper
    name (trademark) is Apache Lucene
    2.2.1 A quick look at Apache Lucene
    Let’s get into a brief introduction to Lucene before diving into the synonym expansion
    bits. This will allow us to focus more on the concepts rather than on the Lucene APIs and
    implementation details.
    In Lucene the main entities to be indexed and searched are represented by Documents
    . A Document, depending on your use case, can represent anything: a page, a book, a
    paragraph, etc. Whatever it is, that’ll be what you get in your search results. A Document
    is composed by a number of Fields, they can be used capture different portions of a
    Document, e.g. if your document is a web page, you can think of having a separate Field
    for the page title, the page contents, the page size, the creation time and so on. The main
    reason for the existence of fields, is that you can:
    configure per field text analysis pipelines
    configure some indexing options, like storing or not the term positions, or whether
    storing or not the value of original text that each term refers to, within the posting lists
    A Lucene search engine can be accessed via a Directory: a list of files where the
    inverted indexes (and other data structures, used for example to record positions) are
    persisted. A view on a Directory for reading can be obtained by opening an IndexReader
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>33
    Listing 2.2 Getting a read view on the indexes (opening an IndexReader)
    Path path = Paths.get('/home/lucene/luceneidx');
    Directory directory = FSDirectory.open(path);
    IndexReader reader = DirectoryReader.open(directory);
    a target path where the inverted indexes are stored on the file system
    open a Directory over the target path
    obtain a read only view over the search engine via an IndexReader
    An IndexReader can be used to obtain some useful statistics over an index, like the
    number of documents currently indexed, or if there’re any documents that have been
    deleted. You can obtain statistics about a certain field or a particular term. Also, if you
    know the identifier of the document you want to retrieve, you can get Documents from an
    IndexReader directly.
    Listing 2.3 Getting documents from an IndexReader
    int identifier = 123;
    Document document = reader.document(identifier);
    An IndexReader is needed to be able to search: you will create an IndexSearcher over
    an IndexReader. The IndexSearcher is the entry point for performing search and
    collecting results; the queries that will be performed over such an IndexSearcher will
    work on the specific view over the index, provided by the IndexReader.
    Running a user entered query without getting too much into coding queries
    programmatically can be done by using a QueryParser. As discussed we’ll need to
    specify a (search time) text analysis when searching. In Lucene the text analysis task is
    performed by implementors of the Analyzer API; Analyzers can be composed by a
    Tokenizer and, optionally, TokenFilter components or you can use out of the box
    implementations like in the case below.
    Listing 2.4 Parsing and running user entered queries
    QueryParser parser = new QueryParser('title', new WhitespaceAnalyzer());
    Query query = parser.parse('+Deep +search');
    create a query parser over the 'title' field with a WhitespaceAnalyzer
    parse the user entered query and obtain a Lucene Query
    In the case above we tell the query parser to split tokens when they find a whitespace
    and run queries over the field named title. If we imagine our user to type the query
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>34
    +Deep +search we pass that to the QueryParser and obtain a Lucene Query object. Now
    we can run the query.
    Listing 2.5 Searching with Lucene
    TopDocs hits = searcher.search(query, 10);
    for (int i = 0; i < hits.scoreDocs.length; i++) {
    ScoreDoc scoreDoc = hits.scoreDocs[i];
    Document doc = reader.document(scoreDoc.doc);
    System.out.println(doc.get('title') + ' : ' + scoreDoc.score);
    }
    perform the Query against the IndexSearcher, returning the first 10 documents
    iterate over the results
    retrieve a ScoreDoc, which holds the returned document identifier and its score
    (given by the underlying retrieval model)
    obtain a Document where you can inspect fields using the document id
    output the value of the title field of the returned document
    Now if we run this we’ll get no results because we’ve not indexed anything yet! Let’s
    fix this and learn how to index some Documents with Lucene. First of all we have to
    decide which fields we’re going to put into our documents and how their (index time)
    text analysis pipelines look like. Let’s get back to the example of books and assume we
    want to have some 'useless' words removed for the book contents while having a simpler
    text analysis pipeline for the title which doesn’t remove anything.
    Listing 2.6 Building per field analyzers
    Map<String, Analyzer> perFieldAnalyzers = new HashMap<>();
    CharArraySet stopWords = new CharArraySet(Arrays.asList('a', 'an', 'the'), true);
    perFieldAnalyzers.put('pages', new StopAnalyzer(stopWords));
    perFieldAnalyzers.put('title', new WhitespaceAnalyzer());
    Analyzer analyzer = new PerFieldAnalyzerWrapper(new EnglishAnalyzer(), perFieldAnalyzers);
    set up a map where the keys are the names of the fields and the values are the
    Analyzers to be used for such fields
    create a stop word list of the tokens that we want to remove from the books'
    contents while indexing
    use a StopAnalyzer with the given stopwords for the field named pages
    use a simple WhitespaceAnalyzer for the _title field
    create a per field Analyzer, which also requires a default analyzer (an
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>35
    EnglishAnalyzer in this case) for any other field that may get added to a Document
    The inverted indexes for a Lucene based search engine are written on disk on a
    Directory by an IndexWriter which will persist Documents according to an
    IndexWriterConfig, such config contains many options, but for us the most important bit
    is the required index time analyzer. Once we have the IndexWriter ready, we can create
    Documents and add Fields.
    Listing 2.7 Adding some documents to the Lucene index
    IndexWriterConfig config = new IndexWriterConfig(analyzer);
    IndexWriter writer = new IndexWriter(directory, config);
    Document doc1 = new Document();
    doc1.add(new TextField('title', 'Deep learning for search', Field.Store.YES));
    doc1.add(new TextField('page', 'Living in the information age ...', Field.Store.YES));
    Document doc2 = new Document();
    doc2.add(new TextField('title', 'Relevant search', Field.Store.YES));
    doc2.add(new TextField('page', 'Getting a search engine to behave can be maddening ...',
    Field.Store.YES));
    writer.addDocument(doc1);
    writer.addDocument(doc2);
    create a configuration for indexing
    create an IndexWriter over a Directory , based on a IndexWriterConfig
    create Document instances
    add fields, each one having a name, a value and an option to store the value as it is
    together with the terms
    add Documents to the search engine
    Once we have finished adding a few documents to our IndexWriter, we can persist
    them on the file system by issuing a commit, until then in fact new IndexReaders won’t
    see the newly added documents.
    Listing 2.8 Persisting changes to the index
    writer.commit();
    writer.close();
    commit the changes
    close the IndexWriter (release resources)
    Now if we run again the same code for searching that’s what we’ll get:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>36
    Listing 2.9 Search output
    Deep learning for search : 0.040937614
    our searching code found a match for the query +Deep +search and prints its title
    and score
    Now that we are all set with a Lucene introduction, let’s move on the topic of synoym
    expansion.
    2.2.2 Setting up a Lucene index with synonym expansion
    We’ll first define the algorithms we want to use for text analysis at indexing and search
    time. Then we’ll proceed to ingesting some lyrics into an inverted index. In many cases
    it’s a good practice to use the same tokenizer both at indexing and search time, so that the
    text gets split according to the same algorithm and therefore it’s easier for queries and
    fragments of documents to match. We’ll start simple and set up:
    a search time Analyzer which uses a tokenizer that splits tokens when it encounters a
    whitespace character (also called whitespace tokenizer)
    an index time Analyzer which uses that a whitespace tokenizer and a synonym filter
    The reason for doing this is that we do not need synonym expansion both at query and
    index time; for two synonym words to match it is sufficient to do expansion once.
    Assuming we have two synonym words aeroplane and plane, the code below will
    build a text analysis chain which can take a term of an original token (e.g. plane) and
    then generate another term for its synonym word (e.g. aeroplane). So both the original
    and the new term will be generated.
    Listing 2.10 Configuring synonym expansion
    SynonymMap.Builder b = new SynonymMap.Builder();
    builder.add(new CharsRef('aeroplane'), new CharsRef('plane'), true);
    final SynonymMap map = b.build();
    Analyzer indexTimeAnalyzer = new Analyzer() {
    @Override
    protected TokenStreamComponents createComponents(String fieldName) {
    Tokenizer tokenizer = new WhitespaceTokenizer();
    SynonymGraphFilter synFilter = new SynonymGraphFilter(tokenizer, map, true);
    return new TokenStreamComponents(tokenizer, synFilter);
    }
    };
    Analyzer searchTimeAnalyzer = new WhitespaceAnalyzer();
    programmatically define synonyms
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>37
    create a custom Analyzer, for indexing, with white space tokenizer and synonym
    filter
    create a synonym filter which receives terms from the whitespace tokenizer,
    expands synonyms according to a map wordsynonym ignoring case
    a simple whitespace analyzer for the search time
    The simplistic example above creates a synonym vocabulary with just one entry.
    There will be more entries, or we’ll be reading them by an external file so that we don’t
    have to write the code for each synonym.
    We’re just about ready to take some song lyrics and put them into the index using the
    indexTimeAnalyzer. Before doing that, let’s have a look at how song lyrics are structured.
    Each song has an author, a title, a publication year, the lyrics text, etc. As stated above,
    it’s very important to have a close look at the data to be indexed. This allows us to see
    what kind of data is in there, and possibly come up with reasoned text analysis chains
    that we expect to work well on that data.
    Listing 2.11 Song lyrics example
    author: Red Hot Chili Peppers
    title: Aeroplane
    year: 1995
    album: One Hot Minute
    text: I like pleasure spiked with pain and music is my aeroplane ...
    Can we keep track of such a structure in a search engine ? Would that be useful ?
    In most cases it’s very handy to keep a lightweight document structure because each
    part of it conveys different semantics, and therefore different requirements in the way
    they are hit by search. For example, the year will always be a numeric value; it makes no
    sense to use a whitespace tokenizer on it, because it’s very unlikely that any whitespace
    will appear in there. On all the other fields, we can probably use the Analyzer we defined
    above for indexing.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>38
    Figure 2.3 Split portions of the text depending on the type of data
    Putting it all together, we’ll have multiple inverted indexes (one for each attribute)
    that address indexing of different parts of a document, all within the same search engine.
    With Lucene we could define a field for each of the attributes in the example above
    (author, title, year, album, text).
    We’ll therefore specify that we want a separate Analyzer for the year field which
    doesn’t touch the value, for all the other values it will use the default indexTimeAnalyzer
    with synonym expansion enabled:
    Listing 2.12 Separate analysis chains for indexing and search
    Directory directory = FSDirectory.open(Paths.get('/path/to/index'));
    Map<String, Analyzer> perFieldAnalyzers = new HashMap<>();
    perFieldAnalyzers.put('year', new KeywordAnalyzer());
    Analyzer analyzer = new PerFieldAnalyzerWrapper(indexTimeAnalyzer, perFieldAnalyzers);
    IndexWriterConfig config = new IndexWriterConfig(analyzer);
    IndexWriter writer = new IndexWriter(directory, config);
    open a Directory for indexing
    create a map whose keys are the names of the fields and the values the
    corresponding analysis chain to be used
    setup a different analyzer (keyword, doesn’t touch the value) for the year
    create a wrapping analyzer which can work with per field analyzers
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>39
    build all the above in a configuration object
    create an IndexWriter to be used for indexing
    This mechanism allows indexing to be very flexible in the way content is analyzed
    before being written into the inverted indexes; it’s very common to play with different
    Analyzers for different portions of Documents, and to change them several times before
    finding the best set of algorithms for a certain data corpus. And even then it’s likely that
    such configurations will need adjustments over time in real life. As an example you may
    only index English songs and, at some point, start ingesting songs in different languages.
    In this case you’ll most probably have to adjust the analyzers to work well with both
    languages (e.g. you can’t expect a whitespace tokenizer to work well on CJK languages
    where words are often not separated by a whitespace).
    Let’s get our first document in the Lucene index.
    Listing 2.13 Indexing documents
    Document doc1 = new Document();
    doc1.add(new Field('title', 'Aeroplane', type));
    doc1.add(new Field('author', 'Red Hot Chili Peppers', type));
    doc1.add(new Field('year', '1995', type));
    doc1.add(new Field('album', 'One Hot Minute', type));
    doc1.add(new Field('text', 'I like pleasure spiked with pain and music is
    my aeroplane ...', type));
    writer.addDocument(doc1);
    writer.commit();
    create a document
    add all the fields from the song lyrics
    add the document
    persist the updated inverted index to the file system, making the changes durable
    (and searchable)
    As you can see we have created a document composed by multiple fields, one per
    song attribute, and then we have added it to the writer.
    In order to search we open the Directory (again) and obtain a view on the index, an
    IndexReader, upon which we can search via an IndexSearcher. We want to make sure
    that our synonym expansion works as expected so we type the query with the word plane
    and expect the Aeroplane song to be retrieved.
    Listing 2.14 Searching
    IndexReader reader = DirectoryReader.open(directory);
    IndexSearcher searcher = new IndexSearcher(reader);
    QueryParser parser = new QueryParser(null, searchTimeAnalyzer);
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>40
    Query query = parser.parse('plane');
    TopDocs hits = searcher.search(query, 10);
    for (int i = 0; i < hits.scoreDocs.length; i++) {
    ScoreDoc scoreDoc = hits.scoreDocs[i];
    Document doc = searcher.doc(scoreDoc.doc);
    System.out.println(doc.get('title') + ' by ' + doc.get('author'));
    }
    open a view on the index
    instantiate a searcher
    create a query parser that will use the search time analyzer over the user entered
    query to produce search terms
    transform a user entered query (as a String) into a proper Lucene query object
    using the QueryParser
    search and obtain the first 10 results
    iterate over the results
    obtain the search result
    output the title and author of the returned song
    As we expected we obtain the following output:
    Listing 2.15 Search output
    Aeroplane by Red Hot Chili Peppers
    We’ve gone through a quick tour of how to set up text analysis for index and search,
    and how to index documents and retrieve them. You’ve also learned how to add the
    synonym expansion capability. However, it should be also clear that the code we’ve just
    read can hardly be maintained in real life, because:
    you can’t expect to write code for each and every synonym you want to add
    you need a synonym vocabulary that can be plugged in and managed separately to avoid
    having to modify the search code every time you need to update it
    you need to manage the evolution of languages, even nowadays new words (and
    synonyms) get added constantly to many languages
    A first step towards resolving these three issues is to write the synonyms into a file
    and let the synonym filter read them from there. We’ll be doing that by putting synonym
    words into the same line, separated by comma. We will build the Analyzer in a more
    compact way, by using a builder pattern 11: .Feeding synonyms from a file
    Footnote 11men.wikipedia.org/wiki/Builder_pattern
    Map<String, String> sffargs = new HashMap<>();
    sffargs.put('synonyms', 'synonyms.txt');
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>41
    CustomAnalyzer.Builder builder = CustomAnalyzer.builder()
    .withTokenizer(WhitespaceTokenizerFactory.class)
    .addTokenFilter(SynonymGraphFilterFactory.class, sffargs)
    return builder.build();
    define the file which contains the synonyms
    define an analyzer
    let the analyzer use a whitespace tokenizer
    let the analyzer use a synonym filter
    And we set up synonyms in the file :
    Listing 2.16 A synonyms file
    plane,aeroplane,aircraft,airplane
    boat,vessel,ship
    ...
    This way the code remains unchanged regardless of any change in the synonyms file,
    as we can update it as much as we need to. While this is much better than having to write
    code for synonyms, you don’t want to be writing this synonyms file by hand, unless you
    already know that you’re going to have a few fixed synonyms. Fortunately, these days we
    have lots of open data that we can take and leverage for free or for a very low cost. A
    very good and very large resource for natural language processing in general is the
    WordNet project 12, a lexical database for English language from the Princeton
    University. We can take advantage of the large synonym vocabulary from WordNet,
    which gets constantly updated, and include it in our indexing analysis pipeline by simply
    downloading it and specifying to use the WordNet format.
    Footnote 12mwordnet.princeton.edu/
    Listing 2.17 Using synoyms from WordNet
    Map<String, String> sffargs = new HashMap<>();
    sffargs.put('synonyms', 'synonyms-wn.txt');
    sffargs.put('format', 'wordnet');
    CustomAnalyzer.Builder builder = CustomAnalyzer.builder()
    .withTokenizer(WhitespaceTokenizerFactory.class)
    .addTokenFilter(SynonymGraphFilterFactory.class, sffargs)
    return builder.build();
    setup a synonym file using WordNet vocabulary
    specify the WordNet format for the synonym file
    With the WordNet dictionary plugged in, we can consider ourselves pretty much safe
    for plain English. Still we haven’t solved every problem. Additional limitations:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>42
    there’s not a WordNet for each and every language
    even for the English case, the synonym expansion for a certain word is based on the
    English grammar rules, but nothing is done to take into account the context in which
    those words appear
    The last point refers to the difference between what linguists define as a synonym
    based on strict rules versus how people use languages and words in real life, or the
    denotation (stricty dictionary definition) of a word, as opposed to its connotation
    (common usage). In informal contexts like social networks or chatrooms or when you
    simply meet a friend in real life, people may use two words as if they were synonyms,
    even if, by grammar rules, they are not synonyms. Here word2vec will kick in and get us
    to a more advanced level than plain expanding synoyms based on the syntax of a certain
    language. In fact we’ll see that using word2vec enables us to build synonym expansions
    which are agnostic to the language; it will learn from the data which words are similar,
    without 'caring' too much about the language used. A very nice feature of word2vec is
    that words with similar contexts are considered similar exactly because of that—there’re
    no grammar or syntax involved. For each word, it just looks at its surrounding words,
    assuming that semantically similar words will appear in similar contexts.
    2.3 Generating synonyms
    The main problem with the approach outlined so far is that synonyms mappings are static
    and not bound to the indexed data. For example, they obey the English grammar
    semantics in the case of WordNet, but do not take into account different slang or informal
    contexts where some words are often used as synonyms even if they are not synonyms
    according to strict rules of grammar and semantics. Another example is the acronyms
    used in chat sessions and emails (e.g. ICYMI in case you missed it, AKA also known
    as), which may be too colloquial to go into an academic dictionary.
    One approach to overcoming these limitations is to have a way to generate synonyms
    from the data to be ingested into the search engine itself. The basic concept we build on
    is that it should be possible to extract the 'nearest neighbours' of a certain word by
    looking at the context of the word., which means analyzing the patterns of surrounding
    words that occur together with it. A nearest neighbour of a certain word in our case
    should be its synonym.
    This idea that words that are used, and occur, in the same contexts tend to purport
    similar meanings is called the distributional hypothesis and stands at the basis of many
    deep learning algorithms for text representations. The very interesting thing about this
    idea is that it disregards language, slang, style, and grammar: every information about a
    certain word is inferred from the word contexts that appear in the text. Think, for
    example, of how words representing cities (Rome, Cape Town, Oakland, etc.) are often
    used. Let’s look at a few sentences.
    Listing 2.18 Similar context for words about cities.
    I like to live in Rome because ...
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>43
    People that love surfing should go to Cape Town ...
    I would like to visit Oakland to see ...
    Traffic is crazy in Rome ...
    Often they are used near the term in or in short distance from verbs like live, visit, etc.
    . This is the basic intuition behind the fact that the context gathers a lot of information
    about each word.
    With this in mind we would like to learn word representations of the words in the data
    we want to index, so that we can generate synonyms out of the data itself, rather than
    manually building or downloading a synonym vocabulary. In the library example in
    chapter 1, we briefly mentioned that it would be best to have more insights about what’s
    in the library; with these additional insights, the librarian could help us more effectively.
    A student coming to the library could ask the librarian, say, for 'books about artificial
    intelligence. Let’s also say the library has only one book on the topic, and it is called 'AI
    principles.' If the librarian (or maybe even the student) were simply searching through
    book titles, they would miss this book, unless they know that AI is an acronym (and
    therefore a synonym) of artificial intelligence. An assistant knowledgeable about
    synonyms would be very useful in this situation. Let’s imagine two hypothetical types of
    such an assistant: John, an English language expert who has studied English grammar
    and syntax for years; and Robbie, another student who collaborates weekly with the
    librarian and has the chance to read most of the books. John could hardly tell you that AI
    stands for artificial intelligence because his background doesn’t give him this
    information. Robbie, on the other hand, has far less formal knowledge of English, but he
    is an expert on the books in the library; he could easily tell us that AI stands for artificial
    intelligence, because he’s read the book AI principles. In this scenario, John is acting like
    our WordNet vocabulary, and Robbie is our word2vec algorithm. While John has a
    proven knowledge of the language, Robbie might be more helpful in this patrticular
    situation.
    In chapter 1, we mentioned that neural networks are very good at learning
    representations (in this case, representations of words) that are sensitive to the context.
    That’s the quality we’re going to leverage here with word2vec. before we get deeper into
    that, I first want to give you a closer look at one of the simplest form of neural networks,
    called feed forward neural networks, because these are the basis for most, if not all, of
    the more complex neural network architectures.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>44
    2.4 Feed forward neural networks
    Neural networks are the key tool for neural search, and many neural network
    architectures extend from feed forward networks. A feed forward neural network is a
    neural network where information flows from the input layer to hidden layers, if any, and
    finally to the output layer; there are no loops present, since the connections among
    neurons do not form a cycle. Think of it as a magic black box with some inputs and
    outputs. The magic mostly happens inside the net, thanks to the way neurons are
    connected to each other and how they react to their inputs. If we were looking for a house
    to buy in a specific country, for instance, we could use the magic box to predict a fair
    price to expect to pay for a certain house, as long as we had some information we have
    about it, such as its size, location, and rating given by the seller.
    Figure 2.4 Predicting price with a feed forward neural network with 3 inputs, 5 hidden
    units and 1 output unit
    A feed forward neural network is composed of :
    an input layer
    optionally one or more hidden layers
    an output layer
    The input layer is responsible for gathering the inputs provided by the user; these
    inputs are usually in the form of real numbers. In the predicting price example, we have 3
    inputs: house size, the location, the amount of money required by the seller. We will
    encode these inputs as 3 real numbers, so that the input we’ll be passing to the network
    will be a 3-dimensional vector [ size, location, price]. The hidden layer role represents
    the more mysterious part of the network. Think of it as the part of the network that allows
    it to be so good at learning and predicting. In the example above we have 5 units in the
    hidden layer, all of them are connected to the units in the input layer and also to all the
    units in the output layer. The connectivity in the network plays a fundamental role in the
    network activity dynamics. Most of the time all the units in a layer (x) are fully connected
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>45
    (forward) to the units in the next layer (x+1). The output layer is responsible for
    providing the final output of the network. In the price example it will provide a real
    number representing what the network estimates the right price should be.
    2.4.1 How it works, weights and activation functions
    As you’ve seen, a feed forward neural network receives some inputs and produces some
    outputs. The fundamental building blocks of these networks are so-called neurons (even
    though the neuron of an actual brain is much more complex). Every neuron in a feed
    forward neural network:
    belongs to a layer
    smooths each input by its incoming weight
    propagates its output according to an activation function
    Looking at the example of the feed forward neural network below, the second layer is
    composed of only one neuron, the green one. This neuron receives input from 3 neurons
    in layer 1 and propagates output to only one neuron in layer 3. This green neuron has an
    associated activation function, and its incoming links with the previous layer have
    associated weights (often real numbers between -1 and 1).
    Figure 2.5 Propagating signal through the network
    Let’s assume that all the incoming weights of the green neuron are set to 0.3 and that
    it receives from the first layer the inputs 0.4, 0.5 and 0.6. Each weight is multiplied by its
    input and the results are summed together so that in our case we have: 0.3 * 0.4 + 0.3 *
    0.5 + 0.3 * 0.6 = 0.45. The activation function is then applied to this intermediate result
    and then propagated to the outgoing links of the neuron. Common activation functions
    are hyperbolic tangent (or tanh), sigmoid, rectified linear unit (or ReLU). (For a more
    detailed description of these functions refer to the appendix.) In the current example, let’s
    use the tanh function. We’ll have that tanh(0.45) = 0.4218990053 so that the red neuron
    in the third layer will receive this number as an input on its only incoming link. The red
    neuron will perform exactly the same steps that the green neuron does, using its own
    weights. It’s for this reason that these networks are called feed forward: each neuron
    transforms and propagates its inputs in order to feed the neurons in the next layer.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>46
    2.4.2 Backpropagation in a nutshell
    In chapter 1 we mentioned that neural networks and deep learning belong to the field of
    machine learning, but we still do not know how they learn. A fundamental point when
    discussing the rise of deep learning is related to how well and how fast neural networks
    can learn. Although artificial neural networks are an old computing paradigm (1950
    circa), they only recently became popular and effective as the modern computers'
    performance improved to a level that allowed neural nets to perform an effective learning
    in reasonable time.
    The most famous and widely used algorithm for deep learning is called
    backpropagation. As opposed to what we’ve seen in the previous section, this algorithm
    lets the signal flow backwards from the output layer until the input layer. The values of
    the activations of the neurons in the output layer, generated by a feed forward pass on a
    certain input, are compared with the ones in the desired output. This comparison is
    performed by cost function which calculates a loss or cost and represents a measure of
    how much the network is wrong in that particular case. Such an error is sent backwards
    through the incoming connections of the output neurons to the corresponding units in the
    hidden layer. You can see in the image below that the orange neuron in the output layer
    sends back its portion of error to the connected units in the hidden layer.
    Figure 2.6 Backpropagating signal from output to hidden layer
    Once a unit receives an error it will update its weights, according to an update
    algorithm, usually the algorithm used here is stochastic gradient descent. This backward
    update of weights happens until the weights on the input layer connections are adjusted
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>47
    and then stops. So a run of backpropagation will update all the weights associated with
    the existing connections. The rationale behind this algorithm is that each weight is
    responsible for a portion of the error, therefore it tries to adjust them in order to reduce
    the error for that particular input / output pair.
    The gradient descent algorithm (or any other update algorithm for adjusting the
    weights) decides how the weights are changed with respect to the portion of error they
    receive. There is a lot of maths behind that but you can think to it as if the cost function
    would define a shape like the one below where the height of 'hills' define the amount of
    error, so that a very low point has very low error :
    Figure 2.7 Geometric interpretation of backpropagation with gradient descent
    low : this is the point with the lowest possible error, having optimal values for the neural
    network weights
    high : this is a point with high error, therefore gradient descent tries to perform descent
    towards points with lower error
    The coordinates of a point are given by the value of the weights in the neural
    network, so the gradient descent tries to find a value of the weights (a point) with very
    low error (a very low height) in the shape.
    2.5 Leveraging word2vec
    Now that you understand what a generic feed forward network is, we can focus on a
    more specific algorithm based on feed forward neural networks, called word2vec.
    Word2vec is our first neural network algorithm we’re going to discover. While it’s fairly
    easy to understand its basics, it’s also fascinating to see the good results—in terms of
    capturing the semantics of words in a text—that you can get out of it. But what does it
    do, and how is it useful for our synonym expansion use case?
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>48
    word2vec takes a piece text and outputs a series of vectors, one for each word in the text
    when the output vectors of word2vec are plotted on a two dimensional graph, vectors
    whose words are very similar in term of semantics are very close to one another
    we can use distance measures like cosine distance to find the most similar words with
    respect to a certain word
    we can use this technique to find a certain word synonyms
    So, in short, what we want to do is setup a word2vec model, feed it with the text of
    the song lyrics we want to index, get some output vectors for each word and use them to
    find synonyms.
    You already heard about the usage of vectors in the context of search—in chapter 1
    when we talked about vector space model and TF-IDF. In a sense, word2vec also
    generates a vector space model whose vectors (one for each word) are weighted by the
    neural network during the learning process. Let’s get back at our example of the
    Aeroplane song, if we feed its text to word2vec we’ll have a vector for each of our
    words:
    Listing 2.19 Two dimensional vectors for Aeroplane words
    0.7976110753441061, -1.300175666666296, i
    -1.1589942649711316, 0.2550385962680938, like
    -1.9136814615251492, 0.0, pleasure
    -0.178102361461314, -5.778459658617458, spiked
    0.11344064895365787, 0.0, with
    0.3778008406249243, -0.11222894354254397, pain
    -2.0494382050792344, 0.5871714329463343, and
    -1.3652666102221962, -0.4866885862322685, music
    -12.878251690899361, 0.7094618209959707, is
    0.8220355668636578, -1.2088098678855501, my
    -0.37314503461270637, 0.4801501371764839, aeroplane
    ...
    We can see them in the coordinate plan.
    Figure 2.8 Plotted word vectors for Aeoroplane
    In the example output above I decided to use two dimensions so that those vectors are
    more easily plottable on a graph. But in practice it’s common to use higher numbers like
    100 or more, and to use dimensionality reduction algorithms like Principal Component
    Analysis or t-SNE to obtain 2-3 dimensional vectors that can be more easily plotted).
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>49
    That’s because these numbers allow you to capture more information as the amount of
    data grows. At this point we won’t discuss this tuning in too much detail, but we will do
    it later in the book as we learn more about neural networks and how they work.
    If we use cosine similarity to measure the distance among each of the generated
    vectors we can find out some interesting results.
    Listing 2.20 word2vec similarity on Aeroplane text
    music -> song, view
    looking -> view, better
    in -> the, like
    sitting -> turning, could
    As you can see we extract the two nearest vectors for a few random vectors, some
    results are good, some not so much.
    music and song are very close terms in semantics; we could even say they are synonyms,
    but it’s not the same for view
    looking and view are just related, better has sort of nothing to do with looking
    in, the and like are not so close to each other
    sitting and turning are both verbs in the ing form but their semantic is loosely coupled,
    could is still a verb but has not much more to do with sitting
    So what’s the problem here; is word2vec not up to the task?
    There are two things at play here:
    the number of dimensions (2) of the generated word vectors (or word embeddings) is
    probably too low
    feeding the word2vec model with the text of just a single song probably doesn’t provide
    enough contexts to each of the words to come with an accurate representation; the model
    needs more examples of the contexts in which the words better and view occur.
    Let’s assume we again build the word2vec model, this time by using 100 dimensions
    and a larger set of song lyrics taken from the Billboard Hot 100 Dataset 13:
    Footnote 13mkaylinwalker.com/50-years-of-pop-music/
    Listing 2.21 word2vec similarity with 100 dimensions and a larger dataset
    music -> song, sing
    view -> visions, gaze
    sitting -> hanging, lying
    in -> with, into
    looking -> lookin, lustin
    We can see now that the results are much better and very appropriate: we can use
    almost all of them as synonyms in the context of search. You can imagine using such a
    technique either at query or indexing time. There would be no more dictionaries or
    vocabularies to keep up to date; the search engine could learn to generate synonyms from
    the data it handles.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>50
    A couple of questions you might have right about now: How does word2vec work?
    And, How can I integrate it, in practice, in my search engine ? The original paper 14 from
    Tomas Mikolov and others describes two different neural network models for learning
    such word representations: one is called Continuous Bag of Words and the other is called
    Continuous Skip-gram Model. Word2vec performs an unsupervised learning of word
    representations, which is very good; these models just need to be fed with a sufficiently
    large text, properly encoded. The main concept behind word2vec is that the neural
    network is given a piece of text, which is split into fragments of a certain size (also called
    window). Every such fragment is fed to the network as a pair of target word and context.
    In the case below the target word is aeroplane while the context is composed by the
    words music, is, my.
    Footnote 14marxiv.org/pdf/1301.3781.pdf
    Figure 2.9 Feeding word2vec (skip gram model) with text fragments
    The hidden layer of the network contains a set of weights (in the case above, 11, the
    number of neurons in the hidden layer) for each of the words. These vectors will be used
    as the word representations when learning ends.
    So an important trick about word2vec is that we do not care too much about the
    outputs of the neural network. Instead, we simply extract the internal state of the hidden
    layer at the end of the training phase, which will yeld exactly one vector representations
    for each word.
    During such training, a portion of each fragment is used as target word while the rest
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>51
    is used as context. The the case of the Continuous Bag of Words model, the target word
    is used as the output of the network, while the remaining words of the text fragments (the
    context) are used as inputs. It’s opposite in the Continuous Skip-gram Model: the target
    word is used as input and the context words as outputs (as in the example above). In
    practice both work well but skip-gram is usually preferred because it works better with
    unfrequent words.
    For example, given the text she keeps moet et chandon in her pretty cabinet let them
    eat cake she says from the Killer Queen song (by The Queen) and a window of 5, a
    word2vec model based on CBOW will receive a sample for each 5 word fragments in
    there. So for the fragment | she | keeps | moet | et | chandon |, the input will be made of
    the words | she | keeps | et | chandon | and its output will consist of the word moet.
    Figure 2.10 Continuous Bag of Words model
    As you can see from the figure, the neural network is composed of an input layer, a
    hidden layer and an output layer. This kind of neural network-- with just one hidden
    layer-- are called shallow, as opposed to the ones having more than one hidden layer,
    which are called deep neural networks.
    The neurons in the hidden layer have no activation function so they linearly combine
    weights and inputs (multiply each input by its weight and sum all of these results
    together). The input layer has a number of neurons equal to the number of words in the
    text for each word; in fact word2vec requires each word to be represented as an hot
    encoded vector.
    Now let’s see what a hot encoded vector looks like. Imagine we have a dataset with 3
    words [cat, dog, mouse]; we’ll have 3 vectors, each one of them having all the values set
    to zero except one, which is set to 1, and is the one that will identify that specific word.
    Listing 2.22 3 hot encoded vectors
    dog
    cat
    : [0,0,1]
    : [0,1,0]
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>52
    mouse : [1,0,0]
    If we add the word 'lion' to the dataset, hot encoded vectors for this dataset will have
    dimension 4:
    Listing 2.23 4 hot encoded vectors
    lion : [0,0,0,1]
    dog
    : [0,0,1,0]
    cat
    : [0,1,0,0]
    mouse : [1,0,0,0]
    So if you have 100 words in your input text, you will have it such that each word will
    be represented as a 100 dimensional vector. Consequently, in the CBOW model, you’ll
    have 100 input neurons multiplied by the window parameter minus one. So if you have a
    window of 4 you’ll have 300 input neurons.
    The hidden layer instead will have a number of neurons equals to the desired
    dimensionality of the resulting word vectors. So this is a parameter that has to be set by
    whoever sets up the network.
    The size of the output layer is equal to the number of words in the input text, in this example 100. A
    CBOW model for an input text with 100 words, dimensionality equals to 50 and window parameter
    have 300 input neurons, 50 hidden neurons and 100 output neurons.
    For word2vec, CBOW model inputs are propagated through the network by first
    multiplying the hot encoded vectors of the input words by their input to hidden weights;
    you can imagine that as a matrix containing a weight for each connection between an
    input and hidden neuron). Those then get combined (multiplied) with the hidden to
    output weights, producing the outputs, and these outputs are then passed through a
    Softmax function. Softmax 'squashes' a K-dimensional vector (our output vector) of
    arbitrary real values to a K-dimensional vector of real values in the range (0, 1) that add
    up to 1, so that they can represent a probability distribution. Our network is telling us the
    probability of each output word to be selected, given the context (the network input).
    After this forward propagation, the backpropagation learning algorithm adjusts the
    weights of each neuron in the different layers so that it would produce a more accurate
    result for each new fragment.
    After the learning process has been completed for all the text fragments with the
    configured window the hidden to output weights represent the vector representation for
    each word in the text.
    The Continuous Skip-gram Model looks reversed with respect to the CBOW model.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>53
    Figure 2.11 Continuous Skip-gram Model
    The same concepts apply for Skip-gram. The input vectors are hot encoded (one for
    each word) so that the input layer has a number of neurons equal to the number of words
    in the input text. The hidden layer has the dimensionality of the desired resulting word
    vectors, while the output layer has a number of neurons equals to the number of words
    multiplied by the windows size minus one. Using the example we used for CBOW,
    having the text she keeps moet et chandon in her pretty cabinet let them eat cake she says
    and a window of 5, a word2vec model based on the Continuous Skip-gram model will
    receive a first sample for | she | keeps | moet | et | chandon | where the input will be made
    of the word moet and its output will consist of the words | she | keeps | et | chandon |.
    Here’s an example excerpt of word vectors calculated by word2vec on the text of the
    Hot 100 Billboard dataset. It shows just a small subset of words plotted, for the sake of
    appreciating some word semantics being expressed geometrically.
    Figure 2.12 Highlights of Word2vec vectors over Hot 100 Billboard dataset
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>54
    You can notice the expected regularities between me and my with respect to you and
    your. You can also have a look at groups of similar words, or words that at least are used
    in similar context and that therefore are good candidates for synonyms.
    Now that we have learned a bit about how word2vec algorithm works, let’s get some
    code and see it in action. Then we will be able to combine it with our search engine for
    synonym expansion.
    SIDEBAR
    Deeplearning4j
    Deeplearning4j is a deep learning library for the JVM. It has a good adoption
    among the Java people and a not-too-steep learning curve for early
    adopters. It also comes with an Apache 2 license, which is handy if you want
    to use it within a company and include it within its possibly non-open-source
    product. Additionally DL4J has tools to import models created with other
    frameworks such as Keras, Caffe, Tensorflow, Theano, etc.
    2.5.1 Setting up word2vec in Deeplearning4J
    In this book you’ll be using DeepLearning4J in order to implement neural network based
    algorithms, so let’s see how we can use it to setup a word2vec model.
    DL4J has an out-of-the-box implementation of word2vec, based on Continuous
    Skip-gram model. What we need to do is just setup its configuration parameters and pass
    the input text we want to ingest in our search engine.
    Keeping our song lyrics use case in mind, we’re going to feed word2vec with the
    Billboard Hot 100 text file. We want output word vectors of a suitable dimension so we
    set that configuration parameter to 100 and the window size to 5.
    Listing 2.24 DL4J Word2Vec example
    String filePath =
    new ClassPathResource('billboard_lyrics_1964-2015.txt').getFile().getAbsolutePath();
    SentenceIterator iter = new BasicLineIterator(filePath);
    Word2Vec vec = new Word2Vec.Builder()
    .layerSize(100)
    .windowSize(5)
    .iterate(iter)
    .build();
    vec.fit();
    String[] words = new String[]{'guitar', 'love', 'rock'};
    for (String w : words) {
    Collection<String> lst = vec.wordsNearest(w, 2);
    System.out.println('2 Words closest to '' + w + '': ' + lst);
    }
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>55
    read the corpus of text containing the lyrics
    set up an iterator over the corpus
    create a configuration for word2vec
    set the number of dimensions the vector representations should have
    set the window parameter
    set word2vec to iterate over the selected corpus
    obtain the closest words to an input word
    print the nearest words
    We obtain the following output, which sounds good enough.
    Listing 2.25 Word2Vec sample output
    2 Words closest to 'guitar': [giggle, piano]
    2 Words closest to 'love': [girl, baby]
    2 Words closest to 'rock': [party, hips]
    As you can see it’s very straightforward to set up such a model and obtain results in a
    reasonable time (training of the word2vec model took around 30s on a 'normal' laptop).
    Keep in mind that we aim to use this now in conjunction with the search engine, which
    should give us a better synonym expansion algorithm.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>56
    2.5.2 Word2vec based synonym expansion
    Now that we have this powerful tool in our hands we need to be careful! When using
    WordNet we have a constraint set of synonyms so we can’t blow up the index, with our
    word vectors generated by word2vec we may ask the model to return us the closest words
    for each word to be indexed. This might be not acceptable from the performance
    perspective (for both runtime and storage), so we have to come with a strategy for how to
    use word2vec responsibly. One thing we can do is constrain the type of words we send to
    word2vec to get their nearest words. In natural language processing is common to tag
    each word with a part of speech (PoS) which labels which syntactic role it has in a
    sentence. Common parts of speech are NOUN, VERB, ADJ, but also more fine-grained
    ones like NP or NC (proper or common noun). So, for example, we could decide to use
    word2vec only for words whose PoS is either NC or VERB and avoid bloating the index
    with synonyms for adjectives. Another technique would be to have a prior look at how
    informative the document is. A very short text is likely to have a relatively poor
    probability to hit a query because it is composed by a few terms. So we could decide to
    focus more on such documents and be eager to expand synonyms there, rather than in
    longer documents. On the other hand the 'informativeness' of a document doesn’t only
    depend on its size. Therefore, other techniques could be used, such as looking at term
    weights (the number of times a term appears in a piece of text) and skipping those ones
    that have a low weight. Additionally we can only use word2vec results if they have a
    very good similarity score. If we use cosine distance for measuring the nearest
    neighbours of a certain word vector, such neighbours could be too far (a low similarity
    score) but still be the nearest ones. In that case we can decide not to use those
    neighbours.
    A WORD2VEC SYNONYM FILTER WITH LUCENE AND DL4J
    As I explained in chapter 1, a token filter takes the terms provided by a tokenizer and
    eventually performs some operations on these terms, like filtering them out, or, as in this
    case, adding some other terms to be indexed. A Lucene TokenFilter is based on the
    incrementToken API which returns a boolean value, which is false at the end of the token
    stream; implementors of this API would consume this way one token at a time (e.g. by
    filtering or expanding a certain token). Earlier in this chapter you saw a diagram of how
    word2vec synonym expansion works. Before being able to use word2vec we need to
    configure and train it using the data to be indexed. In the song lyrics example, we decided
    to use the Billboard Hot 100 Dataset, which we pass as plain text to the word2vec
    algorithm, as shown in the previous code listing.
    Once we are done with word2vec training, we create a synonym filter that will use the
    learned model to predict term synonyms during filtering. Lucene APIs for token filtering
    require us to implement the incrementToken method. By APU contract this method will
    return true if there’re still tokens to consume from the token stream and false if there’re
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>57
    no more tokens left to consider for filtering. The basic idea is that our token filter will
    return true for all the original tokens and for all the related synonyms that we get from
    word2vec.
    Listing 2.26 A word2vec based synonym expansion filter
    protected W2VSynonymFilter(TokenStream input, Word2Vec word2Vec) {
    super(input);
    this.word2Vec = word2Vec;
    }
    @Override
    public boolean incrementToken() throws IOException {
    if (!outputs.isEmpty()) {
    ...
    }
    if (!SynonymFilter.TYPE_SYNONYM.equals(typeAtt.type())) {
    String word = new String(termAtt.buffer()).trim();
    List<String> list = word2Vec.similarWordsInVocabTo(word, minAccuracy);
    int i = 0;
    for (String syn : list) {
    if (i == 2) {
    break;
    }
    if (!syn.equals(word)) {
    CharsRefBuilder charsRefBuilder = new CharsRefBuilder();
    CharsRef cr = charsRefBuilder.append(syn).get();
    State state = captureState();
    outputs.add(new PendingOutput(state, cr));
    i++;
    }
    }
    }
    return !outputs.isEmpty() || input.incrementToken();
    }
    create a token filter that takes an already trained word2vec model
    implement the Lucene API for token filtering
    add cached synonyms to the token stream (see next code listing)
    only expand a token if it’s not a synonym itself (to avoid loops in expansion)
    for each term find the closest words using word2vec that have an accuracy higher
    than a minimum (e.g. 0.35)
    do not record more than two synonyms for each token
    record the synonym value
    record the current state of the original term (not the synonym) in the token stream
    (e.g. starting and ending position)
    create an object to contain the synonyms to be added to the token stream after all
    the original terms have been consumed
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>58
    The above piece of code traverses all the terms, and when it finds a synonym it puts it
    in a list of pending outputs to expand (the outputs List). We apply those pending terms to
    be added (the actual synonyms) after each original term has been processed, in the code
    below.
    Listing 2.27 Expanding pending synonyms
    ...
    if (!outputs.isEmpty()) {
    PendingOutput output = outputs.remove(0);
    restoreState(output.state);
    termAtt.copyBuffer(output.charsRef.chars,
    output.charsRef.offset, output.charsRef.length);
    typeAtt.setType(SynonymFilter.TYPE_SYNONYM);
    return true;
    }
    get the first pending output to expand
    retrieve the state of the original term, including its text, its position in the text
    stream, etc.
    set the synonym text to the one given by word2vec and previously saved in the
    pending output
    set the type of the term as synonym
    The filter only picks the closest two words to the given term according to word2vec
    having at least an accuracy of 0.35 (which is not that high), for each term passed by the
    tokenizer.
    If we pass the sentence 'I like pleasure spiked with pain and music is my airplane' to
    the filter, it will expand the word airplane with two additional words: airplanes and
    aeroplane.
    Figure 2.13 Token stream after word2vec synonym expansion
    We have applied the word2vec technique so that we use its output results as
    synonyms only if they have an accuracy that is above a certain threshold, as discussed in
    the previous section.
    2.6 Evaluations and comparisons
    As mentioned in the first chapter, you can usually capture some metrics, including
    precision, recall, query with zero results, etc, both before the introduction of query
    expansion and afterwards. It’s also usually good to find out what’s the best configuration
    set for all the parameters of a neural network. A generic neural network has a lot of
    parameters you can adjust :
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>59
    The general network architecture, such as using one or more hidden layers
    The transformations performed in each layer
    The number of neurons in each layer
    The connections between neurons belonging to different layers
    The number of times (also called epochs) the network should read through all the training
    sets in order to reach its final state (possibly with a low error and high accuracy)
    The above also applies to other machine learning techniques. In the case of word2vec
    we can decide:
    the size of the generated word embeddings
    the window used to create fragments for unsupervised training of the models
    whether user CBOW or SkipGram models
    This already gives us a lot of possible parameter settings to try out.
    A method to optimize the parameters while making sure a machine learning model
    performs well enough on data that is different from the one used for training is called
    cross validation. With cross validation the original data set is split into three different
    subsets: a training set, a validation set and test set. The training set is used as the data
    source needed to train the model. In practice you often use it to train a bunch of separate
    models having different settings for the available parameters. The cross validation set is
    used to select the model that has the best parameters. This can be done, for example, by
    taking each input / desired output pair in the cross validation set and see if a model gives
    results equals or close to the desired output, when given that particular input. The test set
    is used the same way as the cross validation set, except that it’s only used by the model
    selected by testing on the cross validation set. The accuracy of results on the test set can
    be considered a good measure of the overall effectiveness of the model.
    2.7 Considerations for production systems
    In this chapter we have seen how to use word2vec to generate synonyms from the data to
    be indexed and searched. Most of existing production systems already contain lots of
    indexed documents though, in such cases it’s often not possible to access the original
    data as it existed before it got indexed. If we take the case of indexing top 100 songs of
    the year to build a search engine over song lyrics, we have to take into account that the
    ranking of the most popular songs change every day, week, month, year. This implies
    that our dataset will likely change over time and therefore if you don’t keep such old
    copies in a separate storage it will hardly possible to build a word2vec model for all of
    the indexed documents (song lyrics) later in time.
    The solution to this problem is to work with the search engine as the primary data
    source. When we setup word2vec using DL4J we fetched sentences from a single file:
    Listing 2.28 Fetching sentences for word2vec from a text file
    String filePath = new ClassPathResource('billboard_lyrics.txt').getFile().getAbsolutePath();
    SentenceIterator iter = new BasicLineIterator(filePath);
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>60
    If we think about an evolving system which keeps ingesting different song lyrics from
    different files daily, weekly or monthly we need to take the sentences directly from the
    search engine. For this reason we will build a SentenceIterator which reads stored values
    from the Lucene index.
    Listing 2.29 FieldValuesSentenceIterator to fetch sentences for word2vec from
    the Lucene index
    public class FieldValuesSentenceIterator implements SentenceIterator {
    private final IndexReader reader;
    private final String field;
    private int currentId;
    public FieldValuesSentenceIterator(IndexReader reader, String field) {
    this.reader = reader;
    this.field = field;
    this.currentId = 0;
    }
    ...
    @Override
    public void reset() {
    currentId = 0;
    }
    }
    the view on the index used to fetch the document values
    the specific field to fetch the values from
    since this is an iterator, the identifier of the current document being fetched
    first document id is always 0
    In the example case of the song lyrics search engine, the text of the lyrics were
    indexed into the field with name 'text'. We therefore fetch the sentences and words to be
    used for training the word2vec model from that field.
    Listing 2.30 Reading sentences from the Lucene index
    Path path = Paths.get('/path/to/index');
    Directory directory = FSDirectory.open(path);
    IndexReader reader = DirectoryReader.open(directory);
    SentenceIterator iter = new FieldValuesSentenceIterator(reader, 'text');
    Once we have set things up we pass this new SentenceIterator to the Word2Vec
    implementation:
    SentenceIterator iter = new FieldValuesSentenceIterator(reader, 'text');
    Word2Vec vec = new Word2Vec.Builder()
    .layerSize(100)
    .windowSize(5)
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>61
    .iterate(iter)
    .build();
    vec.fit();
    Within the training phase the SentenceIterator will be asked to iterate over Strings.
    Listing 2.31 For each document, field values are passed to word2vec for training
    @Override
    public String nextSentence() {
    if (!hasNext()) {
    return null;
    }
    try {
    Document document = reader.document(currentId, Collections.singleton(field));
    String sentence = document.getField(field).stringValue();
    return preProcessor != null ? preProcessor.preProcess(sentence) : sentence;
    } catch (IOException e) {
    throw new RuntimeException(e);
    } finally {
    currentId++;
    }
    }
    @Override
    public boolean hasNext() {
    return currentId < reader.numDocs();
    }
    the iterator has more sentences if the current document identifier is not bigger than
    the number of documents contained in the index
    get the document with the current identifier (only the field we need is fetched)
    get the value of the field 'text' from the current Lucene Document as a String
    return the sentence, eventually preprocessed in the case that we set a preprocessor
    (e.g. to remove unwanted characters / tokens)
    finally increment the document id, for the next iteration
    This way word2vec can be retrained very frequently on existing search engines
    without having to maintain the original data. The synonym expansion filter can be kept
    up to date as the data in the search engine is updated.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>62
    2.8 Summary
    Synonym expansion can be an handy technique to improve recall and make the users of
    our search engine happier
    Common synonym expansion techniques are based on static dictionaries / vocabularies
    which might require manual maintenance or, at least, are often very far from the data they
    are used for
    Word2vec is a neural network based algorithm for learning vector representations for
    words which can be used to find words with similar meanings—or at least that appear in
    similar contexts so that it sounds reasonable to exploit it for synonym expansion too
    Word2vec can provide good results, but we need to manage word senses or part of
    speech when using it for synonyms
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>63
    3
    From plain retrieval to text generation
    In the previous chapter we have learned about feed forward neural networks, in particular
    on a specific model called word2vec and how to use it to have better synonym expansion
    and therefore a more effective search engine. Word2vec uses a specifically designed
    neural network to generate dense vector representations for words. Such vectors can be
    used for calculating the similarity of two words by their vectors' distance, like we did in
    the synonym expansion case, but also as inputs for more complex neural network
    architectures, we’ll see this going forward in this chapter. In practice, it’s common to
    train neural networks to accomplish specific tasks by arranging neuron activation
    functions, layers and their connections in ways that make more sense on the basis of the
    problem at hand, so you will see different neural network architectures being used in the
    rest of this book for different purposes. For example, in the computer vision field, where
    the network inputs are usually images or videos, it’s common to use the convolutional
    neural networks, where each layer has a different and very specific function (e.g.
    convolutional layers, pooling layers, ReLU layers, loose layers, etc.). At the same time,
    the aggregation of these layers allows us to build a deep learning network where pixels
    get incrementally transformed into something more abstract (pixels edges objects etc.
    as we briefly mentioned in chapter 1), we will have a closer look at them in chapter 8. It
    would be theoretically possible to perform the same task by using a normal feed forward
    neural network (requiring a much higher number of neurons) but it would require much
    more time and resources because the model is not specialized for that task and therefore
    it would be much harder to train it to be accurate e.g. for recognizing objects in a certain
    image, given its pixels as inputs.
    In this chapter we’ll learn how to use neural networks to let the search engine
    generate queries similar to the ones entered by users. You may wonder why that could be
    useful: let’s first make a step back into the previous chapter. We have learned that it is
    possible to improve the effectiveness of the search engine in returning relevant
    documents by means of synonym expansion; in that case a term in a query could be
    expanded using one (or more) of its synonyms. Going another step backwards to the first
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>64
    chapter we have also seen how a user can express the same information need in a variety
    of slightly different versions (see Table 1.2) and how even small changes in the way a
    query is written can influence which documents get returned first.
    As an additional example, in the early days of the internet and search engines (late
    '90s) people used to search for 'keywords' only. Such users might have typed movie
    zemeckis future to find something about the movie called 'Back to the Future' by Robert
    Zemeckis. While search engines have evolved and we nowadays can also type queries
    using natural language, a lot of users still rely on keywords when searching. For such
    users, it would be advantageous if the search engine could generate a more proper query
    based on the keywords they type; for example, 'movie Zemeckis future' 'Back to the
    Future by Robert Zemeckis'. Let’s call the generated query 'Back to the Future by
    Robert Zemeckis' an alternative query in the sense that is an alternative (text)
    representation of the information need expressed by the user.
    With all this in mind this chapter will teach you how to give text generation
    capabilities to your search engine so that given a user query it will generate a few
    alternative queries to run under the hood together with the original one, the rationale
    behind this is to have the final query being expressed in more ways without asking the
    user to type all of them.
    Going forward we will learn to generate text in natural language and integrate that
    into a search engine using a very powerful architecture for neural networks which has the
    same flexibility of 'plain' feed forward neural nets and has the advantage of working
    very well when dealing with (possibly long) sequences of inputs and outputs; they are
    called recurrent neural networks or RNNs. We’ll learn how RNNs work, how to tune
    them for generating alternative queries and how such a 'RNN backed search engine' has
    an improved effectiveness in returning relevant results for end users.
    3.1 Information need vs Query : bridging the gap
    In chapter 1 we talked about the fundamental problem for users of how to best express an
    information need. However as a user, do you really want to spend a lot of time thinking
    about how to word the best query? Imagine yourself on your way to work on public
    transport early in the morning, searching for some information from your mobile; you
    don’t have the time, or the brainpower yet, to come up with the best way to interact with
    a search engine. If you ask users if they can explain the information they need in three or
    four sentences, it’s quite likely that you’ll get a positive answer, along with a detailed
    explanation of the specific need, and its detailed context. But if you ask the same person
    to express what they are looking for in a short query of no more than five or six words,
    the chances are quite high that they will not be able to do it because it’s not always easy
    to compress a detailed requirement into a short sequence of words. As search engineers,
    we have to do something to bridge this gap between the user intent and the resulting
    queries.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>65
    3.1.1 Generating alternative queries
    A well known technique to help users write their queries is hinting a 'suggested' text
    while the user is typing the query itself. This lets the user interface of the search engine
    guide the user while writing. The search engine will perform an explicit effort to help the
    user type a 'good' query (we’ll have a detailed look at how it does this in the next
    chapter). Another approach to fill the gap between information need and the user-entered
    query is to post process the query right after it gets into the search engine system but
    before actually executing it. Such a post processing task responsibility is to take the
    query as it was entered by the user and create a new one that is 'better' to some extent.
    Of course better can mean several different things in this context; in this chapter we
    focus on producing a query that carries the same information need in more different ways
    so that it is more likely that
    a relevant document is included in the result set
    more relevant documents are ranked first within the search results
    That is something we usually do manually and incrementally these days; we fire a
    first query about 'latest research in artificial intelligence' then a second one 'what is
    deep learning' then a third one 'recurrent neural networks for search'. The term
    'manually' refers to the fact that in the example above we run a query, look at the results,
    reason about them, write and run another query, look at the results, reason about them,
    and so on until we are satisfied (we got the knowledge we were looking for … or gave
    up).
    Our goal here is to produce a set of alternative queries without any interaction with
    the user. Such queries should be a sounding but different version of the original user
    entered query. To see how this should work, let’s go back to the example of the query
    'movie Zemeckis future.' If we type that phrase in, the search engine will:
    accept the user entered query 'movie Zemeckis future'
    pass the query through the query time analysis chain and produce the transformed version
    of the user query—in this case, assuming we’ve configured a filter to lowercase capital
    letters
    pass the filtered query 'movie zemeckis future' to the recurrent neural network and
    obtain one or more alternative queries as output, like 'Back to the Future by Robert
    Zemeckis'
    transform the original filtered query and the generated alternative query into a form that
    is implementation specific to the search engine (a parsed query)
    run the queries against the inverted indexes
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>66
    Figure 3.1 Alternative query generation
    As you can see from the figure, we will be setting up the search engine to use a neural
    network at search time in order to generate appropriate alternative queries to be added to
    the query entered by the user. We will keep the original query as it was written by the
    user user and add the generated queries as addiotional optional queries. Towards the end
    of the chapter we will discuss how to best use the generated queries more thoroughly.
    Automatic query expansion is the name for the technique of generating (portions of)
    queries under the hood to maximize the number of relevant results for the end user. In
    some sense synonym expansion, which we have seen in chapter 2, is a special case of
    automatic query expansion if we use it at query time only (as in not using it to index
    synonyms, but only to expand synonyms for terms in the query).
    Our goal here is to use this query expansion feature to let the query engine :
    minimize queries with zero results (providing an alternative text representation for a
    certain query is more likely to produce hits on search results)
    improve recall by having results that we would have otherwise missed
    improve precision by giving a boost to those results that match both the original query
    and an alternative query, implying that our alternative queries are close enough to the
    original one
    Query expansion is actually not just for neural networks because this approach can be
    implemented using different algorithms. You could, theoretically, replace the neural
    network in the query expansion model above with a black box. In fact, before the advent
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>67
    of (deep) recurrent neural networks, other approaches existed for generating natural
    language (this is a subfield of natural language processing called natural language
    generation). We will have a brief comparison in the dedicated section at the end of this
    chapter with other methods to witness 'the unreasonable effectiveness of recurrent
    neural networks' 15.
    Footnote 15mkarpathy.github.io/2015/05/21/rnn-effectiveness/
    Before seeing RNNs in action, as in many machine learning scenarios, it’s crucial to
    take a close look at how we train the model, what kind of data we want to use and why.
    As you may recall, in supervised learning we are telling the algorithm how we want the
    model to produce an output with respect to a certain input. This means that the way we
    structure inputs and outputs depends a lot on what we want to achieve. In the next section
    we’ll get a quick tour of three possible ways of preparing the data to be fed into the
    recurrent neural network.
    3.1.2 Data preparation
    The reason why we choose recurrent neural networks to implement query expansion is
    that they are surprisingly good and flexible at learning to generate sequences of text,
    including sequences that do not appear in the training data but that still 'make sense'.
    Additionally RNNs need usually less tuning, compared to other natural language
    generation algorithms that use grammars, Markov chains, etc. All this sounds great, but
    what do we expect to happen when generating alternative queries in practice? In specific,
    how should the generated queries look? As all too often in computer science, the answer
    is … it depends!
    The most important thing to define here is what we want to achieve. If you think
    about the case where a user enters the query 'books about artificial intelligence', we may
    want to provide other queries (or sentences) that carry the same semantic information,
    like 'publications from the field of artificial intelligence' or 'books dealing with the topic
    of intelligent machines'. At the same time we should wonder how much such alternative
    representations would be useful in the case of our search engine; in fact while the above
    possibly generated alternative queries may give zero results in our search engine because
    we could have no documents dealing with the topic of artificial intelligence in general.
    So a perfect but not useful alternative query representation is not what we want to
    generate, we may instead look closely to what the users' queries look like and therefore
    provide alternative representations that are built on such information; we may instead
    make the query generation algorithm drain information rather from the indexed data than
    the user data so that the generated alternative queries better reflect what’s in the search
    engine already (and mitigate the problem mentioned above with an alternative query
    returning no results).
    In real life, you often have access to query logs, which are a flat record of what the
    users have queried over the search engine, with minimal information about the results.
    There’re a lot of insights that can be gained from simply looking at the query logs. For
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>68
    instance we can clearly see when people fail to find what they are looking for, as they fire
    queries similar in meaning. Or we can observe how they switch from searching for one
    topic to another. For the sake of an example, let’s say we are building the search engine
    for a media company that provides political, cultural and fashion news to their users.
    Listing 3.1 Sample query log
    time: 2017/01/06 09:06:41, query:{'artificial intelligence'}, results:{size=10,
    ids:['doc1','doc5', ...]}
    time: 2017/01/06 09:08:12, query:{'books about AI'}, results:{size=1, ids:['doc5']}
    time: 2017/01/06 19:21:45, query:{'artificial intelligence hype'}, results:{size=3,
    ids:['doc1','doc8', ...]}
    time: 2017/05/04 14:12:31, query:{'covfefe'}, results:{size=100,
    ids:['doc113','doc588', ...]}
    time: 2017/10/08 13:26:01, query:{'latest trends'}, results:{size=15,
    ids:['doc113','doc23', ...]}
    ...
    the query 'covfefe' returned 100 results, the first two resulting document
    identifiers are 'doc113' and 'doc588'
    Now, imagine you have to build your training set from the above query log,
    correlating similar queries so that you can build training examples where the input is a
    query and the target output is one or more correlated queries. A training set is a collection
    of examples of inputs associated with desired outputs. In our case, each of those
    examples will consist of one input query and one or more output queries, though we
    assume we just have a huge query log of users activity on the search engine. In practice
    it’s very common to use query logs for such learning tasks because:
    query logs reflect behaviour of actual users on that specific system, therefore the
    resulting model will behave relatively close to the users and the data
    using or generating other data sets might require additional costs while possibly training a
    model that is based on different data, users, domains, etc.
    In the example of the search engine for the media company, let’s imagine that we
    have two related queries: 'men clothing latest trends' and 'Paris fashion week'. We can
    use them interchangeably as input and output for training the neural network. A not
    trivial decision we need to make is how to measure the correlation / similarity of two
    queries. Our 'out of the box' knowledge tells us that the above mentioned queries are
    similar in the sense that the Paris fashion week event has a high influence in clothing
    latest (fashion) trends (be it for either men or women), so we, as humans, may decide to
    set 'men clothing latest trends' as an alternative representation of 'Paris fashion week'
    query. However in this context neither the search engine nor the neural network know
    anything about the topic of fashion, they just see input and output texts and vectors.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>69
    Figure 3.2 Learning from queries
    A line from the query log contains a user entered query associated to its search
    results—more precisely, the document ids of the matching results. But this is not what
    we need. Our training examples have to be composed of an input query and one or more
    output queries that are similar or by some mean correlated to the input one. So before we
    can train the network we need to process the lines of the search log and create our
    training set. This type of work that involves manipulating and tweaking the data is often
    called data preparation or preprocessing tasks. Although they may sound a bit tedious,
    they are crucial for the effectiveness of any associated machine learning task. Let’s now
    have a look at a few different ways of selecting such input and output sequences for the
    neural network to learn to generate alternative queries.
    SELECTING INPUT AND OUTPUT SEQUENCES FROM THE SEARCH LOG
    An important step here for achieving the desired results is to decide the way related
    queries are associated. We have several different options, each of them will yield specific
    side effects on the way the neural network will learn to generate new queries. Here, for
    example, is a short list of a few simple approaches for identifying correlated / similar
    queries:
    correlate the ones that generate similar search result sets
    correlate the ones that come from the same users in specific time windows
    correlate the ones that contain similar search terms
    The first approach will group queries that share a portion of their associated search
    result. For example, from the sample query log mentioned before, we could extract the
    following:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>70
    Listing 3.2 Correlating queries using shared results
    query:{'artificial intelligence'} -> {'books about AI', 'artificial intelligence hype'}
    query:{'books about AI'} -> {'artificial intelligence'}
    query:{'artificial intelligence hype'} -> 'artificial intelligence'
    query:{'covfefe'} -> {'latest trends'}
    query:{'latest trends'} -> {'covfefe'}
    share doc1 and doc5
    share doc5
    share doc1
    share doc113
    share doc113
    By correlating queries having shared documents on our sample search log, we can
    observe that 'latest trends' can generate 'covfefe' and viceversa, the artificial
    intelligence related queries seem to suggest good alternative ones. Let’s just observe that
    'latest trends' refers to a rather relative concept: the latest trends of one day may (or will)
    be significantly different than the ones from the day or the week after. If we assume the
    covfefe trend lasted one week, it would be very bad if the neural network generates
    'covfefe' as an alternative query to 'latest trends' one month after the covfefe thing
    showed up. As the real world outside of a search engine changes we need to take care of
    using data that is up to date or at least avoid potential problems by removing training
    examples that might cause bad results, like in this case.
    A second potential approach relies on the assumption that users search for similar
    things in small time windows, like when you’re searching for 'that specific restaurant I
    went to, but I can’t recall its name', you start performing multiple searches which relate
    to the same information need. The key point of this approach is to identify accurate time
    windows in the query logs so that queries which relate to the same information need can
    be grouped together (regardless of their results). In practice, identifying those search
    'sessions' which relate to the same need is not necessarily a simple task, and it depends
    also on how informative the search logs are. For instance, if the search log is a flat list of
    concurrent anonymous searches for all the users, it would be hard to say which queries
    were performed by a single user. If we instead have information about every user like its
    IP address we can try to identify a 'per topic search session'. Let’s again look at the
    sample search log and assume it comes from a single user, if we look at the time
    information in each line, we can see that the first two queries were run in a two minutes
    time window, the others were instead run very far in time from each other, so we could
    correlate the first two queries 'artificial intelligence' and 'books about AI' and skip the
    others. However in real life people can be doing multiple things 'concurrently', like
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>71
    wanting to get information about a technical topic while going to work but also needing
    information about public transport time tables or traffic on the highways. In such cases it
    is very difficult to distinguish which queries are semantically correlated without looking
    into the query terms, which instead we do in the third outlined approach below (so we
    could mix such approaches together).
    Using similar terms to correlate queries sounds is tricky to implement. On one hand it
    sounds very simple, you can find common terms among the queries in the search log, like
    in the example below:
    Listing 3.3 Correlating queries using search terms
    query:{'artificial intelligence'} -> {'artificial intelligence hype'}
    query:{'books about AI'} -> {}
    query:{'artificial intelligence hype'} -> {'artificial intelligence'}
    query:{'covfefe'} -> {}
    query:{'latest trends'} -> {}
    share artificial and intelligence terms
    share nothing
    share artificial and intelligence terms
    share nothing
    share nothing
    In the above extracted correlations we can see that we have lost some information that
    was instead carried by the query results when compared to correlating by using shared
    results, also the training set is much smaller and poorer. Let’s look for example at the
    'books about AI' example. It is surely related to 'artificial intelligence' and, maybe,
    'artificial intelligence hype'. Here, however, simple term matching fails to capture the
    fact that AI simply shortens artificial intelligence. We can mitigate that by applying
    synonym expansion techniques, as we learned in the previous chapter, requiring an
    additional preprocessing step to generate new search log lines in which synonyms are
    expanded. In the given example if our synonym expansion algorithm can map the term
    'AI' into the composite term 'artificial intelligence', we will have the following input /
    output pairs:
    Listing 3.4 Correlating queries using search terms and synonym expansion
    query:{'artificial intelligence'} -> {'artificial intelligence hype'}
    query:{'books about AI'} -> {}
    query:{'books about artificial intelligence'} ->
    {'artificial intelligence', 'artificial intelligence hype'}
    query:{'artificial intelligence hype'} -> {'artificial intelligence'}
    query:{'covfefe'} -> {}
    query:{'latest trends'} -> {}
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>72
    additional mapping, share artificial and intelligence terms
    With respect to the former results we now have an additional mapping because the
    usage of synonyms generated a new input query 'books about artificial intelligence'
    which didn’t exist in the original search log; while this seems good we have to take care
    that there might be more than one synonym for each term in each query. That’s probably
    the case both with large dictionaries like WordNet or even when using word embeddings
    based similarity (e.g. like word2vec) to expand synonyms. While having more data for
    training neural network is usually good, it has to be of good quality to give good results.
    Let’s not forget that we are in a preprocessing stage required to train a neural network
    that will be used to generate sequences. If we feed it with not sounding sequences (e.g.
    not all the synonyms of a certain word make sense in every possible context!) we’ll have
    bad looking sequences generated by the neural network. So in this specific example if we
    plan to use synonym expansion we should probably not expand on every possible
    synonym; we could instead do it only for the input queries which do not have a
    corresponding alternative query, e.g. 'books about AI' in the previous example.
    SELECTING OUTPUT SEQUENCES FROM THE INDEXED DATA
    If the techniques described so far don’t work well enough on your data, e.g. the user
    entered queries are often giving too few or zero results, you may get some help from the
    indexed data. In many real life scenarios the indexed documents have a title, which is
    usually relatively short. So such a title might be used as a query itself if it is correlated to
    the original input query. Let’s pick again the query 'movie Zemeckis future', if we run it
    over our search engine on movies (e.g. think to the IMDB website) we would probably
    return a document like this:
    title: Back to the Future
    director: Robert Zemeckis
    year: 1985
    writers: Robert Zemeckis, Bob Gale
    stars: Michael J. Fox, Christopher Lloyd, Lea Thompson, ...
    Let’s imagine how this document got retrieved:
    the term movie belongs to a stopword list on a search engine about movies so it didn’t
    match
    the term Zemeckis did match in both fields 'writers' and 'director'
    the term future matched in the title
    If we put ourselves in the shoes of someone who can look into both the queries and
    the results as the user types a query, when we see the user writing 'movie Zemeckis
    future' we would immediately tell that he/she should have typed a query like 'back to the
    future' instead. That’s exactly the type of training examples composed by an input
    ('movie Zemeckis future') and a target output ('back to the future') we can pass to a
    neural network. Therefore we can preprocess the search log so that the target alternative
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>73
    query to be generated by the neural network is exactly the query that would have returned
    the best result. Note that this would likely help mitigating the number of queries with
    zero results, because the hints on alternative queries do not come from the user generated
    queries but rather from the text of relevant documents.
    Figure 3.3 Learning from relevant documents' titles
    We can build our training examples by simply associating a query to the titles of the
    first 2-3 resulting documents in the search log.
    A question might arise here: why don’t we simply use the search engine instead of a
    neural network to generate alternative queries then ? In such a way we would constrain
    the set of alternative queries for a certain input text to what the search engine can do in
    terms of matching. In fact 'movie Zemeckis future' will always give the same set of
    alternative queries if we use the search engine to generate them. In the case of our
    example query that would still work but what if the user types 'movie spielberg future'
    (confusing the movie producer with its director), there wouldn’t be any match for the
    search engine for the term spielberg, in such a case the search engine may return a lot of
    movies Steven Spielberg has directed which involve the term future but not Back to the
    Future. On the other hand a neural network can learn to generate an accurate output text
    from previously unseen input text and still provide the desired result, because (in the case
    of the recurrent neural network) it will look at the sequence of input words to produce its
    output and not at the term matching.
    The key takeaway here is that we may use not just queries to train our neural network,
    as long as the the target output is correlated with the input in a way that we think it’s
    useful for representing an alternative query.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>74
    UNSUPERVISED STREAMS OF TEXT SEQUENCES
    A completely different approach for feeding a recurrent neural network for text
    generation is to perform unsupervised learning over streams of text. As mentioned in
    chapter 1, this is a form of machine learning where nothing gets told to the learning
    algorithm about a good (or bad) output; they just build a model of the data as accurate as
    possible. We will see that this is probably the most surprising way RNNs can learn to
    generate text, as no one is actually telling them what a good output is, they simply learn
    to reproduce good quality text sequences on the basis of the input ones.
    In our search log example what we need to do is to simply take the queries one after
    the other, removing everything else.
    Listing 3.5 Query stream obtained from the search log
    artificial intelligence
    books about AI
    artificial intelligence hype
    covfefe
    latest trends
    As you can see it is just plain text. The only thing we need to take care of is that we
    need to decide how to identify the end of a query; in this case we might use the carriage
    return character (\n) as a delimiter for two consecutive queries. So that the text
    generation algorithm will stop whenever it generates a carriage return. This approach is
    very tempting because it requires almost no preprocessing, the data to be used can come
    from everywhere because it’s just plain text. We’ll see later in this chapter the goods and
    bads of this.
    In summary:
    Performing supervised learning over similar queries gives us the advantage of being able
    to specify what we think are good similar queries. The downside is that the neural
    network effectiveness will be based on how good we are at defining when two queries are
    similar during the data preparation phase.
    We may not want to explicitly specify when two queries are similar, but rather let the
    relevant documents for a query provide the alternative query text. This will make the
    neural network generate alternative queries whose text 'comes' from the indexed
    documents (e.g. the document titles) so it will likely reduce the number of queries with
    few / zero results
    An unsupervised approach considers the stream of queries from the search log as a
    sequence of plausible consecutive words, so very little data preparation is needed. The
    advantage here is that it’s very simple to implement and it can closely capture which
    consecutive queries (and hence topics) users tend to be interested in.
    There’re lots of different alternatives and room for creativity to build new ways to
    generate data that suits the need of your users. The key point is to take care of how you
    prepare data for your system. Assuming that we’ve chosen one of the above approaches,
    we can now start unveiling how recurrent neural networks learn to generate sequences of
    text.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>75
    3.2 Learning over sequences
    In chapter 1 I showed you how a general architecture of a neural network looks, with
    input and output layers at the edges of the network and hidden layers in between. Then in
    chapter 2 we started looking at two less general neural network models (Continuous Bag
    of Words and Skip-gram) used to implement the word2vec algorithm.
    All the architectures discussed so far can be used to model how an input can be
    mapped into its corresponding output. In the case of Skip-gram model, we were mapping
    an input vector representing a certain word to an output vector which was representing a
    fixed number of words.
    Let’s think of a simple feed forward neural network we could use for detecting the
    language used in text sentences, e.g. English, German, Portuguese or Italian. This is
    called a multi class classification task, where the input is a piece of text and the output is
    one of the possible classes (3 or more) assigned to that input. A neural network that can
    perform such a task will have 4 output neurons, one for each class. Only one of them will
    be set to 1 in the output layer to signal that the input belongs to a certain class (e.g. if the
    value of the output neuron 1 is equals to 1 then the input text is classified as English, if
    the output neuron 2 value is 1 then the input text is classified as German, and so on…
    The dimension of the input layer is much trickier to define, if we assume that we
    handle fixed size text sequences then we can design the input layer accordingly. For
    language detection a few words are needed, so let’s assume that we will set up the input
    layer with 9 neurons, one per input word. In practice it would be hardly possible to use
    such 1 to 1 mapping between words and neurons, as we have seen in the hot encoding
    technique described for word2vec, each word is represented as a vector of all zeros
    except 1, whose size is equals to the size of the whole words vocabulary. In this case if
    we were to use hot encoding the input layer would contain 9 * size(vocabulary) neurons,
    anyway as we focus on the problem of fixed size inputs this is not very important in this
    context.
    Figure 3.4 Feed forward neural network with 9 inputs and 4 outputs for language
    detection
    Clearly we have a problem if we have text sequences smaller than 9 words, we need
    to fill them with some fake / empty word. For longer sequences we will have to do
    language detection 9 words at a time. Let’s think of the text of a movie review, the
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>76
    contents might be in a language, e.g. Italian, but deal with a movie whose title is kept in
    its original language, e.g. English. We should split the review text in 9 size sequences, we
    could either get the output Italian or English depending on the portion of text fed into the
    neural network.
    With the above limitation in mind, how can we make a neural network learn from
    sequences of inputs whose size is not known in advance? If we knew the size of each
    sequence we want the network to learn, we could make the input layer long enough to
    keep the entire sequence. This, however, would hurt performance badly in the case of
    long sequences, because learning from a larger input requires more neurons in the hidden
    layer for the network to give accurate results. So this solution wouldn’t scale well.
    Recurrent neural networks can handle unbounded sequences of text by keeping their
    input (and output) layer size fixed, so they are the perfect fit for learning to generate
    sequences of text in our use case of automatically expanding queries.
    3.3 Recurrent neural networks
    In short recurrent neural networks can be thought as neural networks that can remember
    information about its inputs as it processes them, so that the outputs produced by
    subsequent inputs depend also on previously seen ones. At the same time the size of
    input (and output, if it generates sequences) layers is fixed.
    For now this is a bit abstract, but we’ll get to understand how that works in practice
    and why it is important one step at a time. Let’s try to generate text sequences without a
    recurrent neural network, let’s use a feed forward neural network with 5 inputs and 4
    outputs.
    In the language detection example we have decided to use one neuron for each word,
    in practice it is often more convenient to use characters instead of strings. The reason for
    that is that the number of possible words is much larger than the number of available
    characters and it can be easier for the network to learn how to handle all possible
    combinations of 255 characters than all possible combinations of more than 300
    thousands words 16. If you think about the hot encoding technique again a character
    would be represented with 255 sized vector, a word as taken from the Oxford English
    Dictionary would be represented as a 301k sized vector! The neural network input layer
    would need 301k neurons for one word as opposed to 255 neurons for one character. On
    the other hand words represent already a combination of characters that has some
    meaning, at the character level such information is not available and therefore a neural
    network with character inputs will first need to learn to generate meaningful words from
    characters; that is not the case if you use words as inputs. So in the end it’s a tradeoff !
    Footnote 16msee en.wikipedia.org/wiki/Oxford_English_Dictionary
    As an example, when using characters, the sentence 'the big brown fox jumped over
    the lazy dog' can be split in chunks of 5 characters and then feed each input into our
    neural network with 5 input neurons.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>77
    Figure 3.5 Neural network ingesting an input sequence with a fixed input layer of 5
    neurons
    As you can see from the image, we can pass a whole sequence to the network
    whatever the input layer size we have. So it appears that we can do it with 'simple'
    neural networks, and have no need for RNNs …
    Imagine if humans listening to someone talking had to understand what that person is
    telling us by only hearing words composed of 5 characters and forgetting each sequence
    right after the next. So for example, if someone tells you 'my name is Yoda' and you
    only get each of the following sequences, and all without remembering all the others.
    Listing 3.6 five length sequences of the sentence 'my name is Yoda'
    my na
    y nam
    name
    name
    ame i
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>78
    me is
    e is
    is Y
    is Yo
    s Yod
    Yoda
    Now you have to say what you heard…weird! With such a short fixed input we could
    get entire words very rarely, and always detached from the rest of the sentence.
    What it makes us possible to understand the sentence is the fact that each time we
    hear a 5-character sequence, we keep track of what we’ve received right before that.
    Let’s say we have a memory of 10.
    Listing 3.7 five length sequences of the sentence 'my name is Yoda', with
    memory in parenthesis
    my na ()
    y nam (m)
    name (my)
    name (my )
    ame i (my n)
    me is (my na)
    e is (my nam)
    is Y (my name)
    is Yo (my name )
    s Yod (my name i)
    Yoda (my name is)
    This is a huge simplification about how both humans and neural networks work with
    input and memory. But it should be enough to provide the rationale behind the
    effectiveness of RNNs in working with sequences with respect to plain feed forward
    neural networks.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>79
    Figure 3.6 A recurrent neural network
    3.3.1 RNN internals and dynamics
    These special neural networks are called recurrent because, via simple looping
    connections in the hidden layer neurons, the network becomes capable of making
    operations that depend on the current input and the previous state of the network with
    respect to the previous input. Let’s a pick a single neuron in the hidden layer, in a
    recurrent neural network. It looks like this:
    Figure 3.7 A recurrent neuron in the hidden layer of a RNN
    The recurrent neuron combines the signal from the input neuron (the arrow from the
    blue neuron) and a signal stored internally (the looping arrow), which plays the role of
    the memory in the 'Yoda' example.
    As you can see, this single neuron doesn’t simply processes an input transforming it
    into an output given its internal state (the hidden layer weights and activation function). It
    also updates its state as a function of the new input and its current state. This is exactly
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>80
    what the neuron needs to do to learn to relate subsequent inputs. By the term 'relate' we
    mean that during training the network will learn, for example, that characters that form
    meaningful words are more likely to appear nearby.
    Going back to the Yoda example, the RNN would learn that, having seen the
    characters 'Y' and 'o', the most probable character to generate is 'd', because the sequence
    'Yod' has been already seen. This is a significant simplification of the learning dynamics
    of a recurrent neural network, but it should give you a basic overview.
    COST FUNCTIONS
    As in many machine learning algorithms, a neural network learns to minimize the error
    that it commits when trying to create 'good' outputs from inputs. The good outputs we
    provide during training together with the inputs tell the network how much it is wrong
    when then it performs prediction. The amount of such error is usually measured by a cost
    function (also called loss function). And the aim of a learning algorithm is to optimize the
    algorithm parameters (in the case of a neural network, optimize the weights) so that the
    loss (or cost) is as low as possible.
    We mentioned before that a RNN for text generation implicitly learns how likely
    certain sequences of texts are in terms of probabilities. In the Yoda example, the
    sequence 'Yoda' could have probability 0.7 while the sequence 'ode ' may have a
    probability of 0.01. An appropriate cost function would compare the probabilities
    calculated by the neural network (with its current weights) against the actual probabilities
    in the input text—for example the sequence 'Yoda' would have an actual probability of
    about 1 in the example test). This would give the amount of loss (the error). Several
    different cost functions exist, however one that intuitively performs the above
    comparison is called cross entropy cost function, which we’ll use in our RNN examples.
    In short you can think such cost function as measuring how much the probabilities
    calculated by the neural network differ from what they should be with respect to a certain
    output. For example, if a network learning over the Yoda sentence gives that the
    probability of the word 'Yoda' is 0.00000001 it is probably going to have a high loss
    because the 'right' one should be high, since 'Yoda' is one of the few known 'good'
    sequences in the input text.
    Cost functions play a key role in machine learning, as they define the actual goal of
    the learning algorithm. Different cost functions will have to be used for different types of
    problems. For example the cross entropy cost function is useful for classification tasks,
    while mean squared error is another cost function useful when a neural network needs to
    predict real values (e.g. remember the stock market example). The mathematical
    foundations of cost functions would probably require an entire chapter, since the focus of
    this book is on the applications of deep learning for search we’ll not going into more
    details. We’ll however suggest the right cost function to use depending on the specific
    problem we’re solving as we proceed in the rest of the book.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>81
    UNROLLING RNNS
    You may have noticed that the only aesthetic difference between a feed forward and a
    recurrent neural network is in some looping arrows in the hidden layers. As you may
    have realised, the term 'recurrent' refers to such loops.
    A better way to understand and visualize how a recurrent neural network works in
    practice is to unroll it. You can think of unrolling a recurrent neural network into a set of
    finite connected copies of the same network. This is very useful in practice when
    implementing a RNN but it also makes it easier to see how recurrent neural networks
    naturally fit into sequence learning.
    In the Yoda example I said that a memory of 10 characters helps us keep in mind the
    previously entered characters as we see new input ones. A recurrent neural network has
    this capability of keeping track of previous inputs (with respect to context) by means of
    recurrent neurons / layers. If we let the recurrent layer of a RNN 'explode' in a set of 10
    copies of such layer, we would be unrolling the RNN by 10.
    Figure 3.8 An unrolled recurrent neural network reading 'my name is Yoda'
    We are feeding the sentence 'my name is Yoda' to a RNN unrolled for 10 'steps'.
    Let’s focus on the highlighted node (filled with black), you can see that it receives inputs
    by 'its' input (the character 's') and the previous red node in the hidden (unrolled) layer,
    which in turns receives input by the character 'i' and the previous red node in the hidden
    layer; this goes back until the first input. The intuition is that each node receives
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>82
    information about plain input (a character of the sequence) and, backwards, from
    previous inputs and internal states of the network for such previous inputs.
    On the other hand, going forward, you can see that the output to the first character
    ('m') only depends on the input and the internal state (weights) of the network. Instead the
    output to character 'y' depends on the input, the current state and the previous state as it
    was for the first character 'm'.
    So you can think to the unrolls parameter as the amount of 'steps' the network can
    look back in time when generating the output for the current input.
    In practice, when setting up recurrent neural networks we can decide how many steps
    we want to use to unroll the network. The more we have, the better they will be able to
    handle longer sequences, although they will also require more data and more time to
    train. Now we should have got a basic idea of how an RNN handles sequences of inputs
    like text, and also keeps track of past sequences, when generating the values in the output
    layers.
    BACKPROPAGATION THROUGH TIME : HOW RECURRENT NEURAL NETWORKS
    LEARN
    In chapter 2 we have briefly introduced backpropagation, the most widely used algorithm
    for feed forward neural network training. Recurrent neural networks can be though as
    feed forward networks with an additional dimension, time. The effectiveness of RNNs
    stands in their capability of learning to take into account information from previous
    inputs in time correctly. So in RNNs we use a learning algorithm called backpropagation
    through time. It is essentially an extension of simple backpropagation where the number
    of weights to learn is much higher than in plain feed forward neural networks because of
    the loops in the recurrent layer, as they have weights which control how past information
    flows through. We just mentioned the concept of unrolling a RNN. Backpropagation
    through time adjustes also the weights of the recurrent layers, so the more unrolls you
    have, the more parameters will need to be adjusted to have good results.
    The need of unrolling a RNN should be now clearer. It is a way to limit the number of
    recursions that the loop does into a recurrent neuron / layer so that learning and
    predicting is bounded and does not recur indefinitely (which would make it very hard to
    compute the value in a recurrent neuron…).
    3.3.2 Long term dependencies
    Let’s start depicting what our recurrent neural network for generating queries would look
    like. Imagine we have two clearly similar queries, like 'books about artificial
    intelligence' and 'books about machine learning.' (Note that this is a quite simple
    example as the two sequences have exactly the same lengths.)
    One of the first thing to do is decide the size of hidden layers and the number of
    unrolls. In the previous section we’ve learned that the number of unrolls control how
    much the network can look back in time. For that to work properly the network needs to
    be 'powerful enough', which means it needs more neurons in the hidden layer to
    correctly handle the information coming from the past as the number of unrolls grows.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>83
    In fact, the number of neurons in a layer defines the maximum 'power' of the
    network. But it’s also important to note that if you want to have a network with lots of
    neurons (and layers) you’ll need to provide lots of data for the network to perform well in
    terms of the accuracy of the outputs.
    Back to the 'unrolls' problem, it has a relation with so-called long term dependency;
    for example, a sentence where words that are quite distant are highly correlated.
    Listing 3.8 Possibly long term dependencies
    In *2017*, despite the bad luck that cursed the 2016 Finals,
    Golden State Warriors won the *championship* again.
    Reading this phrase, you can easily understand that the word championship refers to
    the year 2017. But a not-so-smart algorithm may link championship to the 2016 year, as
    that is also a likely pair to generate. This algorithm would fail to take into account that
    the word 2016 actually refers to Finals in the incidental sentence. These kinds of
    scenarios, where words may have semantic correlations even though they appear further
    from each other in a sequence of text, is called long term dependency. Depending on the
    data you are dealing with, you may need to take this into account to make an RNN work
    effectively.
    Using more unrolls would help mitigate long term dependency problems, but in
    general you may never know how far two correlated words can be (and note that this
    refers seamlessly to characters, words or even phrases). To fix this problem, researchers
    came up with an improved recurrent neural network architecture called Long short term
    memory network.
    3.3.3 Long Short Term Memory networks
    While a layer in recurrent neural network is composed of a number of neurons with
    looping connections, a Long Short Term Memory network uses four layers for each
    'plain' RNN layer.
    Such layers decide respectively:
    which information should go through the next unroll
    which information should be used to update the values of the LSTM internal state
    which information should be used as the next possible state
    which information to output
    With respect to 'vanilla' RNNs there are many more 'handles' in a LSTM. It’s the
    equivalent of a sound engineer in a recording studio tweaking an equalizer (the LSTM),
    versus simply turning the volume handle (the RNN). An equalizer is much more complex
    to operate, but if you tune it correctly, you can get a much better sound.
    There’s a lot more to know about LSTMs, but the key point here is that they perform
    extremely well with long term dependencies, and therefore are a very good fit for our use
    case of generating queries.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>84
    3.4 LSTM networks for unsupervised text generation
    In deeplearning4j we can use an out-of-the-box implementation of Long Short Term
    Memory Networks (based on the work done by Alex Graves for his PhD thesis) 17 .
    Footnote 17mwww.cs.toronto.edu/~graves/phd.pdf
    We will now set up a simple neural network configuration for a recurrent neural
    network with one hidden LSTM layer.
    Let’s build an RNN which can sample text outputs of 50 characters. Though this is
    not a very long sequence, it should be enough to handle short text queries (e.g. 'books
    about artificial intelligence' is 35 characters).
    The unrolling parameter should be ideally larger than the target text sample (output)
    size, so that you can handle longer sequences of input. This code will configure a
    recurrent neural network with 50 neurons in the input and output layers, 200 neurons in
    the hidden (recurrent) layer, unrolling it 10 time steps.
    Listing 3.9 A sample LSTM configuration
    int lstmLayerSize = 200;
    int sequenceSize = 50;
    int unrollSize = 10;
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .list()
    .layer(0, new LSTM.Builder()
    .nIn(sequenceSize)
    .nOut(lstmLayerSize)
    .activation(Activation.TANH).build())
    .layer(1, new LSTM.Builder()
    .nIn(lstmLayerSize)
    .nOut(lstmLayerSize)
    .activation(Activation.TANH).build())
    .layer(2, new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
    .activation(Activation.SOFTMAX)
    .nIn(lstmLayerSize)
    .nOut(sequenceSize).build())
    .backpropType(BackpropType.TruncatedBPTT)
    .tBPTTForwardLength(unrollSize).tBPTTBackwardLength(unrollSize)
    .build();
    the no. of neurons in the hidden (LSTM) layer
    the no. of neurons in the input and output layers
    the no. of unrolls for the RNN
    the input layer is declared with 50 inputs (nIn) and 200 outputs (nOut), using tanh
    activation function
    the hidden LSTM layer is declared with 50 inputs (nIn) and 50 outputs (nOut),
    using tanh activation function
    the output layer is declared with 200 inputs (nIn) and 50 outputs (nOut), using
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>85
    softmax activation function, the cost function is declared here too
    the 'time dimension' of the RNN (LSTM) is declared here with the unrollSize as a
    parameter of the backpropagation through time algorithm
    It’s important to note a few more details about our architecture:
    we have specified the loss function parameter for the cross entropy cost function
    we use the tanh activation function on input and hidden layers
    we use a softmax activation function in the output layer
    The usage of cross entropy cost function is closely tight to the usage of softmax
    function in the output layer. A softmax function in the output layer takes all its incoming
    signals and for each of them it transforms it into an estimated probability with respect to
    the other signals, generating a so called probability distribution, where the sum of all the
    resulting values is equals to 1 and each such value is between 0 and 1.
    In the context of character level text generation we will have one neuron for each
    character seen in the data used to train the network. Once the softmax function is applied
    to the values generated by the hidden LSTM layer we will have that each character will
    have an assigned probability (a number between 0 and 1). Let’s recall the 'Yoda
    example', we have 10 characters in that data, so the output layer will contain 10 neurons.
    The softmax function will make the output layer contain a probability for each such
    character, for example:
    Listing 3.10 softmax generated probabilities
    m -> 0.031
    y -> 0.001
    n -> 0.022
    a -> 0.088
    e -> 0.077
    i -> 0.063
    s -> 0.181
    y -> 0.009
    o -> 0.120
    d -> 0.408
    As you can see the most probable character comes from the neuron associated with
    the character 'd' (probability = 0.408).
    Let’s now pass some sample text to this LSTM network and see what it learns to
    generate from it. Before jumping to generate text for our queries, though, let’s first try it
    with something simpler for us to understand. This will help us make sure that the network
    is doing its job correctly. To do this, let’s take some text written in natural language,
    specifically pieces of literature taken from the Gutenberg project 18 such as … Queen.
    This is mere madness; And thus a while the fit will work on him… . We are going to teach
    the RNN to (re)write Shakespeare poems and comedies!
    Footnote 18mwww.gutenberg.org
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>86
    Figure 3.9 generating Shakespeare text
    This will be our first experience with RNNs so it’s good to start with the simpler
    possible approach to train it. We’ll perform unsupervised training of the network by
    simply feeding the text from Shakespeare works one line at a time. This can be seen in
    the figure below (input and output layer size set to 10 for the sake of readability), as we
    go through the text of Shakespeare’s works, we take excerpts of unroll size + 1 then we
    feed them one character at a time in the input layer, the expected result in the output layer
    is the next character in the input excerpt, so that if we take the sentence work on him
    we’ll see the inputs receiving characters for work on hi and the corresponding outputs be
    ork on him. This way we train the network to generate the next character, by also looking
    back at the previous 10 characters.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>87
    Figure 3.10 Feeding the unrolled RNN with unsupervised sequence learning
    In the previous code listing we have configured our LSTM, now we will train it
    iterating over the character sequences from the Shakespeare texts. First we initialize the
    network with the configuration defined above.
    Listing 3.11 Initialize a neural network given a configuration
    MultiLayerNetwork net = new MultiLayerNetwork(conf);
    net.init();
    In order to train the network we need to iterate through the Shakespeare text. As
    mentioned we are building a RNN that generates text sequences one character at a time,
    therefore we will use a DatasetIterator (the DL4J API for iterating over datasets) that
    creates character sequences, a CharacterIterator 19. We can skip some of the details
    regarding the CharacterIterator, we simply initialize it with:
    Footnote 19msee github.com/deeplearning4j/../CharacterIterator.java
    the source file that contains the text to perform unsupservised training
    the number of examples that should be fed together into the network before it updates its
    weights (called the mini batch paramenter)
    the length of each example sequence
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>88
    Listing 3.12 Iterator over Shakespeare text characters
    CharacterIterator iter = new CharacterIterator('/path/to/shakespeare.txt',
    miniBatchSize, exampleLength);
    Now we have all the pieces of the puzzle to train the network. Training of a
    MultiLayerNetwork is done with the fit(Dataset) method, as in the code listing below:
    MultiLayerNetwork net = new MultiLayerNetwork(conf);
    net.init();
    net.setListeners(new ScoreIterationListener(1));
    while (iter.hasNext()) {
    net.fit(iter);
    }
    we can set some listeners so that we can have look into the training (e.g. for
    checking that the loss is going down over time)
    we iterate over the dataset content
    we train the network on each portion of the dataset
    We want to check that the value of the loss generated by the network during training
    steadily goes down over time. This is very useful as a sanity check, a neural network with
    good settings (learning rate, number and size of (hidden) layers, etc.) will see this number
    going down.
    Listing 3.13 Checking loss value
    11:53:33.223 [main] INFO
    11:53:33.328 [main] INFO
    11:53:33.671 [main] INFO
    11:53:33.767 [main] INFO
    11:53:34.150 [main] INFO
    11:53:34.247 [main] INFO
    11:53:34.584 [main] INFO
    11:53:34.689 [main] INFO
    11:53:35.018 [main] INFO
    11:53:35.119 [main] INFO
    11:53:35.455 [main] INFO
    o.d.o.l.ScoreIterationListener -
    Score at iteration 46 is 4176.819462796047
    o.d.o.l.ScoreIterationListener -
    Score at iteration 47 is 3445.1558312409256
    o.d.o.l.ScoreIterationListener -
    Score at iteration 48 is 3930.8510119434372
    o.d.o.l.ScoreIterationListener -
    Score at iteration 49 is 3368.7542747804177
    o.d.o.l.ScoreIterationListener -
    Score at iteration 50 is 3839.2150762596357
    o.d.o.l.ScoreIterationListener -
    Score at iteration 51 is 3212.1088334832025
    o.d.o.l.ScoreIterationListener -
    Score at iteration 52 is 3785.1824493103672
    o.d.o.l.ScoreIterationListener -
    Score at iteration 53 is 3104.690257065846
    o.d.o.l.ScoreIterationListener -
    Score at iteration 54 is 3648.584794826596
    o.d.o.l.ScoreIterationListener -
    Score at iteration 55 is 3064.9664614373564
    o.d.o.l.ScoreIterationListener -
    Score at iteration 56 is 3490.8566755252486
    You can see in the logs above that in 10 iterations the loss went from 4176 to 3490,
    even if it did it with some ups and downs in between. If you plot the score / loss of more
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>89
    such values (e.g. 100) you may see something like :
    Figure 3.11 Plotting loss trend
    Let’s see a few sequences (of 50 characters each) generated by the above RNN after a
    few minutes of learning:
    …o me a fool of s itter thou go A known that fig..
    ..ou hepive beirel true; They truth fllowsus; and..
    ..ot; suck you a lingerity again! That is abys. T…
    ..old told thy denuless fress When now Majester s…
    While we can recognize that the grammar is not too bad, and some portions may even
    make sense, we can clearly see that this is not something of good quality. You probably
    would not want to use this network to make it write a query in natural language for an
    end user, given its poor outcomes. A complete example of Shakespeare text generation
    with a similar LSTM (with 1 hidden recurrent layer) can be found within DL4J examples
    project 20.
    Footnote 20mgithub.com/deeplearning4j/../GravesLSTMCharModellingExample.java
    One good thing about recurrent neural networks is that it’s been demonstrated 21 that
    adding more hidden layers often improves the accuracy of generated results. This means
    that, given that enough data is available, increasing the number of hidden layers can
    make deeper recurrent neural networks work better. To see if this applies in our use case,
    let’s build a LSTM network with 2 hidden layers:
    Footnote 21marxiv.org/abs/1312.6026
    Listing 3.14 Configuring a LSTM with 2 hidden layers
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .list()
    .layer(0, new LSTM.Builder()
    .nIn(sequenceSize)
    .nOut(lstmLayerSize)
    .activation(Activation.TANH).build())
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>90
    .layer(1, new LSTM.Builder()
    .nIn(lstmLayerSize)
    .nOut(lstmLayerSize)
    .activation(Activation.TANH).build())
    .layer(2, new LSTM.Builder()
    .nIn(lstmLayerSize)
    .nOut(lstmLayerSize)
    .activation(Activation.TANH).build())
    .layer(3, new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT)
    .activation(Activation.SOFTMAX)
    .nIn(lstmLayerSize)
    .nOut(sequenceSize).build())
    .backpropType(BackpropType.TruncatedBPTT)
    .tBPTTForwardLength(unrollSize).tBPTTBackwardLength(unrollSize)
    .build();
    in this new configuration we added a second hidden LSTM layer identical to the
    first one
    With the above configuration, we train again the neural network using the same
    dataset, so the code for training remains the same above. One thing to note is how we
    generate the output text from the trained network. Being this a recurrent neural network,
    we use DL4J API network.rnnTimeStep(INDArray) which takes an input vector,
    produces an output vector using the previous RNN state and then updates it. A further
    call to rnnTimeStep will use this previously stored internal state to produce the output.
    As previously discussed the input to our RNN is a sequence of characters, each of
    such characters is represented in hot encoded manner. So our Shakespeare text contains
    255 distinct characters, a character input will be represented by a 255 sized vector whose
    values are all set to zero except one, having value equals to 1 (each position corresponds
    to a character, so setting the vector value at a certain position to 1 means such an input
    vector represents that certain character). The output generated by the RNN with respect
    to such an input will be a probability distribution, because of the usage of the softmax
    activation in the output layer. Such a distribution will tell us which characters are more
    probable to be generated as response to the corresponding input character (and previous
    ones, as per information stored in the RNN layer). A probability distribution is like a
    mathematical function that can output all possible characters, but some with more
    probability than others. For example, in a vector generated by an RNN trained over the
    'my name is Yoda' sentence, the character 'y' is more probable to be generated by such
    distribution than the character 'n' when the previous input character is 'm' (and hence the
    sequence my is more probable than the mn one). Such a probability distribution is used to
    generate the output character.
    We first convert an initialization character sequence (e.g. a user query) to a sequence
    of character vectors.
    Listing 3.15 Hot encoding a character sequence
    INDArray input = Nd4j.zeros(sequenceSize, initialization.length());
    char[] init = initialization.toCharArray();
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>91
    for (int i = 0; i < init.length; i++) {
    int idx = characterIterator.convertCharacterToIndex(init[i]);
    input.putScalar(new int[] {idx, i}, 1.0f);
    }
    create an input vector of the required size
    iterate over each character in the input sequence
    get the 'index' of each character
    create an hot encoded vector for each character, having the value at position
    'index' set to 1
    For each character vector, we generate an output vector of character probabilities and
    convert it into an actual character by sampling (extracting a probable result) from the
    generated distribution.
    INDArray output = network.rnnTimeStep(input);
    int sampledCharacterIdx = sampleFromDistribution(output);
    char c = characterIterator.convertIndexToCharacter(sampledCharacterIdx);
    predict probability distribution over the given input character (vector)
    sample a probable character from the generated distribution
    convert the index of the sampled character to an actual character
    In the Shakespeare text we initialize the input sequence with a random character and
    then see how the RNN generates subsequent characters. With the text generation part
    covered we can now see that having two hidden LSTM layers give better results:
    … ou for Sir Cathar Will I have in Lewfork what lies …
    … , like end. OTHELLO. I speak on, come go’ds, and …
    … , we have berowire to my years sword; And more …
    … Oh! nor he did he see our strengh …
    … WARDEER. This graver lord. CAMILL. Would I am be …
    … WALD. Husky so shall we have said? MACBETH. She h …
    The generated text looks more accurate than the one generated with the first 1 hidden
    layered LSTM, as expected. At this point you may be wondering what would happen if
    we added another hidden LSTM layer. Will results be even better? How many hidden
    layers should a perfectly fit network have? We can easily answer the first question by
    trying things out with a network having 3 LSTM hidden layers. It’s harder, if possible at
    all, to come with an accurate response for the second question. In fact, finding the best
    architecture and network settings is a complex process, some more details that regard
    RNNs can be found towards the end of this chapter when talking about using them in
    production.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>92
    Using the very same configuration above, but with an additional (third) hidden LSTM
    layer, we have samples like the ones below:
    … J3K. Why, the saunt thou his died There is hast …
    … RICHERS. Ha, she will travel, Kate. Make you about …
    … or beyond There the own smag; know it is that l …
    … or him stepping I saw, above a world’s best fly …
    Given the parameters we set in the above neural network (layer size, sequence size,
    unroll size, etc.), adding a fourth hidden LSTM layer would not make the results better.
    In fact they’d be slightly worse (e.g. … CHOPY. Wencome. My lord 'tM times our
    mabultion …). The reason for this is that adding more layers means adding power but
    also complexity to the network. Therefore training requires more and more time and data;
    sometimes it’s not possible to generate better results 'just' by adding another hidden
    layer. In the last chapter we’ll discuss a few techniques for how to address this balance
    between the needs of computational resources (cpu, data, time) and results accuracy in
    practice.
    3.4.1 Unsupervised query expansion
    Now that we have seen how a recurrent neural network based on LSTMs works in the
    case of literature text, let’s finally assemble one such network for alternative query
    generation. In the literature example we just passed the text to the RNN (unsupervised
    learning) because that was the simplest way to understand and visualize how such
    networks work. So let’s look at using this same approach for the query expansion topic.
    We can try that on publicly available resources like the web09-bst Dataset 22 which
    contains queries from actual information retrieval systems. We expect that our RNN will
    learn to generate queries similar to the ones that can be contained in a search log, one per
    line. Consequently, our data preparation task consists of grabbing all the queries from the
    search log and writing them in a single file one after the other.
    Footnote 22mboston.lti.cs.cmu.edu/Data/web08-bst/planning.html
    So, again from the query log :
    query:{'artificial intelligence'}, results:{size=10, ids:['doc1','doc5', ...]}
    query:{'books about AI'}, results:{size=1, ids:['doc5']}
    query:{'artificial intelligence hype'}, results:{size=3, ids:['doc1','doc8', ...]}
    query:{'covfefe'}, results:{size=100, ids:['doc113','doc588', ...]}
    query:{'latest trends'}, results:{size=15, ids:['doc113','doc23', ...]}
    ...
    the query part consists of 'artificial intelligence'
    the query part consists of 'books about AI'
    Using only the query part of each line, we’ll get a text file like :
    artificial intelligence
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>93
    books about AI
    artificial intelligence hype
    covfefe
    latest trends
    ...
    Once we have that, we can pass it to a LSTM network like the one above described in
    the previous section. The number of hidden layers depends on different constraints
    however two is usually a good starting value. As described in the graph in the first
    section, we decided to build our query expansion algorithm within a query parser, so that
    the user is not exposed to this alternative query generation. For the sake of our example
    we extend a Lucene QueryParser whose responsibility is to build a Lucene Query from a
    String (a user entered query in this case).
    Listing 3.16 Lucene query parser for automatic query expansion
    public class AltQueriesQueryParser extends QueryParser {
    private final MultiLayerNetwork rnn;
    private CharacterIterator characterIterator;
    public AltQueriesQueryParser(String field, Analyzer a,
    MultiLayerNetwork rnn, CharacterIterator characterIterator) {
    super(field, a);
    this.rnn = rnn;
    this.characterIterator = characterIterator;
    }
    @Override
    public Query parse(String query) throws ParseException {
    BooleanQuery.Builder builder = new BooleanQuery.Builder();
    builder.add(new BooleanClause(super.parse(query), BooleanClause.Occur.MUST));
    String[] samples = sampleFromNetwork(query);
    for (String sample : samples) {
    builder.add(new BooleanClause(super.parse(sample), BooleanClause.Occur.SHOULD));
    }
    return builder.build();
    }
    private String[] sampleFromNetwork(String query) {
    // where the magic happens ...
    }
    }
    a query parser will translate a String into a parsed query to be run against the
    Lucene index
    the RNN used by the custom query parser to generate alternative queries
    we initialize a Lucene boolean query to contain the original user entered query and
    the optional ones created by the RNN
    add a mandatory clause for the user entered queries (the results for such a query
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>94
    need to be shown)
    let the RNN generate some samples to be used as additional queries
    parse each text generated by the RNN and include it as an optional clause
    build and return the final query as a combination of the user entered query and the
    RNN generated ones
    this method does query encoding, RNN prediction and output decoding into a new
    query, like in the Shakespeare example
    We initialize the query parser with the recurrent neural network and use it to build a
    number of alternative queries which get added as optional clauses appended to the
    original one. All the 'magic' is contained in the portion of the code which generates new
    query strings from the original one.
    The RNN receives the user entered query as an input, and then produces a new query
    as output. Remember, neural networks 'talk' by means of vectors, so we need to
    transform the text query into a vector. We perform hot encoding of the characters of the
    user entered query. Once the input text is converted into a vector we can sample the
    output query one character at a time. Looking back at the Shakespeare example we:
    encode the user entered query into a series of hot encoded character vectors
    feed this sequence to the network
    get the first output character vector, transform it into a character and feed this generated
    character back into the network
    iterate this last step until an ending character is found (e.g. the carriage return character in
    our case)
    Practically this means that if we feed the RNN with a user entered query that is
    something like a very common term, it’s likely that our RNN will 'complete' the query
    by adding some relevant terms. If we instead we feed a query which looks like a
    'finished query', the RNN will likely generate a query that we could find near the user
    entered query in a search log. With all this in place, we can now generate alternative
    queries, with the following settings:
    Listing 3.17 Trying out the AltQueriesParser using a LSTM with 2 hidden layers
    int lstmLayerSize = 150;
    int miniBatchSize = 10;
    int exampleLength = 50;
    int tbpttLength = 40;
    int numEpochs = 1;
    int noOfHiddenLayers = 2;
    double learningRate = 0.1
    String file = getClass().getResource('/queries.txt').getFile();
    CharacterIterator iter = new CharacterIterator(file, miniBatchSize, exampleLength);
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>95
    MultiLayerNetwork net = NeuralNetworksUtils.trainLSTM(
    lstmLayerSize, tbpttLength, numEpochs, noOfHiddenLayers, iter, learningRate,
    WeightInit.XAVIER,
    Updater.RMSPROP,
    Activation.TANH,
    new ScoreIterationListener(10));
    Analyzer analyzer = new EnglishAnalyzer(null);
    AltQueriesQueryParser altQueriesQueryParser = new AltQueriesQueryParser('text',
    analyzer, net, iter);
    String[] queries = new String[] {'latest trends', 'covfefe', 'concerts', 'music events'};
    for (String query : queries) {
    System.out.println(altQueriesQueryParser.parse(query));
    }
    each hidden LSTM layer number or neurons
    the number of examples to put into a mini batch
    the length of each input sequence to make the RNN learn to generate new ones
    the unroll size (as a parameter of backpropagation through time)
    the number of times the RNN should iterate over the same data
    the number of hidden LSTM layers in the RNN network
    the gradient descent learning rate
    the source file containing the queries
    build an iterator over text characters of the file containing queries
    the algorithm used for initializing the query weights
    the update algorithm used for updating parameters while performing gradient
    descent
    the activation function to be used in the hidden layers
    set up a score iteration listener that outputs the value of loss every 10 iterations (of
    backpropagation through time)
    the analyzer used to identify terms in the query text
    instantiate the AltQueryParser
    create a few sample queries
    print the alternative queries generated by our custom parser
    The standard output will contain the following:
    Listing 3.18 Sampled alternative queries
    latest trends -> (latest trends) about AI, (latest trends) about artificial intelligence
    covfefe -> books about coffee
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>96
    concerts -> gigs in santa monica
    music events -> concerts in california
    The query 'latest trends' gets expanded in a more specific queries involving trends
    about AI; this results in boosting the AI related results
    the second query looks weird (but it actually isn’t from the RNN perspective as the
    chars composing covfefe and coffee are almost the same and almost in the same
    positions).
    The alternative query for 'music events' is a similar query, but more specific. Note
    that there is no term shared between the input and output queries
    The first alternative queries generated sound like more specific versions of the
    original one, that could be definitely not what the user wants. As you can see from the
    latest trends appearing in parenthesis the RNN is generating about AI and about artificial
    intelligence sort of completing the sentence. If you ask a very generic question about
    latest trends, the query parser would rather be a bit cautious in genearing more specific
    versions of an original query, if no more context is given (like in this case, latest trends is
    too generic). So if we don’t want to have alternative queries like the ones for the first
    query, we could do a small trick that will hint the RNN to try to generate a completely
    new query. The data we use to feed the RNN is split into sequences, one per line
    delimited by carriage return; so here’s the trick: we can add a carriage return character at
    the end of the user entered query. The RNN is used to observing sequences of the form
    wordA wordB wordC *CR* (or more precisely character streams often containing a space
    character in between) where CR is carriage return. Implicitly the CR character tells the
    RNN that the sequence of text before CR is finished, but also that a new sequence of text
    is starting… So if we take the user entered query 'latest trends' and let the query parser
    add a CR at the end of it, the RNN will try to generate a new sequence starting from a
    carriage return character. Such a trick makes it much more likely that the RNN will
    generate text that sounds like a new query rather than a more specific version of the
    original query.
    latest trends ->
    3.5 From unsupervised to supervised text generation
    The approach for generating alternative queries that we’ve just seen sounds nice, but we
    want something better than nice; we are focused on providing something that changes
    lives of our end users. We want to make sure that the search engine operates better than
    before, otherwise all this effort would be useless.
    In our query expansion use case, however a key role is defined by the way the
    recurrent neural network learns. We’ve seen how the RNN performs unsupervised
    learning from a text file containing a lot of not directly related user queries. In the data
    preparation section we also mentioned more complex alternatives, where we were
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>97
    creating examples having the desired alternative query with respect to a certain input
    query. In the next sections we’ll briefly introduce supervised text generation for search
    (e.g. using search logs) with two different algorithms.
    3.5.1 Sequence to sequence modeling
    In the previous sections we have learned about LSTMs and how they are good at
    handling sequences. Doing supervised learning for our alternative query generation task
    means giving a desired sequence with respect to an input sequence. In the section where
    we discussed data preparation we have seen that we can obtain such training examples
    deriving them from the search logs. So, if we have pairs like 'latest research in AI'
    'recent publications in artificial intelligence' we can use them in our RNN architecture as
    per picture below.
    Figure 3.12 Supervised sequence learning with single LSTM
    With this input / output pairs it is much harder for the RNN (or LSTM) to learn, in
    fact in the previous unsupervised approach the network was learning to generate the next
    char in the sequence in order to teach the RNN to reproduce the input sequence. In the
    above example instead we are trying to teach the neural network to generate a sequence
    of characters that might be completely different. Let’s make an example to understand
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>98
    this better: if you have the input sequence 'latest resea', it’s very easy to guess that the
    next character will be a 'r', the RNN output to be learned 'just' looks one character ahead
    in time.
    l -> a
    la -> at
    lat -> ate
    late -> ates
    lates -> atest
    latest -> atest
    latest -> atest r
    latest r -> atest re
    latest re -> atest res
    latest res -> atest rese
    latest rese -> atest resea
    latest resea -> atest resear
    On the other hand if you use the portion of sentence 'recent pub' as the target output,
    the RNN should do something like that:
    l -> r
    la -> re
    lat -> rec
    late -> rece
    lates -> recen
    latest -> recent
    latest -> recent
    latest r -> recent p
    latest re -> recent pu
    latest res -> recent pub
    latest rese -> recent publ
    latest resea -> recent publi
    This last task is much more difficult to solve, so we’ll now introduce a very nice and
    fascinating architecture called sequence to sequence models. In such an architecture two
    LSTM networks are used:
    the first one, called the encoder, takes the input sequence as a seqeunce of word vectors
    (and not characters) and generates an output vector, called thought vector, which
    corresponds to the last hidden state of the LSTM, and not a probability distribution like in
    the previously discussed model
    the second one, called the decoder, takes the thought vector as an input and it generates
    an output sequence which represents a probability distribution that will be used to sample
    the output sequence.
    This architecture is also called seq2seq, we will inspect it in more detail in the context
    of chapter 7, as it’s used also for performing machine translation (transforming one
    sequence written in a certain language into a corresponding sequence in another target
    language). Seq2seq is also used for building conversational models for chatbots. In the
    context of search what is also fascinating is the concept of thought vector, a vectorized
    representation of the user intent. There is a lot of research in this area; although it’s called
    a thought vector, what the RNN can learn is based on the given inputs and outputs. So in
    this case if the input is a query and the output is another query, the thought vector in this
    case can be seen as the vector that can map the input query to the output query. If the
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>99
    output query is a relevant one with respect to the input query, then this thought vector
    encodes the information about how a relevant alternative query can be generated from
    that input query, a distributed representation of user intent.
    Figure 3.13 Sequence to sequence modelling for queries
    Since we’ll have a closer look at these sequence to sequence models in chapter 7,
    we’ll be using a previously trained seq2seq model where related input and desired output
    queries have been extracted from a search log on the basis of two measures:
    how close in time they were fired as seen from the search log
    sharing at least one search result
    So in DL4J we load this previously created model from the file system and pass it to
    the previously defined AltQueryParser.
    MultiLayerNetwork net =
    ModelSerializer.restoreMultiLayerNetwork('/path/to/seq2seq.zip');
    AltQueriesQueryParser altQueriesQueryParser =
    new AltQueriesQueryParser('text', new EnglishAnalyzer(null), net, null);
    restore a previously persisted neural network model from a file
    build the AltQueryParser using the neural network implementing the seq2seq
    model, note that we do not need the CharacterIterator anymore
    In order to use the sequence to sequence model we need to change the way we
    generate the sequence. In the unsupervised approach we sampled characters from the
    output probability distributions. In this case we will be instead generating the sequence
    from the decoder LSTM at word level. Here are some results given by the
    AltQueryParser using the seq2seq model.
    museum of contemporary art chicago -> foundation
    joshua music festival -> houston monmouth
    mattel toys -> mexican yellow shoes
    this result looks weird at first, but there’s a foundation running grants for the
    Museum of Contemporary Art in Chicago!
    input query about a music event generates a query which contains a city and the
    name of another event (although the Monmouth Festival takes place in Oregon)
    a query about toys for kids generated a query for mexican yellow shoes, if it’s
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>100
    Christmas it’s a good one (gift for kids and for … someone who may like yellow
    shoes)!
    3.6 Considerations for production systems
    Training RNNs was very tedious a while ago and it was even worse with LSTMs.
    Nowadays we have frameworks like DL4J that can either run on CPUs or on GPUs
    (graphical processing units) or even run in a distributed manner (e.g. via Apache Spark).
    Other frameworks like TensorFlow have dedicated hardware (TPUs, TensorFlow
    Processing Units!) and so on. However it’s not trivial to setup your RNN to work well.
    So you might have to train several different models before being able to come up with
    one that works best on your data. By the way, there’s not just theoretical constraints
    around setting up LSTMs, the data you use to train defines what it can do at test time, e.g.
    when using it on unseen queries.
    In practice it took several hours of trial and error to come up with good settings of the
    different parameters in the unsupervised approach, this process will take less time as you
    get more experienced with the dynamics of LSTMs (and also in general of neural
    networks). Let’s make an example, the Shakespeare example contains sequences that are
    much longer than queries. In fact queries are very short, e.g. on average between 10 and
    50 characters each, while lines from Macbeth can contain something like 300 characters.
    So the example length parameter for the Shakespeare example (200) is longer than the
    one used for learning to generate queries (50).
    Also look at the hidden structures in text, the text coming from Shakespeare comedies
    usually comes with the following pattern: CHARACTER_NAME : SOME_TEXT
    PUNCTUATION CR whereas queries are just sequences of words followed by a carriage
    return. Queries can contain both formal and unformal sentences, with words like
    myspaceeee which can very much 'confuse' the RNN. So while the Shakespeare text
    needed only 1 hidden layer to give ok results, the queries LSTM had to have at least 2
    hidden layers in order to start perform in a way that could be used.
    Also the decision on whether to perform unsupervised LSTM training over characters
    versus sequence to sequence model depends first on the data you have. If you are not able
    to generate good training examples (where the output query is a relevant alternative to the
    input query), you should probably go with the unsupervised approach. Also the
    architecture is lighter and training will likely take less time.
    A key point to take into account is that during training the loss values should be
    tracked in order to make sure that it’s steadily going down. We have seen a graph of the
    loss generated by plotting the values outputted by the ScoreIterationListener in the
    unsupervised LSTM training. That is something very useful to do to make sure training is
    doing good. In fact if the loss starts going up or stops going down at a value far from
    zero, there’s likely something to tune better in the network parameters. The most
    important parameter to care of is the learning rate. Such value (usually between 0 and 1)
    decides the speed at which the gradient descent algorithm goes downhill towards points
    where the error is low. If the learning rate is too high (closer to 1, e.g. 0.9) this will result
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>101
    in the loss starting to diverge (going up to infinity). If the learning rate is instead too low
    (closer to 0, e.g. 0.0000001) the gradient descent may take too much to reach a point with
    low error.
    3.7 Summary
    Neural networks can learn to generate text, even in the form of natural language. This is
    very useful for silently generating queries that are executed together with the user-entered
    ones in order to provide better search results.
    Recurrent neural networks are very helpful for the task of text generation, as they are
    adept at handling possibly long sequences of text.
    Long Short Term Memory networks are an extension of recurrent neural networks that
    can deal with so called long term dependencies. They work better than plain RNNs in
    dealing with natural language text where related concepts / words may sit at significant
    distances in a sentence.
    Providing deeper layers in neural networks can help in cases where the network requires
    more computational power for handling larger data sets and / or more complex patterns.
    Sometimes it is useful to look close at how a neural network is generating its outputs,
    small adjustments (like the CR trick) can make a difference in the quality of the results
    Sequence to sequence models and thought vectors are powerful tools to learn to generate
    sequences from sequences in a supervised manner
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>102
    4
    More sensitive suggestions
    In the previous chapters we have learned about the fundamentals of neural networks,
    some shallow and deep architectures and how we can integrate them into the search
    engine for synonym expansion and alternative query generation. Those techniques both
    work on the search engine to make it smarter but is there something we can do to help the
    user write better queries ?
    It is very common for search engines to provide a so called 'suggestion' or
    'autocomplete' functionality which aids users while typing queries to speed up the
    querying process while suggesting words or sentences which would made up a
    meaningful query. So if a user starts typing 'boo', the autocomplete feature may provide
    the rest of the word the user is likely to be writing: 'book', or a complete sentence which
    starts with 'boo' like 'books about deep learning'. While helping users compose their
    queries is good per se to possibly avoid typos and similar errors, this autocomplete
    functionality gives the search engine the opportunity to 'hint' words or sentences that
    make more sense in the context of the query the user is writing. Therefore it also has an
    impact on the effectiveness of the search engine. Very interestingly the search engine has
    a chance there to favour certain suggestions over other ones, for example for marketing
    purposes.
    Because autocomplete is a very common feature in search engines, there are already
    algorithms that can accomplish such task. So what can neural networks help us with here
    ? In the previous chapter we have seen that (deep) neural networks can learn to generate
    text which looks like it was written by a human. We saw this in the alternative query
    generation use case. In this chapter we will use and extend such neural nets and see how
    they outperform the current most widely used algorithms for autocompletion, generating
    sensitive suggestions.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>103
    4.1 Suggesting while composing queries
    In chapter two we discussed how to help users of a search engine looking for song lyrics,
    in the common scenario in which one doesn’t exactly recall a song title. And in that
    context we introduced the synonym expansion technique in order to allow users to fire a
    possibly incomplete or incorrect query (e.g. music is my aircraft), to get it fixed by
    expanding synonyms under the hood (music is my aeroplane) using word2vec algorithm.
    Synoynm expansion is a very useful technique, but maybe we could have done also
    something simpler to help a user recall that the song chorus reads music is my
    *aeroplane* and not music is my aircraft, by suggesting the right words while the user
    him/herself was typing the query. Instead we let him/her run a suboptimal query—in the
    sense that the user already knew aircraft was not the right word. Having good auto
    completion algorithms has two benefits
    less queries with few / zero results (affects recall)
    less queries with low relevance (affects precision)
    In fact if the suggester algorithm is 'good' it won’t output non existing words or
    terms which never occurred in the indexed data. This means it is very unlikely that a
    query using terms suggested by such algorithm will return no results. Additionally, let’s
    think about the music is my aircraft example. Provided we don’t have synonym
    expansion enabled, there’s probably no song which contains all such terms and therefore
    the 'best' results will contain music and my or my and aircraft with very low relevance to
    the user information need (and hence low score).
    Ideally once we get to write music is my _ the _suggester algorithm will hint us with
    aeroplane e.g. because that’s a sentence that the search engine has already seen
    (indexed). We just touched an important point which plays a key role in having effective
    suggestions: where can suggestions come from ? Most commonly they originate from :
    static (hand crafted) dictionaries of words or sentences to be used for suggestions
    chronology of previously entered queries (e.g. taken from a query log)
    indexed documents as terms for suggestions are taken from possibly different portions of
    the documents (title, main text content, authors, etc.)
    In the rest of this chapter we will have a look at tapping suggestions from the above
    sources by using common techniques from the fields of information retrieval and natural
    language processing. We’ll also see how they compare with suggesters based on neural
    network language models, a long standing NLP technique but implemented through
    neural networks, in terms of features and accuracy of results.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>104
    4.2 Dictionary based suggesters
    Back in the 'old days' when search engine required a lot of hand crafted algorithms, a
    very common approach was to build a dictionary of words that could be used to help
    users type queries. Such dictionaries usually contained important words only, like main
    concepts that were closely related to that specific domain. For example a search engine
    for a music instruments shop may have used a dictionary containing things like guitar,
    bass, drums, piano, etc. In fact it would have been hardly possible to fill the dictionary,
    for example, with all the English words just by compiling it by hand. It is possible
    instead to have such dictionaries build 'themselves' (e.g. using some script) by looking
    into the query logs, get the user entered queries and extract a list of the 1000 (for
    example) more frequent terms. That way we can avoid having misspelled words making
    it into the dictionary, by means of the frequency threshold (hopefully people types
    queries without typos most of the times…). Given the above scenario, dictionaries can
    still be a good resource for query history based suggestions, you can either use that data
    to suggest exactly the same queries or portions of them.
    Let’s take the opportunity here to build a dictionary based suggester using Lucene
    APIs using terms from previous query history. In the rest of the chapter we’ll be
    implementing this same API using different sources and suggestion algorithms; this will
    help us compare them in order to evaluate which one to choose depending on the use
    case.
    4.2.1 Lucene Lookup APIs
    Suggest / autocompletion features are provided by means of the Lookup API in Apache
    Lucene 1. The lifecycle of a Lookup usually include :
    Footnote 1mlucene.apache.org/core/7_0_1/suggest/org/apache/lucene/search/suggest/Lookup.html
    a build phase: where the Lookup is built from a certain data source (e.g. a dictionary)
    a lookup phase: where the Lookup is used to provide suggestions based on a sequence of
    characters (and some other, optional, parameters)
    a rebuild phase: in case the data to be used for suggestion gets updated or a new source
    needs to be used
    a store / load phase: where Lookups are persisted (e.g. for future reuse) and loaded (e.g.
    from a previously saved lookup, e.g. on disk)
    Let’s now build our first Lookup using a dictionary, in particular we will use a file
    containing the 1000 previously entered queries as they were recorded in the search
    engine log. Our queries.txt file will look like this, one query per line:
    ...
    popular quizzes
    music downloads
    music lyrics
    outerspace bedroom
    high school musical sound track
    listem to high school musical soundtrack
    ...
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>105
    We’ll now build a Dictionary from this plain text file and pass it to the Lookup to
    build our first dictionary based suggester.
    Lookup lookup = new JaspellLookup();
    Path path = Paths.get('queries.txt');
    Dictionary dictionary = new PlainTextDictionary(path);
    lookup.build(dictionary);
    instantiate a Lookup
    locate the input file containing the queries (one per line)
    create a plain text dictionary which reads from the queries file
    build the Lookup using the Dictionary
    As you can see the Lookup implementation called JaspellLookup, which is based on a
    ternary search tree, is fed with data coming from a dictionary containing past queries. A
    ternary search tree (or TST) 2 is a data structure where Strings are stored in a way that
    recalls the shape of a tree, in particular a TST is a particular type of trees called prefix
    tree (or trie) where each node in the tree represents a character and has maximum three
    child nodes. Such data structures are particularly useful for autocompletion because
    they’re very efficient in terms of speed when searching for Strings having a certain
    prefix, that’s the reason why prefix trees in general are often used in the context of auto
    completion, because as you search for 'mu' the trie can give you all the Strings in the
    tree that start with 'mu' very efficiently.
    Footnote 2men.wikipedia.org/wiki/Ternary_search_tree
    Now that we’ve built our first suggester, let’s see it in action. Think back to the query
    'music is my aircraft' when we introduced the synonym expansion topic. We’ll split that
    query into bigger sequences and pass them to the lookup to get suggestions, simulating
    the way user types query in a search engine user interface. So we’ll start with 'm', then
    'mu', 'mus', 'musi', etc. and see what kind of results we get from our Lookup based on
    past queries, we generate such 'incremental inputs' by using the code below :
    List<String> inputs = new LinkedList<>();
    for (int i = 1; i < input.length(); i++) {
    inputs.add(input.substring(0, i));
    }
    at each step we create a substring of the original input where the ending index i is
    bigger
    Lucene’s Lookup#lookup API will accept a sequence of characters (the input of the
    user typing the query) and a few other parameters, like if we only want the 'more
    popular' suggestions (e.g. more frequent Strings in the dictionary) and the maximum
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>106
    number of such suggestions to retrieve. Using the above List of incremental inputs we’ll
    generate the suggestions for each such substring.
    List<Lookup.LookupResult> lookupResults = lookup.lookup(substring, false, 2);
    we use the Lookup to obtain maximum 3 results for a given substring (e.g. 'mu'),
    regardless of their frequency (morePopular parameter is set to false)
    We obtain a List of LookupResults, each such result is composed by a key that is the
    suggested String and a value that is the weight of that suggestion, such weight can be
    thought as a measure of how much relevant / frequent the suggester implementation
    thinks the related String is, so its value may vary depending on the Lookup
    implementation used. We’ll show each suggestion result together with its weight:
    for (Lookup.LookupResult result : lookupResults) {
    System.out.println('--> ' + result.key + '(' + result.value + ')');
    }
    If we pass all the generated substrings of 'music is my aircraft' to our suggester we’ll
    get:
    'm'
    --> m(1)
    --> m &(1)
    ----
    'mu'
    --> mu(1)
    --> mu alumni events(1)
    ----
    'mus'
    --> musak(1)
    --> musc(1)
    ----
    'musi'
    --> musi(1)
    --> musi for wish you could see me now(1)
    ----
    'music'
    --> music(1)
    --> music &dvd whereeaglesdare(1)
    ----
    'music '
    --> music &dvd whereeaglesdare(1)
    --> music - mfs curtains up(1)
    ----
    'music i'
    --> music i can download for free no credit cards and music parental advisary(1)
    --> music in atlanta(1)
    ----
    'music is'
    ----
    ...
    no more suggestions
    As you can see we get no more suggestions for inputs beyond 'music i.' Not too
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>107
    good. The reason for that is that we have built a Lookup solely based on entire query
    strings, and we did not provide any means for the suggester to split such text lines into
    smaller text units. Therefore the Lookup was not able to suggest 'is' after 'music:' because
    no previously entered query started with 'music is…'. That’s a very strong limitation. On
    the other hand this kind of suggestions is very handy for 'chronological only'
    autocompletion, so that a user will start seeing previous queries he or she entered in the
    past as soon as he or she start typing a new one. This is not bad per se and can already be
    a good boost for user experience in terms of speed. In fact, if such a user wants to run
    exactly that same query fired e.g. a week before, that would show up by using the above
    described Lookup implementation based on a Dictionary of previously entered queries.
    However we want to do more :
    we want to be able to suggest not just entire strings that the user typed in the past, but
    also single words e.g. that composed past queries (music, is, my, aircraft)
    even with entire query strings, we’d like to be able to suggest such strings even if the user
    starts typing a word that stands in the middle of a previously entered query. Think again
    to 'music is my aircraft.' the method shown above gives results if the query string starts
    with what user is typing, but we’d like to be able to suggest 'music is my aircraft' even if
    the user types 'my a'
    we’d like to suggest also unseen word sequences, so the suggest functionality should be
    able to compose natural language to help users write better sounding queries
    we’d like suggestions to reflect the data from the search engine. It would be extremely
    frustrating for a user if the suggestions would lead to an empty list of results
    help users disambiguate when a query may have different scopes among the possible
    interpretations. Imagine a query like 'neuron connectivity,' which could relate to the
    field of neuroscience as well as to artificial neural networks. It would be very helpful if
    we could give a hint to the user that such a query may possibly hit very different
    domains, and let him filter the results before firing the query
    In the next sections we’ll work on each of these points and see how the usage of
    neural networks allows us to achieve more accurate suggestions when compared to other
    techniques.
    4.3 Analyzed suggesters
    Think to when you’re typing a query on a web search engine: in many cases you don’t
    know upfront the whole query you’re going to write. This might have not been true years
    ago when most of web search was based on keywords, so people had to think in advance
    : 'what are the most important words I have to look for in order to get search results that
    should be relevant?'. That way of searching was also a lot moreof a 'trial and error'
    process than it is today. One thing that has influenced it is that good web search engines
    give good hints while you are typing a query and therefore you type, look at the
    suggestions, select one, then start typing again and look for additional suggestions, select
    another suggestion, etc. Let’s run a simple experiment and look at the suggestions we get
    when searching for 'books about search and deep learning' on Google.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>108
    Figure 4.1 Suggestions for 'book'
    As soon as we type 'book' the results are very generic (as we may expect, since
    'book' can have a lot of different meanings in different contexts). So we get suggestions
    about bookings for going on vacation in some Italian places (Roma, Ischia, Sardegna,
    Firenze, Ponza). At this stage suggestions do not seem very much different in shape from
    what we created with our dictionary based suggester with Lucene in the previous section:
    all suggestions simply start with book. We are not selecting any of such suggestions
    because none is relevant for our intents and we keep typing 'books about sear':
    Figure 4.2 Suggestions for 'books about sear'
    The suggestions start to become more meaningful and closer to our search intent,
    though the first results are not relevant (books about search engine optimization, books
    about searching for identity, books about search and rescue). The fifth suggestion is
    probably the closest. It’s interesting to note that we also got:
    an infix suggestion (a suggestion string that contains new tokens placed infix between
    two existing tokens of the original string). Let’s look at 'books about google search'
    suggestion, the word google stands between about and sear in the query we typed, let’s
    keep this in mind as that’s something we’ll want to achieve, but for now we’ll skip it is as
    we’re not interested in 'google search' results.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>109
    a suggestion which skipped the word about (the last three ones, books search…). Let’s
    also keep this in mind, we can discard some terms from the query while giving
    suggestions
    We select 'books about search engines' suggestion and start typing 'and', getting the
    results below:
    Figure 4.3 Suggestions for 'books about search engines and'
    If we look at the results we probably realise the topic of integration of search engines
    and deep learning does not have much book coverage, as none of the suggestions hints
    'deep learning'. A more important thing to note though is that it seems that the suggester
    has discarded some of the query text to give us hints; if you look at the suggestions box,
    the results all start with 'engine and', however this might only be a user interface thing
    because the suggestions seems to be accurate; they’re not about 'engines' in general,
    they clearly reflect the fact that engine is a search engine. Anyway this also gives us
    another hint, we may want to discard some of the query text as they become longer. For
    the sake of our information need, let’s keep typing:
    Figure 4.4 Suggestions for 'books about search engines and dee'
    The final suggestion makes up for the query we had in mind, actually with a small
    modification: we had 'books about search and deep learning' in mind while we came out
    with 'books about search engines and deep learning'.
    In this experiment we didn’t want to learn how Google search engine implements
    autocompletion, however it was useful to do some observations, reason and decide
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>110
    what’s useful in practice for our own search engine applications. We observed that some
    suggestions had:
    some words removed (books about search engines)
    some infix suggestions (books about google search)
    some prefix possibly removed (books about seemed not to take part in last suggestions)
    All this, and much more, is possible by applying text analysis to the incoming query
    and the data from the dictionary we use to build our suggester. We can, for example,
    remove certain terms by using a stop word filter. Or we can break long queries into
    multiple subsequences, and generate suggestions for each subsequence by using a filter
    that breaks a text stream at a certain length. This fits very nicely with the fact that text
    analysis is heavily used within search engines. Lucene has such a Lookup
    implementation, called AnalyzingSuggester . Instead of relying on a fixed data structure,
    it uses text analysis to let us define what we expect to be done—first during Lookup build
    and secondly when passing a piece of text to the Lookup to get suggestions.
    Analyzer buildTimeAnalyzer = new StandardAnalyzer();
    Analyzer suggestTimeAnalyzer = new StandardAnalyzer();
    Directory dir = FSDirectory.open(Paths.get('suggestDirectory'));
    AnalyzingSuggester lookup = new AnalyzingSuggester(dir, 'prefix',
    buildTimeAnalyzer suggestTimeAnalyzer);
    when we build the Lookup we use a StandardAnalyzer which removes stopwords
    and split tokens on whitespace
    when we look for suggestions we use the same analyzer used at build time
    we need a Directory because the Analazying suggester uses it internally to create
    the required data structures for generating suggestions
    create an AnalyzingSuggester instance
    The AnalyzingSuggester can be created using separate Analyzers for build ans lookup
    times, this allows us to be very creative when setting up the suggester. Internally this
    Lookup implementation uses a finite state transducer, a data structure used in several
    places in Lucene.
    Figure 4.5 A finite state transducer
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>111
    You can think to a finite state transducer as a graph in which each edge is associated
    with a character and optionally a weight. At build time all possible suggestions coming
    from applying the build time Analyzer to the dictionary entries are compiled into a big
    FST. At query time traversing the FST with the (analyzed) input query will produce all
    the possible paths and, consequently, suggestion strings to output. Let’s see what it can
    do with respect to the one based on the ternary search tree.
    'm'
    --> m(1)
    --> .m(1)
    ----
    'mu'
    --> mu(1)
    --> mu'(1)
    ----
    'mus'
    --> musak(1)
    --> musc(1)
    ----
    'musi'
    --> musi(1)
    --> musi for wish you could see me now(1)
    ----
    'music'
    --> music(1)
    --> music'(1)
    ----
    'music '
    --> music'(1)
    --> music by the the(1)
    ----
    'music i'
    --> music i can download for free no credit cards and music parental advisary(1)
    --> music industry careers(1)
    ----
    'music is'
    --> music'(1)
    --> music by the the(1)
    ----
    'music is '
    --> music'(1)
    --> music by the the(1)
    ----
    'music is m'
    --> music by mack taunton(1)
    --> music that matters(1)
    ----
    'music is my'
    --> music of my heart by nicole c mullen(1)
    --> music in my life by bette midler(1)
    ----
    'music is my '
    --> music of my heart by nicole c mullen(1)
    --> music in my life by bette midler(1)
    ----
    'music is my a'
    --> music of my heart by nicole c mullen(1)
    --> music in my life by bette midler(1)
    ----
    'music is my ai'
    ----
    ...
    no more suggestions
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>112
    The previous ternary search tree based suggester stopped providing suggestions
    beyond 'music i' as no entry in the dictionary started with 'music is'. We can see that with
    this analyzed version even though the dictionary is the very same, it keeps suggestions
    coming until there’s at least a token suggestion matching. In the case of 'music is', the
    token 'music' is matching with some suggestions and therefore the related results are
    provided, even though 'is' doesn’t give any suggestion. Even more interestingly as the
    query becomes 'music is my', there are some suggestions which both contain 'music' and
    'my'. However at a certain point, where there’re too many tokens that do not match the
    Lookup stops providing suggestions as they might be too poorly related to the given
    query (starting with 'music is my ai'). This is already a good advance over the previous
    implementation and solves one of the mentioned problems: we can provide suggestions
    based on single tokens, not just on entire strings. We can also enhance things a bit by
    using a slightly modified version of AnalyzedSuggester which works better with 'infix'
    suggestions.
    AnalyzingInfixSuggester lookup = new AnalyzingInfixSuggester(dir, buildTimeAnalyzer,
    lookupTimeAnalyzer, ... );
    By using this infix suggester we get fancier results:
    'm'
    --> 2007 s550 mercedes(1)
    --> 2007 qualifying times for the boston marathon(1)
    ----
    'mu'
    --> 2007 nissan murano(1)
    --> 2007 mustang rims com(1)
    ----
    'mus'
    --> 2007 mustang rims com(1)
    --> 2007 mustang(1)
    We do not get results starting with 'm', 'mu' or 'mus', instead such sequences seem to
    be used to match the most important part of a String, like '2007 s550 mercedes', '2007
    qualifying times for the boston marathon', '2007 nissan murano', '2007 mustang rims
    com'. Another noticeable difference is that token matching can happen in between a
    suggestion, that’s why it’s called infix:
    'music is my'
    --> 1990's music for myspace(1)
    --> words to music my humps(1)
    ----
    'music is my '
    --> words to music my humps(1)
    --> where can i upload my music(1)
    ----
    'music is my a'
    --> words to music my humps(1)
    --> where can i upload my music(1)
    ----
    With the AnalyzingInfixSuggester we managed to get infix suggestions. In fact it
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>113
    takes the input sequence, analyze it so that tokens are created and then suggests matched
    based on prefix matches of any such tokens. However we still have the problems of
    making suggestions closer to the data stored in the search engine, making suggestions
    look more like natural language and be able to better disambiguate when two words have
    different meanings. Now that we have hopefully got deep enough into the problem of
    providing good suggestions, we’ll go into discussing language models, first implemented
    through natural language processing ngrams models and then via neural networks (neural
    language models).
    4.4 Leveraging language models
    In the suggestions we’ve seen in the previous sections we noticed some text sequences
    that made very little sense. For example 'music by the the.' Having fed our suggesters
    with data from previously entered queries, we probably found an entry where that
    particular user made a mistake by typing 'the' twice. Beyond that, we have provided
    suggestions that were composed by the whole query. While it is a good thing in case we
    want to use autocompletion to bring back entire text of previous queries (e.g. this can be
    useful if you search for a book on an online book store), this can’t work well for
    composing new queries.
    In medium to large search engines the search logs may contain a huge number of
    diverse queries, coming up with a good suggester algorithm is very hard because of the
    amount and diversity of such text sequences. Let’s make an example: if you look inside
    the web09-bst dataset 3 you will find queries like : hobbs police department, ipod file
    sharing and liz taylor’s biography. Such queries look good and can be used as sources for
    the suggester algorithm. On the other hand you can also find queries like hhhhh,
    hqwebdev or hhht hootdithuinshithins … you probably don’t want the suggester to
    provide similar suggestions. The problem is not filtering out hhhh, which can be cleared
    up from the dataset by removing all the lines or words which contain more than 3
    consecutive same characters. Filtering out hqwebdev is much harder, if you look at it
    contains the word webdev (shortened version of web developer), prefixed by hq. While
    such query might make sense (e.g. a website with such name exists), we don’t want to
    use over specific suggestions for a general purpose suggester service. So the challenge
    here is to work with very diverse text sequences, some of which may not make sense to
    use because they are too specific and therefore rare. A way to address this is to use
    language models.
    Footnote 3mboston.lti.cs.cmu.edu/Data/web08-bst/planning.html
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>114
    SIDEBAR
    Language models
    In NLP field, language models main task is to predict the probability of a
    certain sequence of text. Probability is a measure of how likely a certain
    event is, and it is ranged between 0 and 1. So if we take the weird query we
    got before-- 'music by the the'-- and pass it to a language model, we’ll get a
    quite low probability (e.g. 0.05). Language models represent a probability
    distribution and therefore can help us predict the likelihood of a certain word /
    character sequence in a certain context. Language models can help us in:
    excluding sequences which are very unlikely (low probability), generating
    unseen word sequences because they manage to capture which sequences
    are most likely (even though they may not actually appear in the text).
    Very commonly language models are implemented by calculating the probabilities of
    ngrams.
    SIDEBAR
    NGrams
    An ngram is a sequence of characters made up by n consecutive units,
    where a unit can be a character ('a','b','c',…) or a word ('music','is','my',…).
    So imagine an ngram language model (using words as units), where n = 2.
    When given the word 'music,' it would look for the most likely (highest
    probability) 2-gram containing 'music' (so a sequence of two words, one of
    which is 'music') and output it: For example 'concert'. At the next iteration, it
    would look at the most likely 2-gram containing 'concert.' This would output a
    word that would be decoupled from 'music' (e.g. 'in'), so that the final
    sentence generated by such a language model would have highly probable
    words, one pair at a time. So the final sentence would be constructed in a
    way that is based on the probability of adjacent words (or characters), not on
    entire, previously seen, text sequences (like in the previous suggesters).
    As an implementation note, the probability of a (sequence of) n-gram for a language
    model can be calculated in a number of ways. Most of them rely on the Markov
    assumption that the probability of some future 'event' (e.g. the next character or word)
    depends only on a limited history of preceding 'events' (characters or words). So if you
    use an ngram model with n = 2, also called bigram models, the probability of the next
    word given a current workd (e.g. probability the next word is is given the current word
    music can be written as P(is|music)) is given by counting the number of occurrences of
    the two words music is and dividing such number by the number of occurrences of the
    current word (music) alone. If you want to calculate the probability of a word given a
    certain sequence of words bigger than two, e.g. probability of aeroplane given music is
    my, you will split that sentence in bigrams, calculate the probabilities of all such bigrams
    and multiply them: P(music is my aeroplane) = P(is|music) * P(my|is) * P(aeroplane|my)
    .
    For reference many ngram language models use a slightly more advanced method
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>115
    called stupid backoff foonote:[see section 4 of www.aclweb.org/anthology/D07-1090.pdf
    ] which tries first to calculate the probability of ngrams with a higher N (e.g. N = 3) and
    recursively fallback to smaller ngrams (e.g. N = 2), if an ngram with the current N
    doesn’t exist in the data. Such fallback probabilities get 'discounted' so that probabilities
    from bigger ngrams have more positive influence on the overall probability measure.
    Lucene has ngram based language model Lookup called FreeTextSuggester (using stupid
    backoff algorithm) which requires an Analyzer used to decide how to split ngrams.
    Lookup lookup = new FreeTextSuggester(new WhitespaceAnalyzer());
    Let’s see it in action, with n set to 2, on our previous query 'music is my aircraft':
    'm'
    --> my
    --> music
    ----
    'mu'
    --> music
    --> museum
    ----
    'mus'
    --> music
    --> museum
    ----
    'musi'
    --> music
    --> musical
    ----
    'music'
    --> music
    --> musical
    ----
    'music '
    --> music video
    --> music for
    ----
    'music i'
    --> music in
    --> music industry
    ----
    'music is'
    --> island
    --> music is
    ----
    'music is '
    --> is the
    --> is a
    ----
    'music is m'
    --> is my
    --> is missing
    ----
    'music is my'
    --> is my
    --> is myspace.com
    ----
    'music is my '
    --> my space
    --> my life
    ----
    'music is my a'
    --> my account
    --> my aol
    ----
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>116
    'music is my ai'
    --> my aim
    --> air
    ----
    'music is my air'
    --> air
    --> airport
    ----
    'music is my airc'
    --> aircraft
    --> airconditioning
    ----
    'music is my aircr'
    --> aircraft
    --> aircraftbarnstormer.com
    ----
    ...
    one of the suggestions for 'music is m' matched what we wanted to type (is my) on
    character in advance
    the suggestions for 'music is my ' (my space, my life) are not what we look for, but
    good sounding
    the suggetions for 'music is my ai' are not so good (my aim, air) but closer to what
    we wanted type
    the suggestions for 'music is my airc' caused a match 4 characters in advance
    (aircraft) and some some possibly funny sentence (airconditioning)
    One positive thing is that the language model-based suggester always gave
    suggestions. There was no point when the end user couldn’t rely on suggestions, even if
    they’re not super accurate. That’s a good advantage over previous methods. Most
    importantly we can see the stream of suggestions from 'music' onwards:
    We could have generated queries like 'music is my space' or 'music is my life' or
    even 'music is my airconditioning' which do not appear in the search log with the ngram
    language model. So we reached our goal of being able to generate new sequences of
    words. However, you may have noticed that because of the nature of ngrams—being a
    fixed sequence of tokens—longer queries are not provided with 'full suggestions.' So we
    did not see 'music is my aircraft' in the suggestions in the final stages, but just 'aircraft'.
    This is not bad per se, but it highlights the fact that such ngram language models are not
    very effective in calculating good probabilities for long sentences; therefore they may
    give weird suggestions like 'music is my airconditioning'.
    All that we’ve just learned relates to existing methods for generating suggestions.
    That’s because I wanted you to witness all the issues that affect each of these approaches
    before diving into neural language models, which aggregate many benefits from these
    methods. We’ll now tackle another fundamental issue: we can’t live with static
    dictionaries of suggestions. We already discussed a similar issue when talking about
    synonym expansion: it’s important that we do not have to manually maintain a dictionary
    of 'good' text sequences because that does not scale in large scenarios, is tedious and
    annoying and relies on human supervision. Also, most importantly, we need to have
    suggestions to come from the indexed data too, as the final goal of suggestions is to give
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>117
    the users better search results. We’ll have a look at pulling suggestions from content in
    the next section.
    4.5 Content based suggesters
    With content based suggesters we mean that the content comes directly from the search
    engine. If you think of the search engine for a book shop, it’s very probable that users
    will look for book titles and / or their authors, much more often than they will search
    through the text of a book. Each book indexed has separate fields for title, author(s) and,
    eventually, the text of the book. Also, as new books get indexed and old ones go out of
    production, you need to add such new documents to the search engine and delete the ones
    related to books that can’t be bought anymore. The same has to happen for the
    suggestions, you do not want to miss suggesting brand new titles and you want to avoid
    suggesting book titles for books that are no longer being sold.
    We can directly access the inverted index in the search engine that contains data
    about to the book titles and use those values the same way we used the lines of a static
    dictionary. Since suggester implementations usually require a dedicated data structure or
    model to be built before being used, keeping the suggester up to date might require
    rebuilding it as soon as the search engine gets updated (online or in batched manner). In
    Lucene feeding Lookups with data can be done by using a DocumentDictionary which is
    not a static Dictionary like the one we used to store queries, but reads data from the
    search engine, specifically from an IndexReader (a view on the search engine at a certain
    point in time) using one field for getting the values (to be used for suggestions) and
    another field to get the weights for each of those values (how much important such a
    suggestion would be).
    IndexReader reader = DirectoryReader.open(directory);
    Dictionary dictionary = new DocumentDictionary(reader, 'title', 'price');
    lookup.build(dictionary);
    get a view (an IndexReader) on the search engine
    create a dictionary based on the contents of the 'title' field, let the 'price' decide how
    much weight a suggestion might have
    build the Lookup with the data from the index, same way we built it with a static
    dictionary
    Note how this way we are doing what we described early in this chapter, we guide the
    user to select the search results we hope them to find. As the owner of the book shop
    you’re probably happier if more expensive books are shown more often … Other metrics
    to 'boost' the suggestion might instead be related to ratings, so that a user is suggested
    more often with books that are better rated, improving the chance that user buys a
    well-reviewed book.
    Now that we are all set with getting the data for suggestions from the search engine,
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>118
    we can have a look at neural language models. What we expect from them is to be able to
    mix all the good things from the methods we have just seen with a better accuracy,
    composing queries which would sound like they are typed by a human.
    4.6 Neural language models
    A neural language model is supposed to have the same capabilities of other types of
    language models, such as the ngram models discussed above. The difference lies in how
    they learn to predict probabilities and how much better their predictions are. Some of you
    may have already noticed that in chapter 3 we had introduced a recurrent neural network
    which tried to reproduce text from Shakespeare’s works. At that point we needed to just
    focus on learning how RNNs work but in practice we had already set up a character level
    neural language model! Back then we witnessed that RNNs were very good at learning
    sequences of text also in an unsupervised way, so that they could generate new
    good-sounding sequences based on previously seen ones. A language model learns to get
    accurate probabilities for word sequences so this looks like a perfect fit for recurrent
    neural networks. Let’s start with a simple non deep recurrent neural network
    implementing a character level language model, so that the model will predict the
    probabilities of all the possible output characters, given a sequence of input characters.
    Let’s visualize it:
    LanguageModel lm = ...
    for (char c : chars) {
    System.out.println('mus' + c + ':' + lm.getProbs('mus'+c));
    }
    ....
    musa:0.01
    musb:0.003
    musc:0.02
    musd:0.005
    muse:0.02
    musf:0.001
    musg:0.0005
    mush:...
    musi:...
    ...
    We know that a neural network uses vectors for inputs and outputs, the output layer
    of the RNN we used for text generation in chapter 3 produced a vector holding a real
    number (between 0 and 1) for each possible character to output, such number will
    represent the probability of such a character to be outputted from the network. We’ve alo
    seen this task of generating probability distributions (the probability for all the possible
    characters, in this case) is accomplished by the SoftMax function. Now that we know
    what the output layer does, we put a recurrent layer in the middle whose responsibility is
    to be able to keep memory of previously seen sequences, and an input layer for sending
    input characters to our network. This will result in the following diagram :
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>119
    Figure 4.6 RNN for sequence learning
    With DL4J we already configured such a network in the following way:
    int layerSize = 50;
    int sequenceSize = chars.length();
    int unrollSize = 100
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .layer(0, new LSTM.Builder().nIn(sequenceSize).nOut(layerSize).
    activation(Activation.TANH).build())
    .layer(1, new RnnOutputLayer.Builder(LossFunction.MCXENT).activation(Activation.SOFTMAX)
    .nIn(layerSize).nOut(sequenceSize).build())
    .backpropType(BackpropType.TruncatedBPTT).tBPTTForwardLength(unrollSize).
    tBPTTBackwardLength(unrollSize)
    .build();
    the size of the hidden layer
    the input and output size
    the number of 'unrolls' of the RNN
    While the fundamental architecture is the same (LSTM network with 1 or more
    hidden layers), our goal here is different with respect to what we wanted to achieve in
    chapter 3 (alternative query generation). In fact, we need the RNN to guess a good
    completion of the query the user is writing before he’s finished typing it.
    4.7 Character based neural language model for suggestions
    In chapter 3 we fed the RNN with a CharacterIterator which simply iterated over the
    characters of a file. So far we have built suggestions from text files. As the plan is to use
    the neural network as a tool to help the search engine, the data to feed it should come
    from the search engine itself. Let’s index the Hot 100 Billboard dataset:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>120
    IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig());
    for (String line :
    IOUtils.readLines(getClass().getResourceAsStream('/billboard_lyrics_1964-2015.csv'))) {
    if (!line.startsWith('\'R')) {
    String[] fields = line.split(',');
    Document doc = new Document();
    doc.add(new TextField('rank', fields[0], Field.Store.YES));
    doc.add(new TextField('song', fields[1], Field.Store.YES));
    doc.add(new TextField('artist', fields[2], Field.Store.YES));
    doc.add(new TextField('lyrics', fields[4], Field.Store.YES));
    writer.addDocument(doc);
    }
    }
    writer.commit();
    create an IndexWriter to put documents into the index
    read each line of the dataset, one at a time
    do not use the 'header' line
    each row in the file has the following attributes, separated by a comma: Rank,
    Song, Artist, Year, Lyrics, Source
    index the rank of the song into a dedicated field (with its stored value)
    index the title of the song into a dedicated field (with its stored value)
    index the artist who played the song into a dedicated field (with its stored value)
    index the song lyrics into a dedicated field (with its stored value)
    add the created Lucene document to the index
    persist the index into the file system
    We can use the indexed data to build a char LSTM based Lookup implementation,
    called CharLSTMNeuralLookup. Similarly to what we’ve been doing for the
    FreeTextSuggester we can use a DocumentDictionary to feed the
    CharLSTMNeuralLookup.
    Dictionary dictionary = new DocumentDictionary(reader, 'lyrics', null);
    Lookup lookup = new CharLSTMNeuralLookup(...);
    lookup.build(dictionary);
    create a DocumentDictionary whose content is fetched from the indexed song
    lyrics
    create the Lookup based on the charLSTM
    train the charLSTM based Lookup
    The DocumentDictionary will fetch the text from the field lyrics. In order to
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>121
    instantiate the CharLSTMNeuralLookup we need to pass the network configuration as a
    constructor parameter so that:
    at build time the LSTM will iterate over the characters of Lucene document values and
    learn to generate similar sequences
    at runtime the LSTM will generate characters based on the portion of query already
    written by the user
    Expanding the code listing above, the CharLSTMNeuralLookup constructor requires
    the parameters for building and training the LSTM.
    int lstmLayerSize = 120;
    int miniBatchSize = 40;
    int exampleLength = 1000;
    int tbpttLength = 50;
    int numEpochs = 1;
    int noOfHiddenLayers = 1;
    double learningRate = 0.1;
    WeightInit weightInit = WeightInit.XAVIER;
    Updater updater = Updater.RMSPROP;
    Activation activation = Activation.TANH;
    Lookup lookup = new CharLSTMNeuralLookup(lstmLayerSize, miniBatchSize, exampleLength, tbpttLength,
    numEpochs, noOfHiddenLayers, learningRate, weightInit, updater, activation);
    The implementation of the neural language model is very similar to the one we used
    in chapter 3 for the alternative query generation use case. The basic configuration of a 1
    hidden layer LSTM is defined above and we use the same sampling code to pass typed
    query to the network and get the network output with its probability.
    INDArray output = network.rnnTimeStep(input);
    int sampledCharacterIdx = sampleFromDistribution(output);
    char c = characterIterator.convertIndexToCharacter(sampledCharacterIdx);
    predict probability distribution over the given input character (vector)
    sample a probable character from the generated distribution
    convert the index of the sampled character to an actual character
    We can therefore implement the Lookup#lookup API with our neural language model
    like this:
    @Override
    public List<LookupResult> lookup(CharSequence key, boolean onlyMorePopular, int num) throws IOExcepti
    List<LookupResult> results = new LinkedList<>();
    Map<String, Double> output = NeuralNetworksUtils.sampleFromNetwork(network,
    characterIterator, key.toString(), num);
    for (Map.Entry<String, Double> entry : output.entrySet()) {
    results.add(new LookupResult(entry.getKey(), entry.getValue().longValue()));
    }
    return results;
    }
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>122
    prepare the list of results
    sample num sequences from the network, given the CharSequence (key parameter)
    entered by the user
    add the sampled outputs to the list of results, using their probabilities (from the
    SoftMax function) as suggestion weights
    The CharLSTMNeuralLookup also needs to implement the build API. Since the
    CharacterIterator implementation relies on text coming from a file, we can create a
    temporary file where to store the values of
    @Override
    public void build(InputIterator inputIterator) throws IOException {
    Path tempFile = Files.createTempFile('chars','.txt');
    FileOutputStream outputStream = new FileOutputStream(tempFile.toFile());
    for (BytesRef surfaceForm; (surfaceForm = inputIterator.next()) != null;) {
    outputStream.write(surfaceForm.bytes);
    }
    outputStream.flush();
    outputStream.close();
    characterIterator = new CharacterIterator(tempFile.toAbsolutePath().toString(),
    miniBatchSize, exampleLength);
    this.network = NeuralNetworksUtils.trainLSTM(lstmLayerSize, tbpttLength, numEpochs,
    noOfHiddenLayers, ...);
    FileUtils.forceDeleteOnExit(tempFile.toFile());
    }
    create a temporary file
    fetch text coming from the lyrics field from the Lucene index (Lucene uses
    BytesRef instead of String for performance reasons)
    write the text into the temporary file
    release resources for writing into the temporary file
    create a CharacterIterator (using the CharLSTMNeuralLookup configuration
    parameters)
    build and train the LSTM (using the CharLSTMNeuralLookup configuration
    parameters)
    remove the temporary file
    Before going forward and using this Lookup in our search application, we need to
    make sure that the neural language model works well and gives good results; like most of
    other algorithms in computer science neural networks aren’t just 'magic', we need to set
    them up correctly if we want them to work nicely.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>123
    4.8 Tuning the LSTM language model
    Instead of doing what we did in chapter 3 by simply adding more layers to the network
    we’ll start simple with a single layer and see if we can make it with that alone, by
    adjusting other parameters. The most important reason for this is that as the complexity
    of the network grows (e.g. more layers) the data and time required for the training phase
    to generate a good model (which gives good results) grows as well So while we know
    that small shallow networks can’t beat deeper ones with lots of different data, this
    language modelling example is a good place to learn to start simple and go deeper only
    when needed.
    As we’ll be working more and more with neural networks we’ll get to know how to
    best set and tune them. For now we know that when data is large and diverse it might be
    a good idea to have a deep RNN for language modelling. But let’s be pragmatic and see
    if that’s true. In order to do that we need a way to evaluate the NN learning process.
    Neural network training is an optimization problem, where we want to optimize the
    weights in the connections between the neurons in order to let them generate the results
    we desire. This in practice means that we have an initial set of weights in each layer.
    These weights get adjusted during training so that the error the network commits when
    trying to predict outputs is lower and lower, as training goes on. If we see that the error
    committed by the network doesn’t go down or goes up, we have done something wrong
    in our setup. In chapter 3 we have talked about cost functions as they measure such error
    and the fact the neural network training algorithm objective is to minimize such cost
    function. A good way to start measuring if the training is doing well is to plot the cost (or
    loss) and makes sure it keeps going down as backpropagation proceeds.
    So to make sure that our neural language model would give us good results we want
    to track if the cost goes down. In the case of DL4J we can use IterationListeners like the
    ScoreIterationListener we used in chapter 3 (which will log the loss value) or, even
    better, a StatsListener which will collect and send stats to a remote server for better
    monitoring our learning process, with a proper user interface. Let’s have a look at how
    such server visualizes the learning process:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>124
    Figure 4.7 DL4J Training UI
    The Overview page of the DL4J Training UI contains a lot of information about the
    training process, the one that we’ll focus for now is the score vs iteration panel on the top
    left, that’s the place where we need to see the score going down as the number of
    iterations grows over time, ideally to reach a near zero score. On the top right some
    general information about the network parameters and training speed. We’ll skip for now
    the graphs on the bottom as they carry more detailed information about the size of the
    parameters (e.g. weights) and how they vary over time.
    It’s super easy to set this UI up:
    UIServer uiServer = UIServer.getInstance();
    StatsStorage statsStorage = new InMemoryStatsStorage();
    uiServer.attach(statsStorage);
    initialize the user interface backend
    configure where the network information is to be stored, in this case it’s in memory
    attach the StatsStorage instance to the UI so that the contents of the StatsStorage
    will be visualized
    Having configured and started the UI server, we have to tell the network to send
    statistics to it by adding a StatsListener
    MultiLayerNetwork net = new MultiLayerNetwork(conf);
    net.init();
    net.setListeners(new StatsListener(statsStorage));
    net.fit()
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>125
    our neural network to monitor
    initialize the network (e.g. set the initial weights in the layers)
    se the StatsListener
    let training start
    So let’s start with the char LSTM with 2 hidden layers and see how it performs on our
    queries dataset, by looking at the DL4J training UI.
    Figure 4.8 Char LSTM Neural language model with 2 hidden layers (300 neurons each)
    As you can see the score goes down very low with the number of iterations, this
    means we won’t probably have good results. Let’s start instead with a simpler and
    smaller setup, with 1 hidden layer with 100 neurons and see how it goes:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>126
    Figure 4.9 Char LSTM Neural language model with 1 hidden layer (100 neurons each)
    We got a much nicer score curve, in fact it degrades smoothly, although the final
    point is not nearly close to zero. Let’s anyway test it with our 'music is my aircraft'
    query, we expect not so good results.
    'm'
    --> musorida hosking floa
    --> miesxams reald 20
    ----
    'mu'
    --> muicu
    --> musci searudists
    ----
    'mus'
    --> mush im
    --> mustam pitylues and art
    ----
    'musi'
    --> musingle
    --> music
    ----
    'music'
    --> music bestles fam
    --> musics nurghisen leathers
    ----
    'music '
    --> music tents in sauraborls
    --> music kart
    ----
    'music i'
    --> music instente rairs
    --> music in toff chare sive he
    ----
    'music is'
    --> music island kn5 stendattion
    --> music is losting clutple
    ----
    'music is '
    --> music is seill butter
    --> music is the amehia faches of
    ----
    'music is m'
    --> music is masights
    --> music is marpuile abajomanial
    ----
    'music is my'
    --> music is myagh gue phatee on ca
    --> music is mysuctic
    ----
    'music is my '
    --> music is my stroon for zout perc
    --> music is my yay laries ingball n
    ----
    'music is my a'
    --> music is my and cheanby gas
    --> music is my alphnit
    ----
    'music is my ai'
    --> music is my airborty cioderopaship
    --> music is my air dea a
    ----
    'music is my air'
    --> music is my air met
    --> music is my air college
    ----
    'music is my airc'
    --> music is my aircentival ad distures
    --> music is my aircomute in fresight op
    ----
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>127
    'music is my aircr'
    --> music is my aircrichs of nwire
    --> music is my aircra
    mon(0)
    --> music is my aircric of
    ----
    'music is my aircra'
    --> music is my aircrations sime
    --> music is my aircracts fast
    ----
    'music is my aircraf'
    --> music is my aircraffems 2
    --> music is my aircrafthons and parin
    ----
    'music is my aircraft'
    --> music is my aircrafted
    --> music is my aircrafts njrmen
    ----
    This results are worse than the ones with previous non neural network based
    solutions. If a TST or an ngram based Lookup would still output complete words, this
    neural language model is learning to generate words one character at a time, a much
    harder task. If trained fine, the neural language model can learn to generate much more
    combinations of meaningful sentences. On the other hand if training doesn’t converge to
    a good set of parameters having a small loss, it’s possible that suggestions are worse than
    ngram models for example.
    We want the network to learn better, so we look at the two most important
    configuration options:
    the learning rate
    the number of parameters in the network
    The learning rate measures how fast the network should optimize the score (reduce
    the error) with regards to the given examples. If the learning rate is too high, the network
    will fail to minimize the loss; if it’s too low it’ll take too much to get to a good set of
    parameters (the weights in the connections between neurons). The number of parameters
    in the network gives a measure of how 'much' it can learn. If the number of training
    examples is lower than the number of parameters, the network may overfit. Overfitting
    happens when the network learns too well about the given training data and then it is not
    able to generalize on unseen data. This results in having very good results if an input
    sequence is an already seen one (or close to it) but gives bad results on unseen sequences.
    The number of parameters is given by a combination of the size of input and output
    layers and the shape and size of the hidden layers. For shape here we refer to the fact that
    hidden layers can be recurrent, like in this case, and therefore have more parameters (for
    each neuron an incoming weight and a looping weight for each incoming connection).
    The examples of our queries dictionary sum up to about 500000, while we have
    around 90000 parameters with our one layered LSTM with 100 neurons. Let’s raise the
    number of neurons in the layer to 200 and start the training again. Once we have finished
    training, let’s get the lookup results.
    'm'
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>128
    --> month jeans of saids
    --> mie free in manufact
    ----
    'mu'
    --> musications head socie
    --> musican toels
    ----
    'mus'
    --> muse sc
    --> muse germany nc
    ----
    'musi'
    --> musical federations
    --> musicating outlet
    ----
    'music'
    --> musican 2006
    --> musical swin daith program
    ----
    'music '
    --> music on the grade county
    --> music of after
    ----
    'music i'
    --> music island fire grin school
    --> music insurance
    ----
    'music is'
    --> music ish
    --> music island recipe
    ----
    'music is '
    --> music is befied
    --> music is an
    ----
    'music is m'
    --> music is michigan rup dogs
    --> music is math sandthome
    ----
    'music is my'
    --> music is my labs
    --> music is my less
    ----
    'music is my '
    --> music is my free
    --> music is my hamby bar finance
    ----
    'music is my a'
    --> music is my acket
    --> music is my appedia
    ----
    'music is my ai'
    --> music is my air brown
    --> music is my air jerseys
    ----
    'music is my air'
    --> music is my air bar nude
    --> music is my air ambrank
    ----
    'music is my airc'
    --> music is my airclass
    --> music is my aircicle
    ----
    'music is my aircr'
    --> music is my aircraft
    --> music is my aircross of mortgage choo
    ----
    'music is my aircra'
    --> music is my aircraft
    --> music is my aircraft popper
    ----
    'music is my aircraf'
    --> music is my aircraft in star
    --> music is my aircraft bouble
    ----
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>129
    'music is my aircraft'
    --> music is my aircraft
    --> music is my aircraftless theatre
    ----
    They are not as good as we would like them to be, but they have improved. Many of
    them are composed by correct English words, some of them are even funny like 'music is
    my aircraft popper' or 'music is my aircraftless theatre'! Let’s have another look at the
    'Overview' tab of the just trained neural language model.
    Figure 4.10 More parameters but still slow convergence
    We can see that the loss was going down, however it still didn’t reach a small value,
    the training phase was probably going too slow. Therefore let’s try to boost it by setting
    the learning rate to a higher value. We had it set to 0.1, let’s try with 0.6 (a very high one,
    by the way). Finally let’s add a second hidden layer, because that would allow the
    network to have more parameters while being able to better track the structure of
    sentences (we’ll expand on this further in the book). With all this set, let’s train the
    network again:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>130
    Figure 4.11 More parameters but still slow convergence
    We got a lower loss, and also the neural network reached that with more parameters.
    This means it 'knows' more about the training data while can still generalize well. We
    can consider ourselves satisfied now and set our char LSTM neural language model with
    2 hidden recurrent layers of 200 neurons each.
    Note that we can also look closer at each layer within the neural network and see how
    their parameters get adjusted during training. This can be done by clicking on the
    'model' tab in the DL4J UI on the top left and click on each layer.
    Figure 4.12 Model view in DL4J Training UI
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>131
    4.9 Diversifying suggestions using word vectors
    In chapter 2 we have seen how useful it is to use word vectors for synonym expansion. In
    this section we’ll mix them together with the results of LSTM generated suggestions in
    order to provide more diverse suggestions for the end user. In production systems it is
    very common to combine the results from different models to provide a user experience.
    The word2vec model allows to create a vectorized representation of a word. Such vectors
    are learned by a shallow neural network by looking at the surrounding context (other
    nearby words) of each word. The very nice thing of word2vec and similar algorithms for
    representing word as vectors is that they place similar words very close in the vector
    space, so that the vectors representing aircraft and aeroplane will lie very close to one
    another.
    Let’s build a word2vec model from the Lucene index containing the song lyrics,
    similarly to what we did in chapter 2.
    FieldValuesSentenceIterator iterator = new FieldValuesSentenceIterator(reader,'lyrics');
    Word2Vec vec = new Word2Vec.Builder()
    .layerSize(100)
    .iterate(iterator)
    .build();
    vec.fit();
    create a DataSetIterator over the contents of the Lucene field lyrics
    configure a word2vec model with 100 sized word vectors
    perform word2vec model training
    With the word2vec model trained on the very same data, we can now combine it
    within the CharLSTMNeuralLookup and generate more (and nicer) suggestions .
    4.10 Summary
    Search suggestions are very important in order to help users write good queries
    The data for generating such suggestions can be both static (e.g. dictionaries of
    previously entered queries) or dynamic (e.g. the documents stored in the search engine)
    We can use text analysis and / or ngram language models to build good suggester
    algorithms
    Neural language models are language models based on neural networks, like RNNs (or
    LSTMs)
    By using neural language models we can get better sounding suggestions
    It’s important to monitor the neural network training process in order to make sure that
    we’ll get good results
    We can combine the results of original suggester with word vectors to augment the
    diversity of the suggestions
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>132
    5
    Word embeddings based ranking
    This chapter covers:
    notions of statistical and probabilistic retrieval models
    ranking in Lucene
    neural information retrieval models
    using averaged word embeddings in ranking
    Since chapter 2 we have started building neural networks based components for the
    search engine. All such components aimed to help the search engine better capture the
    user intent by expanding synonyms, generating alternative representations of a query,
    giving smarter suggestions while typing a query. A query can be therefore expanded,
    adapted and transformed before 'reaching' the actual data (e.g. stored in inverted
    indexes).
    At that point, as also discussed in chapter 1, the terms of a query will be used to find
    matching documents. Such search results will be sorted with respect to the input query,
    this task is generally known as ranking (or scoring). In this chapter we’ll learn about
    common ranking functions and how we can leverage dense vector representations (also
    known as embeddings) of text (words, sentences, documents, etc.) in order to provide
    ranking functions that score documents better with respect to the user intent. The ranking
    function has a fundamental impact on the relevance of search results, therefore getting
    that right means that the users will get the most relevant important information first
    (higher precision).
    A somewhat funny meme that floated around on the internet for a while says: 'The
    best place to hide a dead body is page two of Google'. This is of course an hyperbolic
    sentence which applies mostly to web search (searching for content, e.g. pages, coming
    from websites). However it can tell a lot about the way users expect search engines to be
    very good at returning relevant results. It’s often mentally easier for a user to write a
    'better query' than scrolling down to the 'Page 2' button of the results page and wait for
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>133
    them to come up. The above meme could be probably rephrased as 'If it didn’t show up
    in the first page, it can’t be relevant'. This should tell something about why relevance is
    relevant, users should be considered:
    lazy : they don’t want to scroll down, look at more than 2 or 3 results, etc. before
    deciding whether results are good or not returning 1000s of results is often useless
    uninformed : they don’t know how a search engine works internally, they can just write a
    query and hope to get good results
    So if a search engine ranking function works very well, it’s possible to just return the
    first top 10 - 20 results and the user will be satisfied. Note that this can also have a
    positive impact on the performance of the search engine, as the user won’t browse
    through all the matching documents. You might wonder if the relevance problem applies
    in all cases though. For example if you think about a very short query made by one or
    two words which clearly identifies a small set of search results, then the relevance
    problem is less evident. Think about all the search queries we perform on Google in order
    to just retrieve a Wikipedia page that describes a famous person, e.g. Bernhard Riemann.
    It’s very annoying to type en.wikipedia.org…., search for Bernhard Riemann in the text
    box and click the lens button to get the results. It’s much faster to type Bernhard
    Riemann on the Google search box and you’ll most probably get the Wikipedia page as
    the first or second search result on the first page. This is an example where you (think
    you) know in advance what you want to retrieve (you’ve been lazy but you were
    informed about what you wanted and how the search engine usually works when
    searching for persons, by prior experience). In many cases however this doesn’t apply.
    Put yourself in the shoes of an undergrad student of Maths that is not just interested in
    generic information about Riemann, but instead wants to understand why his works are
    considered important in several different fields of Science. The user doesn’t know in
    advance the specific resources he needs, he knows the type of resource needed, and will
    type a query according to that. So such a student may type the importance of Bernhard
    Riemann works or Bernhard Riemann influence in academic research. If you try to run
    these queries on Google yourself, you’ll surely observe :
    different search results
    even for those search results that appear in both cases, they will appear in different
    ordering
    More notably, at the time of writing, the first query returns again the Wikipedia page
    as first result while the second query first result title is herbart’s influence on bernhard
    riemann. That is weird because is sort of turning the user intent upside down, the student
    wanted to know how Riemann influenced others, and not vice versa (e.g. the second
    result riemann’s contribution to differential geometry sounds much more relevant).
    This is the kind of problems that make ranking search results a very hard problem to
    solve. Let’s now see how ranking comes into place in the lifecycle of a query:
    a query written by the user is parsed and analyzed so that it gets decomposed into
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>134
    (smaller) terms, eventually decorated with some constraints, like, for example, boolean
    operators (AND, OR, etc.), let’s call it an encoded query (that’s because at this point it is
    represented in a way that depends on the search engine implementation)
    the encoded query is executed over the search engine data structures (e.g. for each term a
    lookup in the inverted index table is performed)
    the matching documents are collected and 'sent' to the ranking function
    each document is scored by the ranking function
    in most cases the list of search result is composed by the scored documents, sorted
    according to their score in descending order (e.g. first result has highest score)
    Figure 5.1 Querying, retrieving and ranking
    So the ranking function takes a bunch of documents and assigns a score value which
    is an indicator of the importance of that document with respect to the input query, the
    higher the score the more important the document.
    Additionally when ranking results, a smart search engine should consider:
    the user history: record the past activity of a certain user and let this also have an impact
    on ranking. For example recurring terms in past queries might indicate a user’s interest in
    a certain topic, so search results on that same topic should have a higher ranking
    the user geographical location: record the user location to eventually raise the score of
    search results written in the pertaining language
    temporal changes in information: think about the latest trends query we mentioned
    several times, such a query should not just match the words latest and / or trends but also
    'boost' the score of newer documents (more recent information)
    all possible 'signals' that can provide more context to the query itself, for example look
    at the search logs and look into if a certain query had already been executed, then look at
    the next query in time in the search log and see if there’s any shared result, if so give an
    higher ranking to them
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>135
    We’ll now dive into answering the key question: 'how does the search engine decide
    how to rank search results with respect to a given query' ?
    5.1 Retrieval models
    So far we have talked about the task of ranking a document as a function that takes a
    document as input and generates a score value, representing the document’s relevance.
    In practice ranking functions are often part of an information retrieval model (or IR
    model). Such a model defines how the search engine tackles the whole problem of
    providing relevant results with respect to an information need: from query parsing to
    matching, retrieving and ranking search results. The rationale for having a 'model' is that
    it’s very hard to come with a ranking function that gives an accurate score without
    knowing how the search engine handles a query. In a query like +riemann -influenced
    influencing, if there’s a document containing both the terms riemann and influencing, the
    resulting final score should be a combination of the score on the first and second terms (
    score = score(riemann) + score(influencing)), however the riemann term had a
    mandatory constraint (the + sign) so it should contribute a higher score than the other
    term which was optional. So the way a search engine calculates the relevance of a
    document with respect to a query has an impact on the design and infrastructure behind a
    search engine. Since chapter 1 we have assumed that when text is ingested into the search
    engine, it gets analyzed and split into chunks that can get altered depending on tokenizers
    and token filters. This text analysis chain generate terms which end up in inverted
    indexes, also known as posting lists. The search by keyword use case motivated the
    choice of posting lists to efficiently retrieve documents by matching terms. Similarly the
    choice of how to rank query - document pairs might impact system requirements: for
    example, the ranking function might need to access more information about the indexed
    data than just the presence or absence of a term in the posting list. A widely used set of
    retrieval models called statistical models make decisions about the ranking of a certain
    document based on how frequent a matching term is within a specific document and in
    the whole document set.
    In the previous chapters we have already gone beyond simple matching of terms
    between query and documents. In fact we used synonym expansion to generate synonym
    terms, e.g. at search time to extend the number of possible ways a user could 'say' the
    same thing (at word level). We expanded this approach in chapter 3 by generating brand
    new alternative queries in addition to the original query entered by the user. All this work
    looks into building a search engine which is eager to understand the semantics of text:
    in the synonym expansion case: if you type 'hello' or 'hi', you are semantically saying
    the same thing
    in the alternative query expansion case: if you type 'latest trends', you get alternative
    queries that are spelled in a different way but are semantically close to the original one
    Overall the (simplified) idea is that a document that is relevant with respect to a
    certain query should be returned even if there’s no exact matching between query and
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>136
    indexed terms. Synonyms and alternative query representations provide a wider range of
    relevant query terms that can match the document ones. Those methods make it more
    probable to find a document using semantically similar words / queries. In an ideal world
    a search engine would go beyond query - document term matching and understand the
    user information need. Based on that it would return that are relevant to that need, again
    not constraining retrieval to term matching.
    Having a search engine with good semantic understanding capabilities is hard. The
    good news is that, as we see, deep learning based techniques can help a lot in reducing
    the gap between a the plain query string and the actual user intent. Think about the
    though vector we briefly met in chapter 3 in seq2seq models, we can think to it as the
    kind of representation of user intent we need to go beyond simple term matching. A good
    retrieval model should take care of the above considerations of semantics. As you may
    imagine this 'semantic' perspective applies to ranking documents as well. For example:
    when ranking a result whose matching terms came from one of the alternative queries
    generated by a LSTM network, should such documents scored differently than the ones
    that matched based on terms coming from the original user query ?
    if we plan to use deep learning generated representations (e.g. like thought vectors) to
    capture user intent, how do we use them to retrieve and rank results ?
    We’ll now start a small journey where we’ll touch
    the more 'traditional' retrieval models
    extension of traditional models that leverage vector representations of text learned
    through neural networks this will be our main focus
    the so called neural IR models which purely rely on deep neural networks
    5.1.1 TF-IDF and Vector space model
    In chapter 1 we have briefly mentioned TF-IDF and Vector Space Model, let’s now take
    a closer look at them in order to understand how it works. The fundamental problem of a
    ranking function is to assign a score to a query - document pair. A very common way to
    measure the importance of a document with respect to a query is based on calculating and
    fetching statistics over query and document terms. Such retrieval models are called in
    fact statistical models for information retrieval. Let’s think of a query bernhard riemann
    influence and two resulting documents document1 = riemann bernhard - life and works
    of bernhard riemann and document2 = thomas bernhard biography - bio and influence in
    literature. Both the query and the documents are composed by terms, let’s look at which
    of them matched and we can observe that:
    document1 matched in the terms riemann and bernhard, both these terms matched twice
    document2 matched in the terms bernhard and influence, both these terms matched once
    Very frequently statistical models combine term frequency and document frequency
    to come up with a measure of the relevance of a document, given a query. The rationale
    in the choice of this metrics is the following: calculating frequencies and statistics about
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>137
    terms give us a measure of how informative each of them is. More specifically: the
    number of times a query term appears in a certain document gives a measure of how
    pertinent such a document could be to that query, this is the term frequency. On the other
    hand, terms that appear very rarely in the indexed data are considered more important
    and informative than the more common ones, the frequency of a term within all the
    indexed documents is called document frequency (think about terms like the or in, they
    are usually not very informative because they are way too common). Let’s get back at our
    example:
    the term frequency for document1 is 2 for each matching term, while document2 term
    frequency for its two matching terms is 1
    the document frequency for bernhard is 2 (it appears in both documents, we don’t count
    repeated occurrences in a singular document), the document frequency for riemann is 1,
    the document frequency for influence is 1.
    If we sum all the term frequencies of each matching terms, we’ll have a score of 4 for
    document1 and 2 for document2. Let’s add another document whose content is riemann
    hypothesis - a deep dive into a mathematical mystery and score it against the same query.
    Document3 will have a score of 1 because only the riemann term matches. This is not so
    good because this document3 is more relevant than document2, although not pertinent to
    Riemann’s influence.
    A better way to express ranking is to score each document using the sum of the
    logarithms of term frequencies divided by the logarithm of its document frequency. This
    very famous weighting scheme is called TF-IDF.
    Listing 5.1 TF-IDF weighting scheme
    weight(term) = (1+log(tf(term)))*log(N/df(term))
    where N is the number of indexed documents.
    With the new document3 added, the document frequency for the term riemann has
    become 2.
    Using the formula above for each matching term we add each TF-IDF and obtain the
    following scores :
    score(document1) = tf-idf(riemann)+tf-idf(bernhard) = 1.28 + 1.28 = 2.56
    score(document2) = tf-idf(bernhard)+tf-idf(influence) = 1 + 1 = 2
    score(document3) = tf-idf(riemann) = 1
    We’ve just witnessed that TF-IDF based scoring only relies on pure frequencies of
    terms, so a not relevant document (document2) is scored above a somewhat relevant one
    (document3). This is a case where the retrieval model is missing semantic understanding
    of query intent, as discussed in the previous section.
    In this book so far you have encountered vectors a lot of time, using them in
    information retrieval is not a novel idea, the Vector Space Model relies on representing
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>138
    queries and documents as vectors and measure how similar they are based on a TF-IDF
    weighting scheme. In fact each document can be represented by a one dimensional vector
    with size equals to the number of existing terms in the index. Each position in the vector
    represents a term having a value equals to the TF-IDF value for that document for that
    term. The same can be done for queries, as they are equally compopsed by terms, the
    only difference stands in the fact that term frequencies can either be local (frequency of
    query terms as they appear in the query) or from the index itself (frequency of query
    terms as they appear in the indexed data). This way we represent documents and queries
    as vectors.
    termsbernhardbio divehypothesisin
    doc11.280.00.00.0doc21.01.00.0doc30.00.01.0query0.01.00.0
    influence
    intolife mathematicalriemann
    0.0 0.00.01.0 0.01.28
    0.01.0 1.00.00.0 0.00.0
    1.00.0 0.01.00.0 1.01.0
    0.00.0 0.00.00.0 1.00.0
    The above representation is generally known as bag of words. The reason for that
    name is that the information about positions of terms is lost, every document / query is
    represented as a collection of words. In fact the vectors of bernhard riemann influence
    and riemann influence bernhard would look exactly the same, however the facts that the
    two queries are different and, additionally, the first query is more meaningful than the
    second one is not captured. Now that we have documents and queries represented in a
    vector space, we want to calculate what’s the document that best matches the input query.
    We do that by calculating the cosine similarity between each document and the input
    query and that will give us the final ranking for each document. The cosine similarity is a
    measure of the amplitude of the angle between a document and the query vectors, as per
    image below:
    Figure 5.2 Cosine similarity
    The image shows vectors for the input query, document1 and document2 in a
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>139
    (simplified, two dimensional) vector space only considering the words bernhard and
    riemann.
    The similarity between the query vector and a document is evaluated by looking at
    existing angle between the two vectors. The smaller the angle, the more similar two
    vectors are.
    Once we apply that to the above vectors we’ll get the similarity scores below:
    Listing 5.2 Cosine similarities between query and documents
    cosineSimilarity(query,doc1) = 0.51
    cosineSimilarity(query,doc2) = 0.38
    cosineSimilarity(query,doc3) = 0.17
    As you can see from the above table, with just three documents, the resulting vectors
    size is 20. In production systems this would be a lot more. So one problem with this bag
    of words representation is that the size of vectors grows linearly with the number of
    existing terms (e.g. all the distinct words contained in indexed documents). This is
    another reason why word vectors like the ones generated by word2vec are better than bag
    of words ones. Word2vec generated vectors have a fixed size, so they do not grow with
    the number of terms in the search engine and therefore there’s a much lower resource
    consumption (beyond the fact that word2vec generated vectors better capture word
    semantics, as explained in chapter 2) when using them.
    Despite the above limitations, VSM and TF-IDF keep being used a lot with good
    results in many production systems. Before proceeding with discussing other information
    retrieval models, let’s get pragmatic and try to ingest the mentioned documents in Lucene
    and see how they get scored using TF-IDF and VSM.
    5.1.2 Ranking documents in Lucene
    In Lucene the Similarity API serves as the base for ranking functions. Lucene comes with
    some information retrieval models already implemented out of the box, like the above
    described Vector Space Model with TF-IDF (which was the default used up to version 5)
    but also other models like Okapi BM25, Divergence from Randomness, Language
    models and others.
    vc The _Similarity_ needs to be set at both indexing and search time. In Lucene 7, the Vector Space
    TF-IDF similarity is _ClassicSimilarity_. At index time this is done in the _IndexWriterConfigurati
    Listing 5.3 Index time configuration for Similarity
    IndexWriterConfig config = new IndexWriterConfig();
    config.setSimilarity(new ClassicSimilarity());
    IndexWriter writer = new IndexWriter(directory, config);
    create a configuration for indexing
    set the similarity to ClassicSimilarity
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>140
    create an IndexWriter using the configured Similarity
    At search time setting the Similarity has to be done in the IndexSearcher
    Listing 5.4 Search time configuration for Similarity
    IndexReader reader = DirectoryReader.open(directory);
    IndexSearcher searcher = new IndexSearcher(reader);
    searcher.setSimilarity(new ClassicSimilarity);
    open an IndexReader
    create an IndexSearcher over the reader
    set the Similarity in the IndexSearcher
    If we index and search over the three documents above we can see if ranking behaves
    as we expect.
    Listing 5.5 Indexing three documents
    FieldType fieldType = ...
    Document doc1 = new Document();
    doc1.add(new Field('title', 'riemann bernhard - life and works of bernhard riemann', ft));
    Document doc2 = new Document();
    doc2.add(new Field('title', 'thomas bernhard biography - bio and influence in literature', ft));
    Document doc3 = new Document();
    doc3.add(new Field('title', 'riemann hypothesis - a deep dive into a mathematical mystery', ft));
    writer.addDocument(doc1);
    writer.addDocument(doc2);
    writer.addDocument(doc3);
    writer.commit();
    you can define the features of a Lucene field yourself (storing values, storing term
    positions, etc.)
    for each document, create a new Document and add contents in a title field
    add all three documents and commit the changes
    Listing 5.6 Searching and ranking
    String queryString = 'bernhard riemann influence';
    QueryParser parser = new QueryParser('title', new WhitespaceAnalyzer());
    Query query = parser.parse(queryString);
    TopDocs hits = searcher.search(query, 3);
    for (int i = 0; i < hits.scoreDocs.length; i++) {
    ScoreDoc scoreDoc = hits.scoreDocs[i];
    Document doc = searcher.doc(scoreDoc.doc);
    String title = doc.get('title');
    System.out.println(title + ' : ' + scoreDoc.score);
    System.out.println('--');
    Explanation explanation = searcher.explain(query, scoreDoc.doc);
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>141
    System.out.println(explanation);
    }
    write a query
    parse the user entered query
    perform search
    print document title and score on the standard output
    get an Explanation of how the score has been calculated
    With ClassicSimilarity we’ll get the following output.
    riemann bernhard - life and works of bernhard riemann : 1.2140384
    --
    1.2140384 = sum of:
    0.6070192 = weight(title:bernhard in 0) [ClassicSimilarity], result of:
    0.6070192 = fieldWeight in 0, product of:
    ...
    0.6070192 = weight(title:riemann in 0) [ClassicSimilarity], result of:
    0.6070192 = fieldWeight in 0, product of:
    ...
    --
    thomas bernhard biography - bio and influence in literature : 0.9936098
    --
    0.9936098 = sum of:
    0.42922735 = weight(title:bernhard in 1) [ClassicSimilarity], result of:
    0.42922735 = fieldWeight in 1, product of:
    ...
    0.56438243 = weight(title:influence in 1) [ClassicSimilarity], result of:
    0.56438243 = fieldWeight in 1, product of:
    ...
    --
    riemann hypothesis - a deep dive into a mathematical mystery : 0.4072008
    --
    0.4072008 = sum of:
    0.4072008 = weight(title:riemann in 2) [ClassicSimilarity], result of:
    0.4072008 = fieldWeight in 2, product of:
    ...
    As we expected the ranking respected what we described in the previous section. We
    can see from the explanation that each term that matched the query contributes with its
    weight as they get summed.
    0.9936098 = sum of:
    0.42922735 = weight(title:bernhard in 1)...
    0.56438243 = weight(title:influence in 1)...
    On the other hand the scores are not exactly the same we saw when we manually
    computed TF-IDF weights for terms. The reason for this is that there are many possible
    variations of TF-IDF schemes, so for example here Lucene calculates inverse document
    frequency as:
    log((N+1)/(df(term)+1))
    instead of
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>142
    log(N/df(term))
    Additionally Lucene does not take the logarithm of term frequency but simply term
    frequency as it is. Lucene also uses normalization, a technique to mitigate the fact that
    documents with more terms would score too high with respect to short documents (with
    less terms), which can be approximated as: 1.0 / Math.sqrt(numberOfTerms)). When
    using this normalization technique calculating the cosine similarity between a query
    vector and a document vector is equivalent to calculating their scalar product.
    score(query,document1) = tf-idf(query, bernhard)*tf-idf(document1, bernhard)+tf-idf(query,
    riemann)*tf-idf(document, riemann)
    So Lucene does not store vectors. It is enough to be able to compute TF-IDF for each
    matching term and combine them as above to compute the score.
    5.1.3 Probabilistic models
    We have learned about some Vector Space Model (VSM) theory and how it is applied in
    practice in Lucene. We have seen scores being calculated using plain term statistics. In
    this section you’ll learn about probabilistic retrieval models, where scores will still be
    calculated on the basis of probabilities. The search engine will rank a document using its
    probability of relevance with respect to the query. Probabilities are a very powerful tool
    to address uncertainty. We have discussed a lot how hard is to bridge the gap between a
    user intent and relevant search results. Probabilistic models try to model ranking by
    measuring how much probable is that a certain document is relevant with respect to the
    input query. If you roll a six faced dice, you have that each face has 1/6 probabilities to
    come out, e.g. the probability of rolling a 3 is P(3) = 0.16. In practice if you roll that dice
    6 times, you most probably won’t be having all the 6 different results. Probability is an
    estimation of how likely is for a certain event to occurr, this doesn’t imply that it will
    occur exactly along those measures. Rolling a dice unconditioned probability is 1/6, but
    what about the probability of rolling two consecutive same results ? Such conditioned
    probabilities can be expressed as P(event|condition). Back to our ranking task, we
    estimate the probability that a certain document is relevant (with respect to a given
    query). We can represent that as P(r=1|x), where r is a binary measure of relevance r=1 :
    relevant, r=0 : not relevant. In a probabilistic retrieval model we generally rank all
    documents with respect to a given query by P(r=1|x). This is best expressed by the
    Probability Ranking Principle : if retrieved documents are ordered by decreasing
    probability of relevance on the data available, then the system’s effectiveness is the best
    that can be obtained for the data.
    One of the most famous and widely adopted probabilistic model is Okapi BM25.
    Briefly it tries to mitigate two limitations of TF-IDF:
    limit the exceeding impact of term frequency to avoid excessive scoring based on highly
    repeated terms
    provide a better estimate of the importance of the document frequency of a certain term
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>143
    BM25 expresses the above conditional probability P(r=1|x) by means of two
    probabilities that depend on term frequencies. So BM25 approximates probabilities by
    calculating probability distribution over term frequencies.
    If you think about the bernhard riemann influence example. In a classic TF-IDF
    scheme having a high term frequency can lead to a high score. So if you have a dummy
    document4 having just lots of bernhard occurrences (bernhard bernhard bernhard
    bernhard bernhard bernhard bernhard bernhard bernhard bernhard), it might score
    higher than the more relevant ones. If we index into the previously built index, we get the
    following outputs with TF-IDF and VSM (ClassicSimilarity):
    Listing 5.7 Search results when adding document4 (in the form title : score) using
    ClassicSimilarity
    riemann bernhard - life and works of bernhard riemann : 1.2888055
    bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard : 1.2231436
    thomas bernhard biography - bio and influence in literature : 1.0464782
    riemann hypothesis - a deep dive into a mathematical mystery : 0.47776502
    As you can see this dummy document gets returned as the second result, which
    sounds weird. Additionally document4’s score is almost equals to the first ranked result,
    so the search engine has ranked this dummy document as very important, while it’s not at
    all. Let’s set the BM25Similarity in Lucene (the default since version 6) using the same
    code we used for ClassicSimilarity tests.
    searcher.setSimilarity(new BM25Similarity());
    With BM25 similarity set, we have the following ranking:
    Listing 5.8 Search results when adding document4 using BM25Similarity
    riemann bernhard - life and works of bernhard riemann : 1.6426628
    thomas bernhard biography - bio and influence in literature : 1.5724708
    bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard : 0.9965918
    riemann hypothesis - a deep dive into a mathematical mystery : 0.68797445
    We get the dummy document ranked in third position instead of second. Although
    this is not optimal, we can see that its score has greatly decreased when compared to the
    most relevant one. The reason for that is that BM25 'squashes' the term frequency that to
    keep it below a certain configurable threshold. So in this case BM25 mitigated the impact
    of the very high term frequency for the term bernhard. The other good thing about BM25
    is that it tries to estimate the probability of terms appearing together in a certain
    document. The document frequency of a number of terms in a certain document is given
    by the sum of the logs of the probability that each single term appears in that document.
    However also BM25 has some limitations:
    like TF-IDF, BM25 is a bag of words model so it disregards terms ordering when ranking
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>144
    while in general performing good, BM25 is based on some heuristics (functions that
    reach a somewhat good result, although not guaranteed to work in general) which may
    not apply well to your data (so you may have to adjust those heuristics yourself).
    BM25 performs some approximation / simplification on probability estimation, that
    causes not too good results in some cases (it doesn’t work very well with long
    documents)
    Other probabilistic approaches to ranking based on language models are generally
    better at plain probability estimations, however this doesn’t always result in better
    scoring. In general BM25 is an ok baseline ranking function.
    Now that we have discovered some of the most commonly used ranking models for
    search engines, let’s dive into how neural networks can help us:
    make them better
    provide completely new (and better) ranking models
    5.2 Neural information retrieval
    So far we have tackled the problem of effective ranking by looking at terms and their
    local (per document) and global (per collection) frequencies. If we want to use neural
    networks to help us obtain a better ranking function we need to think in terms of vectors.
    Actually this doesn’t solely apply to neural networks. We have seen that even the
    'classic' vector space model was treating documents and queries as vectors and measure
    their similarity using cosine distance. One problem with that is the size of such vectors
    can grow enormously (linearly) with the number of indexed words.
    Before neural information retrieval, other techniques have been developed to provide
    more compact (fixed size) representations of words, mainly based on matrix factorization
    algorithms, like the Latent Semantic Indexing algorithm (or LSI) based on Singular Value
    Decomposition (also known as SVD) factorization. In short in LSI, you create a matrix of
    terms and documents, for each document row: put a 1 in each element where the
    document contains the corresponding term, 0 for all others. Then transform (factorize)
    this sparse matrix (lots of zeros) with reduced Singular Value Decomposition
    factorization method, resulting in three (denser) matrices whose product is a good
    approximation of the original one. Each resulting document row has fixed dimensionality
    and it’s not sparse anymore. Query vectors can be also transformed using the SVD
    factorized matrices. A somewhat similar technique is called Latent Dirichlet Allocation.
    The 'juice' here is that no term matching is required, query and document vectors are
    compared so that the most similar document vectors are ranked first.
    Learning good representations of data is one of the tasks deep learning can do best.
    We will now look into using such vector representations for ranking. You should now be
    familiar with the algorithm we’re going to use: word2vec. It infact learns distributed
    representations of words. Word vectors lie close to one another when the word they
    represent appear in similar contexts and, hence, have similar semantics.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>145
    5.3 From word to document vectors
    Let’s start building a retrieval system based on vectors generated by word2vec. Our goal
    is to rank documents against queries, however word2vec gives vectors for words, not
    sequences of words. So the first thing to do is find a way to use these word vectors to
    represent documents and queries. In fact a query will be most likely composed by more
    than one word and the same applies to documents indexed. For example, let’s take the
    word vectors for each of the terms in the query bernhard riemann influence and plot
    them.
    Figure 5.3 Word vectors for words bernhard, riemann and influence
    A simple method to create document vectors from word vectors is to 'average' word
    vectors into a single document vector. This is a very simple mathematical operation,
    every element at position j in each vector is added and then divided by the number of
    vectors being averaged (same as an arithmetic averaging operation). We can do that with
    DL4J vectors (INDArrays objects) as follows:
    Listing 5.9 Averaging word2vec vectors of given terms into a document vector
    public static INDArray toDenseAverageVector(Word2Vec word2Vec, String... terms) {
    return word2Vec.getWordVectorsMean(Arrays.asList(terms));
    }
    If you look at the previous picture, you should expect an average vector to spawn at
    the centre of those three word vectors.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>146
    Figure 5.4 Averaging bernhard, riemann and influence word vectors
    The mean vector is the result of the averaging operation. As expected it sits at the
    centre of the three word vectors.
    Note that this technique can be applied both in the case of documents and queries as
    they are simply compositions of words. For each document - query pair we can calculte
    their document vectors using by averaging word vectors and assign the score to each
    document based on how close they are. This is similar to what we’ve done in vector
    space model scenario, however the big difference is that the values of these document
    vectors are not calculated using TF-IDF but come from averaging word2vec vectors. In
    summary these dense vectors are less heavy in terms of required memory (and space if
    stored to disk) and more informative in terms of semantics.
    Let’s repeat the experiment we did before but rank documents using averaged word
    vectors. We first feed word2vec with data from the search engine:
    IndexReader reader = DirectoryReader.open(directory);
    FieldValuesSentenceIterator iterator = new FieldValuesSentenceIterator(reader, 'title');
    Word2Vec vec = new Word2Vec.Builder()
    .layerSize(3)
    .windowSize(3)
    .tokenizerFactory(new DefaultTokenizerFactory())
    .iterate(iterator)
    .build();
    vec.fit();
    create a reader over the search engine document set
    create a DL4J iterator that can read data from the reader on the title field
    configure word2vec
    we work with a super small data set therefore we use very small vectors
    let word2vec learn word vectors
    Once we have extracted the word vectors we can build query and document vectors.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>147
    String[] terms = ...
    INDArray queryVector = toDenseAverageVector(vec, terms);
    for (int i = 0; i < hits.scoreDocs.length; i++) {
    ScoreDoc scoreDoc = hits.scoreDocs[i];
    Document doc = searcher.doc(scoreDoc.doc);
    String title = doc.get('title');
    Terms docTerms = reader.getTermVector(scoreDoc.doc, 'title');
    INDArray denseDocumentVector = VectorizeUtils.toDenseAverageVector(docTerms, vec);
    double sim = Transforms.cosineSim(denseQueryVector, denseDocumentVector)
    System.out.println(title + ' : ' + sim);
    }
    an array containing the terms entered in the query (bernhard, riemann, influence)
    convert the query terms into a query vector by averaging the word vectors of its
    terms
    for each search result: ignore score as given by Lucene and transform them into
    document vectors
    get the document title
    extract the terms contained in that document (using IndexReader#getTermVector
    API)
    convert document terms into a document vector using the averaging technique
    shown above
    calculate cosine similarity between query and document vector and print it
    For the sake of readability we show the outputs ordered from highest to lowest scores:
    riemann hypothesis - a deep dive into a mathematical mystery : 0.6171551942825317
    thomas bernhard biography - bio and influence in literature : 0.4961382746696472
    bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard :
    0.32834646105766296
    riemann bernhard - life and works of bernhard riemann : 0.2925628423690796
    the top scored document is relevant, regardless of low term frequency
    the second document is not very relevant with respect to the user intent
    the third document is the dummy one
    the (probably) most relevant document has the lowest score
    This looks weird: the technique we expected to help us get a better ranking turned out
    to rank the dummy document better than the most relevant one! The reason for this weird
    ordering are the following:
    there is not enough training data available for word2vec to provide word vectors that
    carefully represent word semantics. Four short documents give way too few word -
    context pairs for the word2vec neural network to adjust its hidden layer weights
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>148
    accurately
    if we pick the document vector of the top scored document, that will be equals to the
    word vector for the word bernhard. The query vector is an average vector of the vectors
    for bernhard, riemann and influence , therefore these vectors will always lie quite close
    in the vector space.
    Let us visualize the second statement by plotting the generated document / query
    vectors in a (reduced) two dimensional space.
    Figure 5.5 Similarity between query and document embeddings
    As expected, document4 and the query embeddings are so close that their labels
    almost overlap.
    One way to improve the above results is to make sure the word2vec algorithm has
    more training data. We can take, for example, an English dump of Wikipedia and index
    each page title and content in Lucene. Additionally we can mitigate the impact of text
    fragments like the one from document4, which mostly (or only) contains single terms that
    also appear in the query. In order to do so a common technique is to 'smooth' the
    averaged document vectors by using term frequencies. Instead of dividing each word
    vector by the document length, we divide each word vector by its term frequency
    accoding to the pseudo code below.
    documentVector(wordA wordB) = wordVector(wordA)/termFreq(wordA) + wordVector(wordB)/termFreq(wordB)
    This can be implemented in Lucene and DL4J as follows:
    Listing 5.10 Averaged document vectors composed by word vectors smoothed
    with term frequencies
    public static INDArray toDenseAverageTFVector(Terms docTerms, Terms fieldTerms,
    Word2Vec word2Vec) throws IOException {
    INDArray vector = Nd4j.zeros(word2Vec.getLayerSize());
    TermsEnum docTermsEnum = docTerms.iterator();
    BytesRef term;
    while ((term = docTermsEnum.next()) != null) {
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>149
    long termFreq = docTermsEnum.totalTermFreq();
    INDArray wordVector = word2Vec.getLookupTable().vector(term.utf8ToString()).div(termFreq);
    vector.addi(wordVector);
    }
    return vector;
    }
    When introducing average word vectors we saw that such document vectors were
    placed right at the centre of its composing word vectors. In the picture below we can see
    that term frequency smoothing can help detach the generated document vectors from
    sitting at the centre of its word vectors, getting nearer to the less frequent (and hopefully
    more important) word.
    Figure 5.6 Averaged word vector smoothed by term frequencies
    The terms bernhard and riemann are more frequent than influence and in fact the
    generated document vector called tf gets closer to the influence word vector. This has a
    positive impact in better ranking documents whose term frequency is low, but still they
    lie close enough to the query vector.
    riemann hypothesis - a deep dive into a mathematical mystery : 0.6436703205108643
    thomas bernhard biography - bio and influence in literature : 0.527758002281189
    riemann bernhard - life and works of bernhard riemann : 0.2937617599964142
    bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard :
    0.2569074332714081
    For the first time we managed to give the lowest score to the dummy document. If we
    switch from plain term frequencies to TF-IDF as smoothing factors for generating
    averaged document vectors from word embeddings, we get the following ranking:
    riemann hypothesis - a deep dive into a mathematical mystery : 0.7083401679992676
    riemann bernhard - life and works of bernhard riemann : 0.4424433362483978
    thomas bernhard biography - bio and influence in literature : 0.3514146476984024
    bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard :
    0.09490833431482315
    With the tf-idf based smoothing the ranking of documents is the best we can achieve.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>150
    We got away from strict term weighting based similarity: the most relevant document has
    a term frequency of 1 for the term riemann, whereas the document with the highest term
    frequency has the lowest score. From a semantic perspective we got the most relevant
    ones scored higher than the others.
    Figure 5.7 Averaged word vector smoothed by TF-IDF
    5.4 Evaluations and comparisons
    Are we good with this way of ranking documents based on TF-IDF averaged word
    vectors ? In the above example we have trained word2vec with specific settings: layer
    size set to 60, SkipGram model, window size set to 6, etc. The ranking was optimized
    with respect to a specific query and set of four documents. While this is a useful exercise
    for learning pros and cons of different approaches, we can’t do such fine grained
    optimizations for all the possible input queries, especially for large knowledge bases.
    Being relevance so hard to get right, it is good to find ways to automate the evaluation of
    ranking effectiveness. So before jumping on other ways to address ranking (e.g. with the
    help of neural text embeddings), let’s quickly introduce some tooling to speed up ranking
    functions evaluation.
    A nice tool for evaluating effectiveness of Lucene based search engines is Lucene4IR
    (Lucene for Information Retrieval). It originated from a nice collaboration between
    people from research and industry 1. A quick tutorial can be found at
    github.com/lucene4ir/lucene4ir/blob/master/README.md. Lucene4IR does is makes it
    possible to try out different indexing / retrieval and ranking strategies over the same
    dataset. In order to try it out, you can run Lucene4IR’s IndexerApp, RetrievalApp and
    ExampleStatsApp. This will index, search and record statistics over returned versus
    relevant results e.g. according to the chosen Lucene Similarity, Analyzers, etc. By default
    these apps run on the CACM dataset 2 using BM25Similarity. Once you have performed
    data evaluation with Lucen4IR tools, you can measure the precision, recall and other IR
    metrics using the trec_eval tool (a tool developed to measure quality of search results on
    the data from TREC conference series 3).
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>151
    Footnote 1msee sigir.org/wp-content/uploads/2017/01/p058.pdf
    Footnote 2msee ir.dcs.gla.ac.uk/resources/test_collections/cacm/
    Footnote 3mtrec.nist.gov/
    Example of a trec_eval terminal output on CACM dataset using BM25 ranking
    ./trec_eval ~/lucene4ir/data/cacm/cacm.qrels ~/lucene4ir/data/cacm/bm25_results.res
    ...
    num_qall51
    num_retall5067
    num_relall793
    num_rel_retall341
    mapall0.2430
    Rprecall0.2634
    P_5all0.3608
    P_10all0.2745
    the number of queries performed
    the number of returned results
    the number of relevant results
    the number of returned results that are also relevant
    map gives the Mean Average Precision
    Rprec gives the R-Precision
    P_5, P_10, etc. give the precision at 5, 10 etc. retrieved documents
    If we change the similarity parameter in the Lucene4IR configuration file and run
    again the RetrievalApp and ExampleStatsApp we can observe how precision, recall and
    other measueres commonly used in information retrieval change in our dataset.
    Example of a trec_eval terminal output on CACM dataset using language model
    (Lucene’s LMJelinekMercerSimilarity 4) based ranking
    Footnote 4mwww.iro.umontreal.ca/~nie/IFT6255/zhai-lafferty.pdf
    ./trec_eval ~/lucene4ir/data/cacm/cacm.qrels ~/lucene4ir/data/cacm/bm25_results.res
    ...
    map
    all
    0.2292
    Rprec
    all
    0.2552
    P_5
    all
    0.3373
    P_10
    all
    0.2529
    In the above case the similarity was switched to use language models for estimating
    probabilities of relevance. We can observe that the results are worst than with BM25
    because all the metrics give slightly lower values. The very nice thing of using these tools
    together is that you can evaluate how good your decisions impact accuracy of search
    results in a series of quick easy steps. This doesn’t guarantee that you can achieve perfect
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>152
    ranking, but can be used to define the baseline ranking function for your search engine
    and data.
    So after this short intro to Lucene4IR you are encouraged to develop your own
    Similarity e.g. based on word2vec and see whether it makes an actual difference e.g. with
    respect to BM25Similarity.
    5.4.1 Averaged word embeddings based Similarity
    We witnessed the effectiveness of document embeddings we have generated using word
    and paragraph vectors in the small experiment of bernhard riemann influence sample
    query. At the same time in real life we need a better evidence of the effectiveness of a
    retrieval model. In this section we will work on a Similarity implementations based on
    averaged word2vec word vectors. We will then measure their effectiveness on a small
    dataset using Lucene4IR project. These measures will help us get a sense of how good
    these ranking models behave in general.
    Extending a Lucene Similarity correctly is a hard task which requires quite some
    insights on how Lucene works. We focus only on the relevant bits of Similarity API to
    use document embeddings for scoring documents against queries. Let’s start by creating
    a WordEmbeddingsSimilarity which creates document embeddings via averaged word
    embeddings. It requires a trained word2vec model, a smoothing method which is used to
    average word vectors to combine them together in a document vector and a Lucene field
    to fetch document content from.
    public class WordEmbeddingsSimilarity extends Similarity {
    public WordEmbeddingsSimilarity(Word2Vec word2Vec, String fieldName, Smoothing smoothing) {
    this.word2Vec = word2Vec;
    this.fieldName = fieldName;
    this.smoothing = smoothing;
    }
    A Lucene Similarity will implement the following two methods:
    @Override
    public SimWeight computeWeight(float boost, CollectionStatistics collectionStats,
    TermStatistics... termStats) {
    return new EmbeddingsSimWeight(boost, collectionStats, termStats);
    }
    @Override
    public SimScorer simScorer(SimWeight weight, LeafReaderContext context) throws IOException {
    return new EmbeddingsSimScorer(weight, context);
    }
    The most important part for our task is to implement the EmbeddingsSimScorer,
    which is responsible for scoring documents.
    private class EmbeddingsSimScorer extends SimScorer {
    @Override
    public float score(int doc, float freq) throws IOException {
    INDArray denseQueryVector = getQueryVector();
    INDArray denseDocumentVector = VectorizeUtils.toDenseAverageVector(
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>153
    reader.getTermVector(doc, fieldName), reader.numDocs(), word2Vec, smoothing);
    return (float) Transforms.cosineSim(denseQueryVector, denseDocumentVector);
    }
    }
    generate the query vector
    generated the document vector
    calculate cosine similarity between document and query vectors and use that as
    document score
    As you can see the score method does what we did in the previous section, but within
    the Similarity class. The only difference with respect to the previous approach is that the
    toDenseAverageVector utility class takes also a Smoothing parameter which specifies
    how to average word vectors.
    public static INDArray toDenseAverageVector(Terms docTerms, double n, Word2Vec word2Vec,
    WordEmbeddingsSimilarity.Smoothing smoothing) throws IOException {
    INDArray vector = Nd4j.zeros(word2Vec.getLayerSize());
    if (docTerms != null) {
    TermsEnum docTermsEnum = docTerms.iterator();
    BytesRef term;
    while ((term = docTermsEnum.next()) != null) {
    INDArray wordVector = word2Vec.getLookupTable().vector(term.utf8ToString());
    if (wordVector != null) {
    double smooth;
    switch (smoothing) {
    case MEAN:
    smooth = docTerms.size();
    break;
    case TF:
    smooth = docTermsEnum.totalTermFreq();
    break;
    case IDF:
    smooth = docTermsEnum.docFreq();
    break;
    case TF_IDF:
    smooth = VectorizeUtils.tfIdf(n, docTermsEnum.totalTermFreq(), docTermsEnum.docFreq());
    break;
    default:
    smooth = VectorizeUtils.tfIdf(n, docTermsEnum.totalTermFreq(), docTermsEnum.docFreq());
    }
    vector.addi(wordVector.div(smooth));
    }
    }
    }
    return vector;
    }
    The getQueryVector does exactly the same, but instead of iterating over docTerms it
    iterates over the terms in the query.
    Lucene4IR project already comes with tools run evaluations over the CACM dataset
    which can be done using different Similarities. Following the instructions on Lucene4IR
    README 5 we can generate statistics to evaluate different rankings. For example here’s
    the precision over the first five results using different Similarities:
    Footnote 5mgithub.com/lucene4ir/lucene4ir/blob/master/README.md
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>154
    WordEmbeddingsSimilarity:
    ClassicSimilarity:
    BM25Similarity:
    LMJelinekMercerSimilarity:
    0.2993
    0.2784
    0.2706
    0.2588
    We get some interesting numbers. First of all VSM with TF-IDF weighting is not the
    worst one. The word embeddings similarity was 2% better than the others, not bad.
    However one simple take away we can get from this quick evaluation is that the
    effectiveness of a ranking model can change depending on the data and that care should
    be taken when choosing one such model. Theoretical results and evaluations must always
    be measured against real life usage of our search engine. Another important thing is
    deciding what to optimize ranking for. It’s often hard to get high precision together with
    high recall, for example. Let’s introduce another metric for the evaluation of a ranking
    model effectiveness called Normalized Discounted Cumulative Gain, or NDCG. NDCG
    measures the usefulness, or gain, of a document based on its position in the result list.
    The gain is accumulated from the top of the result list to the bottom, with the gain of each
    result discounted at lower ranks. If we evaluate NDCG of the above Similarities over the
    CACM dataset, we get some even more interesting results:
    WordEmbeddingsSimilarity:
    BM25Similarity:
    ClassicSimilarity:
    LMJelinekMercerSimilarity:
    0.3894
    0.3805
    0.3805
    0.3684
    VSM and BM25 performed exactly the same, our word embeddings based ranking
    function got a slightly better NDCG value. So if we’re interested in having a more
    precise ranking over the first five results we should probably choose word embeddings
    based rankings, but this last evaluation suggests that for an overall higher NDCG this
    might not make a big difference.
    Additionally, a good solution, also supported by some recent research 6, can be to mix
    classic and neural ranking models by using multiple scoring functions at the same time.
    We can do that by using MultiSimilarity class in Lucene. If we do the same evaluation
    but with different flavours of MultiSimilarities we get that mixing language modelling
    and paragraph vectors yelds the best NDCG values.
    Footnote 6marxiv.org/abs/1606.07869
    WV+BM25 :
    0.4229
    WV+LM :
    0.4073
    WV+Classic :
    0.3973
    BM25+LM :
    0.3927
    Classic+LM :
    0.3698
    Classic+BM25 : 0.3698
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>155
    5.5 Summary
    Classic retrieval models like VSM and BM25 provide good baseline for ranking
    documents but also lack semantic understanding of text capabilities
    Neural information retrieval models aim to provide better semantic understanding
    capabilities for ranking documents
    Distributed representations of words (like the ones generated by word2vec) can be
    combined together to generated document embeddings for queries and documents
    Averaged word embeddings can be used to generate effective Lucene Similarities which
    can achieve good results when evaluated against IR datasets
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>156
    6
    Document embeddings for ranking and
    recommendation
    This chapter covers:
    generating document embeddings using paragraph vectors
    using paragraph vectors in ranking
    common approaches to related content retrieval
    using paragraph vectors in related content retrieval
    In the previous chapter we have started getting into neural information retrieval models
    by building a ranking function based on averaged word embeddings. We averaged word
    embeddings generated by word2vec to obtain a document embedding, a dense
    representation of a sequence of words which demonstrated an interestingly good
    effectiveness in ranking.
    However when we learned about common retrieval models like Vector Space Model with
    TF-IDF and BM25 we have seen that looking at single terms only to rank documents can
    lead to suboptimal results, the context information is discarded. With this in mind we’ll
    dive into generating document embeddings which look not just at words, but at the whole
    text fragment to generate a vector representation which carries as much semantic
    information as possible.
    In this chapter we’re going to explore a technique for learning document embeddings
    'directly', no word vector averaging will be done. Using extensions of the word2vec
    neural network learning algorithms we’ll be able to generate document embeddings for
    text sequences of different granularities (sentences, paragraphs, documents, etc.). We’ll
    experiment with this technique and see how it provides better numbers when used in
    ranking. Additionally we’ll use document embeddings to find 'related content'.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>157
    6.1 Distributed representations for sentences and documents
    In this section we’ll introduce an extension of word2vec that aims at learning document
    embeddings during neural network training. This is different than the previously used
    method of mixing word vectors (averaging and, eventually, smoothing e.g. with TF-IDF
    weights) and often gives better results in capturing document semantics 1. This method,
    also known as Paragraph Vectors 2, extends the two word2vec architectures, Continuous
    Bag of Words and SkipGram models, incorporating information about the current
    document in the context. Word2vec performs unsupervised learning of word embeddings
    by taking fragments of texts of a certain size, called window, and it trains the neural
    network to either predict the context given a word belonging to that context or predict the
    word given a context the word belongs to.
    Footnote 1msee comparisons in arxiv.org/pdf/1507.07998.pdf
    Footnote 2mcs.stanford.edu/~quocle/paragraph_vector.pdf
    Specifically, in word2vec CBOW neural network we had three layers:
    an input layer containing context words
    an hidden layer containing one vector for each word
    an output layer containing the word to predict
    Figure 6.1 Word2vec continuous bag of
    words model
    The nice intuition of the paragraph vectors based methods is to either decorate or replace
    the context with a label representing a document, so that the neural network will learn to
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>158
    correlate words and contexts with labels rather than words with other words. The CBOW
    model gets consequently expanded so that the input layer also contains the label (or
    identifier) of the document that contains the current fragment being used for training:
    Figure 6.2 Distributed memory model from paragraph vectors
    As you can see, the architecture of this model is very similar to CBOW. It 'just' adds an
    input label representing the document in the input layer. Consequently, the hidden layer
    will need to be equipped with a vector for each label, so that at the end of the training we
    will have a vector representation for each of such labels. The interesting thing about this
    approach is that it allows us to handle documents at a different granularity. We can use
    labels for either entire documents or smaller portions of them, like paragraphs or
    sentences. These labels act as a sort of memory that wire contexts to (missing) words,
    therefore this method is called Distributed Memory Model of Paragraph Vectors
    (PV-DM).
    In the case of documents like 'riemann hypothesis - a deep dive into a mathematical
    mystery' it would make sense to use a single label because its text is relatively short. But
    for longer documents, like Wikipedia pages, it could be useful to create a label for each
    paragraph or for each sentence. Let’s pick first paragraph of Riemann’s Wikipedia page :
    'Georg Friedrich Bernhard Riemann (17 September 1826 – 20 July 1866) was a German
    mathematician who made contributions to analysis, number theory, and differential
    geometry. In the field of real analysis, he is mostly known for the first rigorous
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>159
    formulation of the integral, the Riemann integral, and his work on Fourier series'. We
    can tag each sentence with a different label and therefore generate a vector representation
    that will help find similar sentences instead of similar Wikipedia pages.
    Paragraph vectors also extend word2vec SkipGram model by the so called Distributed
    Bag of Words model (PV-DBOW). In the Continuous SkipGram model we a neural
    network with three layers:
    an input layer with one input word
    an hidden layer containing a vector representation for each word in the vocabulary
    an output layer containing a number of words representing the predicted context with
    respect to the input word
    The DBOW model from paragraph vectors inputs labels instead of words, so that the
    network learns to predict portions of text belonging to the document/paragraph/sentence
    having that certain label.
    Figure 6.3 Distributed bag of words model from paragraph
    vectors
    Both PV-DBOW and PV-DV models can be used to calculate similarities between
    labelled documents. Same as in word2vec, they reach surprisingly good results in
    capturing the documents semantics. Let’s try using paragraph vectors on our small
    example scenario by using DL4J ParagraphVectors implementation:
    ParagraphVectors paragraphVectors = new ParagraphVectors.Builder()
    .iterate(iterator)
    .layerSize(50)
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>160
    .minWordFrequency(7)
    .sequenceLearningAlgorithm(new DM<>())
    .tokenizerFactory(new DefaultTokenizerFactory())
    .build();
    paragraphVectors.fit();
    configure paragraph vectors
    set the document embeddings dimensions
    same as in Word2Vec, you can set the minimum frequency threshold for a word to
    be used during learning
    select the chosen paragraph vectors model, in this case PV-DM
    finalize the configuration
    perform (unsupervised) learning over the input data
    Similarly to what we did with word2vec we can ask similarity questions to paragraph
    vectors models:
    what are the nearest labels of label xyx ? This will allow us to find the most similar
    documents (as each document is tagged with a label)
    what are the nearest labels, given a new piece of text ? This will make it possible to use
    paragraph vectors over unseen documents / queries
    If we use titles from Wikipedia pages to train paragraph vectors we can look for
    Wikipedia pages whose title is semantically similar to a certain input text. Suppose you
    want to get information about your next big trip in South America, we can get the the top
    3 closest documents to the sentence 'Travelling in South America' from the paragraph
    vectors model trained above:
    Collection<String> strings = paragraphVectors.nearestLabels('Travelling in South America', 3);
    for (String s : strings) {
    int docId = Integer.parseInt(s.substring(4));
    Document document = reader.document(docId);
    System.out.println(document.get(fieldName));
    }
    get the nearest labels to the given input string
    each label is in the form 'doc_'+ documentId, therefore we only get the document
    identifier part to fetch the document from the index
    retrieve the Lucene document having the given id
    print the document title on the console
    Obtaining the following output:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>161
    Transport in São Tomé and Príncipe
    Transport in South Africa
    Telecommunications in São Tomé and Príncipe
    São Tomé and Príncipe (a South American republic) information about transport
    and telecommunications
    Not perfectly relevant
    If we train paragraph vectors using the whole text of Wikipedia pages, instead of just the
    title, we do not get not relevant results. This is mostly due to the fact that paragraph
    vectors, same as word2vec, learn text representations by looking at the contexts and this
    is harder with shorter texts (titles) than with longer ones (whole text from a Wikipedia
    page).
    Latin America
    Crime and violence in Latin America
    Overseas Adventure Travel
    Document embeddings like the ones generated by paragraph vectors aim to provide a
    good representation of the semantics of the whole text, in the form of a vector. We can
    use them in the context of search to address the problem of semantic understanding in
    ranking. In fact the similarity between such embeddings depends more on the meaning of
    text and less on simple term matching.
    6.2 Using paragraph vectors in ranking
    Leveraging paragraph vectors in ranking is very simple as you can ask the model to
    either provide the vector for an already trained label / document or train a new vector for
    a new one (e.g. an unseen query). While with word vectors we had to decide how to
    combine them (we did it at ranking time but we could have also done it at indexing time),
    paragraph vectors based models make it possible to fetch query and document
    embeddings easily to compare and rank them.
    Before jumping into using paragraph vectors for ranking, let’s make a step back: in the
    previous section we have talked about using the data indexed in Lucene to train a
    paragraph vectors model. That can be done by implementing a LabelAwareIterator, an
    iterator over documents' contents that also assigns label to each Lucene document. We
    tag each Lucene document with its internal Lucene document identifier, so that the label
    will look like doc_1234.
    public class FieldValuesLabelAwareIterator implements LabelAwareIterator {
    private final IndexReader reader;
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>162
    private final String field;
    private int currentId;
    @Override
    public boolean hasNextDocument() {
    return currentId < reader.numDocs();
    }
    @Override
    public LabelledDocument nextDocument() {
    if (!hasNextDocument()) {
    return null;
    }
    try {
    Document document = reader.document(currentId, Collections.singleton(field));
    LabelledDocument labelledDocument = new LabelledDocument();
    labelledDocument.addLabel('doc_' + currentId);
    labelledDocument.setContent(document.getField(field).stringValue());
    return labelledDocument;
    } catch (IOException e) {
    throw new RuntimeException(e);
    } finally {
    currentId++;
    }
    }
    ...
    }
    FieldValuesLabelAwareIterator fetches sequences from an IndexReader (a read
    view on the search engine)
    the contents will be fetched from a single field, not from all possibly existing fields
    in the Lucene document
    the identifier of the current document being fetched
    there’re more documents to read if the current identifier is lower than the number
    of documents in the index
    fetch contents from the Lucene index
    create a new LabelledDocument to be used to train DL4J’s ParagraphVectors, the
    internal Lucene identifier is used as label
    set the contents of the specified Lucene field into the LabelledDocument
    We initialize the iterator for paragraph vectors this way:
    IndexReader reader = DirectoryReader.open(writer);
    String fieldName = 'title';
    FieldValuesLabelAwareIterator iterator = new FieldValuesLabelAwareIterator(reader, fieldName);
    ParagraphVectors paragraphVectors = new ParagraphVectors.Builder()
    .iterate(iterator)
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>163
    .build();
    paragraphVectors.fit();
    create an IndexReader
    define the field to be used
    create the iterator
    set the iterator in ParagraphVectors
    build a paragraph vectors model (still to be trained)
    let paragraph vectors perform (unsupervised) learning
    Once the model has finished training we can use them to re-score documents after the
    retrieval phase.
    IndexSearcher searcher = new IndexSearcher(reader);
    INDArray queryParagraphVector = paragraphVectors.getLookupTable().vector(queryString);
    if (queryParagraphVector == null) {
    queryParagraphVector = paragraphVectors.inferVector(queryString);
    }
    QueryParser parser = new QueryParser(fieldName, new WhitespaceAnalyzer());
    Query query = parser.parse(queryString);
    TopDocs hits = searcher.search(query, 10);
    for (int i = 0; i < hits.scoreDocs.length; i++) {
    ScoreDoc scoreDoc = hits.scoreDocs[i];
    Document doc = searcher.doc(scoreDoc.doc);
    String label = 'doc_' + scoreDoc.doc;
    INDArray documentParagraphVector = paragraphVectors.getLookupTable().vector(label);
    double score = Transforms.cosineSim(queryParagraphVector, documentParagraphVector);
    String title = doc.get(fieldName);
    System.out.println(title + ' : ' + score);
    }
    create an IndexSearcher to perform the first query that identifies the result set
    try to fetch an already existing vector representation for the current query, this can
    fail because we have trained the model over search engine contents and not queries
    if the query vector doesn’t exist, let the underlying neural network train and infer a
    vector on that new piece of text (whose label will be the entire text of the String
    perform search
    build the label of the current doc
    fetch the existing vector for the document having the specified label
    calculate the score as the cosine similarity between the query and document vector
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>164
    print the results on the console
    The above listing shows how easy is to fetch a distribute representation for query and
    documents without having to work on the word embeddings. For the sake of readability
    the results are again shown as scored from best to worst (even though the code above
    doesn’t do that). The ranking is very and the scores are consistent with the document
    relevance:
    riemann hypothesis - a deep dive into a mathematical mystery : 0.7749797701835632
    riemann bernhard - life and works of bernhard riemann : 0.7671164274215698
    thomas bernhard biography - bio and influence in literature :
    0.3246484398841858
    bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard bernhard :
    0.03593694418668747
    The two best documents have high and scores that are rightfully close, the third one has a
    significantly lower score, that is ok because it’s not relevant. Finally, the dummy
    document has been ranked with a close to zero score.
    6.2.1 Paragraph vectors based Similarity
    Similarly we can introduce a ParagraphVectorsSimilarity that leverages paragraph
    vectors to measure similarity between a query and a document. The interesting part of
    this Similarity is the implementation of the SimScorer#score API.
    @Override
    public float score(int doc, float freq) throws IOException {
    INDArray denseQueryVector = paragraphVectors.inferVector(query);
    String label = 'doc_' + doc;
    INDArray documentParagraphVector = paragraphVectors.getLookupTable().vector(label);
    if (documentParagraphVector == null) {
    LabelledDocument document = new LabelledDocument();
    document.setLabels(Collections.singletonList(label));
    document.setContent(reader.document(doc).getField(fieldName).stringValue());
    documentParagraphVector = paragraphVectors.inferVector(document);
    }
    return (float) Transforms.cosineSim(denseQueryVector, documentParagraphVector);
    }
    extract a paragraph vector for the text of the query, if the query was never seen
    before this will imply performing a training step of the paragraph vector network
    extract the paragraph vector for the document with the label equals to its document
    identifier
    if a vector with the given label (docId) can’t be found, perform a training step over
    the paragraph vector network to extract the new vector
    calculate the cosine similarity between query and document paragraph vectors and
    use it as score for the given document
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>165
    6.3 Document embeddings and related content
    As a user you may have experienced the feeling that a certain search result is almost
    good, but for some reason you don’t like that 'enough'. Think about searching for 'a
    book about implementing neural network algorithms' on a retail site. You get the search
    results: the first result is a book titled 'Learning to program neural nets', so you click that
    result and get to a page containing more details about that book and you suddenly realize
    you like the book contents. The author is a recognized authority on the subject but it
    decided to use Python as programming language for teaching examples, which you don’t
    know well enough. You may wonder 'isn’t there a similar such book, but using Java for
    teaching how to program neural nets?'. The retail site may show you a list of 'related'
    books which are similar to that one in the hope that if you don’t want to buy the one with
    examples written in Python, you may instead buy another one with similar contents
    (which may include a book which has examples in Java).
    In this section we’ll see how to provide such 'related' content by finding additional
    documents in the search engine that are similar not just because they are from the same
    author or have some words in common, but rather because there is some more
    meaningful semantic correlation between two such documents. This should remind you
    the semantic understanding issue we addressed in ranking functions using document
    embeddings learned with paragraph vectors.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>166
    6.3.1 Search, recommendations and related content
    In order to understand how much important is indicating appropriate related content
    within a search engine, let’s think about the stream of actions a user performs on a video
    sharing platform (like YouTube or others). The primary (or even only) interface is the
    search box where users write a query, so a user types 'Lucene tutorial' within the search
    box and clicks the search button. A list of search is shown and the user eventually picks
    one of them which he/she finds interesting. From then on, it’s common that the user stops
    searching but rather starts clicking on videos from the 'related' box / column. Typical
    recommendations for a video entitled 'Lucene tutorial' could be videos with titles like
    'Lucene for beginners', 'Intro to search engines' or 'Building recommender systems
    with Lucene'. The user can click on either of these recommendations; for example if the
    user got enough knowledge from the 'Lucene tutorial' video, he / she could then jump
    wathing a more advanced video, otherwise may want to watch another introductory video
    for Lucene, or even one introducing search engines as he/she realised some more prior
    knowledge is required to understand how to use Lucene. This stream of consume
    retrieved content and then navigate through related content can go ahead indefinitely. So
    providing relevant 'related content' is of utmost importance to best satisfy the user need.
    The 'related content' box can even make the user intent shift towards topics that are far
    away from the initial query. In the previous example the user wanted to learn how to use
    Lucene, the search engine provided then provided a related item whose main topic was
    not directly related to Lucene, but on building a machine learning system for generating
    recommendations based on Lucene, that is a big switch: from needing information for
    working with Lucene as a beginner to learning about recommender systems based on
    Lucene (a more advanced topic). This brief example on a video sharing platform can also
    be applied to many e-commerce websites. The main purpose of such websites is to sell
    you something. So while you are encouraged to search for the product you (may) need,
    you are also flooded with lots of 'recommended for you' spots. These recommendations
    are based on:
    which products you have searched for in the past
    which topics you mostly searched for
    new products
    which products you have seen (browsed/clicked) recently
    etc.
    One of the main points of this flood of recommendations is the so called user retention:
    an e-commerce site wants to keep you on browsing and searching as long as possible,
    hoping that any of the products they sell will be interesting enough for you to buy it. This
    goes of course beyond buying and selling. Such capabilities are very important for many
    applications, like in the field of healthcare: a doctor looking at a patient medical records
    would surely benefit from being able to look into similar medical records from other
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>167
    patients (and their histories) in order to perform better diagnosis. We’ll focus now on
    implementing algorithms for retrieving related / similar documents with respect to an
    input document, based on their contents. First we’ll use the search engine itself to extract
    the related content and then we’ll see how to use different approaches to build document
    embeddings to overcome some limitations in the first approach. We’ll also take this
    chance to discuss how to use paragraph vectors to perform document classification,
    which is useful in the context of providing semantically relevant suggestions.
    Figure 6.4 Using neural networks to retrieve related content
    6.3.2 Using frequent terms for finding similar content
    In the previous chapter we have seen how TF-IDF weighting scheme for ranking relies
    on term and document frequencies in order to provide a measure of the importance of a
    certain document. The rationale behind TF-IDF ranking is that a document importance
    grows with the local frequency and global rarity of its terms, with respect to an input
    query. Building on top of these assumptions, we can define an algorithm to find
    documents that are similar to an input document, solely based on the search engine
    retrieval capabilities.
    Wikipedia dumps can be a very good collection to evaluate the effectiveness of an
    algorithm for retrieving related contents. Each Wipiedia page contains quite some content
    and very useful metadata that we will leverage (e.g. title, categories, references and …
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>168
    even some link to related content in the See also section). There are several tools that can
    be used to index Wikipedia dumps into Lucene, like the lucene-benchmark module 3.
    Suppose to have indexed each Wikipedia page with its title and text into two separate
    Lucene indexes. Given the search results returned by a certain query we want to fetch the
    4 top most similar documents to be shown to the end users as related content. In order to
    do that we pick each search result, extract the most important terms from its content (in
    this case from the text field) and perform another query using the extracted terms. The
    first 5 resulting documents can be used as 'related content'.
    Footnote 3msee github.com/apache/lucene-solr/tree/master/lucene/benchmark
    Figure 6.5 Retrieving related content by using a document’s most important terms,
    weighted by TF-IDF scheme
    Suppose we run a query travel hints and get a search result about a traffic circle in New
    Jersey called Ledgewood Circle. We take all the terms contained in the Wikipedia page 4
    and extract the ones that have at least a term frequency of 2 and a document frequency of
    5. This way we obtain the following list of terms:
    Footnote 4msee en.wikipedia.org/wiki/Ledgewood_Circle
    record govern left depart west onto intersect 1997 wish move cite turn township signal 10 lane travel
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>169
    We then use these terms as a query to finally obtain the Documents to be used as related
    content to be presented to the end user.
    Lucene allows to do this by using a component called MoreLikeThis 5 which can extract
    the most important terms from a Document and create a Query object to be run via the
    same IndexSearcher used to run the original query.
    Footnote 5msee
    github.com/apache/lucene-solr/blob/master/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
    Listing 6.1 Searching and getting related content via MLT
    EnglishAnalyzer analyzer = new EnglishAnalyzer();
    MoreLikeThis moreLikeThis = new MoreLikeThis(reader);
    moreLikeThis.setAnalyzer(analyzer);
    IndexSearcher searcher = new IndexSearcher(reader);
    String fieldName = 'text';
    QueryParser parser = new QueryParser(fieldName, analyzer);
    Query query = parser.parse('travel hints');
    TopDocs hits = searcher.search(query, 10);
    for (int i = 0; i < hits.scoreDocs.length; i++) {
    ScoreDoc scoreDoc = hits.scoreDocs[i];
    Document doc = searcher.doc(scoreDoc.doc);
    String title = doc.get('title');
    System.out.println(title + ' : ' + scoreDoc.score);
    String text = doc.get(fieldName);
    Query simQuery = moreLikeThis.like(fieldName, new StringReader(text));
    TopDocs related = searcher.search(simQuery, 5);
    for (ScoreDoc rd : related.scoreDocs) {
    Document document = reader.document(rd.doc);
    System.out.println('-> ' + document.get('title'));
    }
    }
    define an Analyzer to be used while searching and while extracting terms from
    search results' content
    create a MoreLikeThis instance
    specify the Analyzer to be used by MoreLikeThis
    create an IndexSearcher
    define which field to use when doing the first query and when looking for related
    content via the MLT generated query
    create a QueryParser
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>170
    parse the user entered query
    execute the query and return the top 10 search results
    retrieve the Document object related to the current search result
    print current document title and score
    extract the content of the text field from the current Document
    use MoreLikeThis to generate a query based on the content of the retrieved
    Document by extracting the most important terms (TF-IDF ranking wise)
    perform the query generated by MoreLikeThis
    print the title of the Document found by the Query generated by MoreLikeThis
    No machine learning is involved here to extract related content: we use search engine
    capabilities to return related documents containing the most important terms of a certain
    search result. Here’s an example output for the travel hints query and Ledgewood Circle
    search result.
    Ledgewood Circle : 7.880041
    -> Ledgewood Circle
    -> Mount Baker Tunnel
    -> K-5 (Kansas highway)
    -> Interstate 80 in Illinois
    -> Modal dispersion
    The first three related contents in the above example (not counting the Ledgewood Circle
    document itself) are quite similar to the original document. In fact they all relate to things
    very much correlated with traffic circles, like a tunnel or an highway or an interstate
    road. The fourth one though is completely unrelated as it deals with fiber optics stuff.
    Let’s try to dig deeper into why this fourth result were fetched. In order to do this we can
    turn Lucene Explanation on.
    Query simQuery = moreLikeThis.like(fieldName, new StringReader(text));
    TopDocs related = searcher.search(simQuery, 5);
    for (ScoreDoc rd : related.scoreDocs) {
    Document document = reader.document(rd.doc);
    Explanation e = searcher.explain(simQuery, rd.doc);
    System.out.println(document.get('title') + ' : ' + e);
    }
    get the Explanation for the MLT query
    The explanation allows us to inspect that the terms signal, 10, travel and new matched.
    Modal dispersion :
    20.007288 = sum of:
    7.978141 = weight(text:signal in 1972) [BM25Similarity], result of:
    ...
    2.600343 = weight(text:10 in 1972) [BM25Similarity], result of:
    ...
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>171
    7.5186286 = weight(text:travel in 1972) [BM25Similarity], result of:
    ...
    1.9101752 = weight(text:new in 1972) [BM25Similarity], result of:
    ...
    The issue with this approach is that MoreLikeThis extracted the most important terms
    according to TF-IDF weighting. This, as we have already observed in the previous
    chapter, has the problem of just relying on frequencies. Let’s look at these important
    terms extracted from the Ledgewood Circle document text: the terms record, govern, left,
    depart, west, onto, intersect, 1997, wish, move, etc. do not seem to suggest that the
    document deals with a traffic circle. If you try to read them as a sentence you cannot even
    derive much sense from it. As you can see from the explanation we used default Lucene
    BM25Similarity, in the previous chapter we have seen that we can use different ranking
    functions and test whether we can get better results. If we adopt the ClassicSimilarity
    (vector space model with TF-IDF) we get the following:
    Query simQuery = moreLikeThis.like(fieldName, new StringReader(text));
    searcher.setSimilarity(new ClassicSimilarity());
    TopDocs related = searcher.search(simQuery, 5);
    for (ScoreDoc rd : related.scoreDocs) {
    Document document = reader.document(rd.doc);
    System.out.println(searcher.getSimilarity() + ' -> ' + document.get('title'));
    }
    use ClassicSimilarity instead of default (only for the similar content search)
    ClassicSimilarity -> Ledgewood Circle
    ClassicSimilarity -> Mount Baker Tunnel
    ClassicSimilarity -> Cherry Tree
    ClassicSimilarity -> K-5 (Kansas highway)
    ClassicSimilarity -> Category:Speech processing
    We get even worse results as both Cherry Tree and Speech processing are completely
    unrelated with respect to the original Ledgewood Circle document. Let’s try with a
    language model based Similarity (LMDirichletSimilarity 6):
    Footnote 6msee related paper at www.iro.umontreal.ca/~nie/IFT6255/zhai-lafferty.pdf
    Query simQuery = moreLikeThis.like(fieldName, new StringReader(text));
    searcher.setSimilarity(new LMDirichletSimilarity());
    TopDocs related = searcher.search(simQuery, 5);
    for (ScoreDoc rd : related.scoreDocs) {
    Document document = reader.document(rd.doc);
    System.out.println(searcher.getSimilarity() + ' -> ' + document.get('title'));
    }
    LM Dirichlet(2000.000000) -> Ledgewood Circle
    LM Dirichlet(2000.000000) -> Mount Baker Tunnel
    LM Dirichlet(2000.000000) -> K-5 (Kansas highway)
    LM Dirichlet(2000.000000) -> Interstate 80 in Illinois
    LM Dirichlet(2000.000000) -> Creek Turnpike
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>172
    Interestingly enough we got all good sounding results, in fact all of them relate to
    infrastructures for cars, like highways or tunnels.
    MEASURING 'RELATED CONTENT' QUALITY USING CATEGORIES
    In the previous chapter we have learned how important is to do not just single
    experiments. While they allow a fine grained understanding of how retrieval models
    work in some cases, they can hardly give an overall measure of how well such a model
    works on more data. Since Wikipedia pages come with categories, we can make a first
    evaluation of the accuracy of related content using them. If documents found by the
    related content algorithm (in this case Lucene’s MoreLikeThis) fall in any of the original
    document categories we can consider it relevant. In real life you might want to this
    evaluation slightly differently: for example you may also consider relevant a suggested
    document if its category is a sub category of the original document category. You may do
    this (and much more) by building a taxonomy yourself by extracting it from Wikipedia 7
    or by leveraging a DBPedia project (a crowdsourced effort to build structured
    information about content in Wikipedia) 8. However for the sake of our experiments we
    can define an accuracy measure as the sum of the times a related content shares one or
    more categories with the original document divided by the number of related documents
    retrieved. Let’s get pick the Wikipedia page for the soccer player Radamel Falcao, its
    Wikipedia page has lots of categories (1986 births, AS Monaco FC players, …). When
    using BM25Similarity to rank the MLT generated Query, we get the following top 5
    related documents, with the shared category in parenthesis (if any):
    Footnote 7men.wikipedia.org/wiki/Help:Category
    Footnote 8mwiki.dbpedia.org/
    Bacary Sagna (*Expatriate footballers in England*)
    Steffen Hagen (*1986 births*)
    Andrés Scotti (*Living people*)
    Iyseden Christie (*Association football forwards*)
    Pelé ()
    The first four results have a category in common with the Radamel Falcao’s Wikipedia
    page, but Pelè does not. Therefore the accuracy is 4 (the number of results sharing a
    category with Radamel Falcao’s page) divided by 5 (the number of returned similar
    results), which is equals to 0.8.
    To evaluate our algorithm, we can generate a number of random queries and measure the
    above defined average accuracy over the returned related contents. We generate 100
    queries using words that exist in the index (in order to make sure at least one search
    result is returned), then retrieve the 10 most similar documents using paragraph vectors
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>173
    and cosine similarity, for each of these related documents we check whether one of its
    category also appears in the search result.
    Listing 6.2 Fetch related content with MLT query over different Similarities and
    calculate accuracy
    int topN = 10;
    String[] originalCategories = doc.getValues('category');
    Query simQuery = moreLikeThis.like(fieldName, new StringReader(s));
    for (Similarity similarity : similarities) {
    searcher.setSimilarity(similarity);
    TopDocs related = searcher.search(simQuery, topN);
    double acc = 0;
    for (ScoreDoc rd : related.scoreDocs) {
    if (rd.doc == scoreDoc.doc) {
    topN--;
    continue;
    }
    Document document = reader.document(rd.doc);
    String[] categories = document.getValues('category');
    if (categories != null && originalCategories != null) {
    if (find(categories, originalCategories)) {
    acc += 1d;
    }
    }
    }
    acc /= topN;
    System.out.println(similarity + ' accuracy : ' + acc);
    }
    get the categories associated with the original Wikipedia page returned by a query
    create the 'related content query' with MoreLikeThis
    run the same query with multiple Similarity implementations to evaluate what
    works best
    use a specific Similarity within the IndexSearcher
    perform the 'related content query'
    initialize the accuracy to zero
    skip a result if it’s equals to the original document
    retrieve the related Document
    if any of the categories of the related content is contained in the original Document,
    increase the accuracy
    divide the accuracy by the number of returned related documents
    The corresponding output with BM25Similarity,
    LMDirichletSimilarity will look like below:
    ClassicSimilarity
    and
    Muted group theory : 1.1707447
    BM25(k1=1.2,b=0.75) accuracy : 0.2
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>174
    ClassicSimilarity accuracy : 0.2
    LM Dirichlet(2000.000000) accuracy : 0.1
    If you run this over 100 randomly generated queries and corresponding 10 top results, we
    get the following average accuracies.
    BM25(k1=1.2,b=0.75) average accuracy : 0.09
    ClassicSimilarity average accuracy : 0.07
    LM Dirichlet(2000.000000) average accuracy : 0.07
    Given the fact that the best possible accuracy is 1.0, we got quite low accuracy values, in
    fact the best one finds a related document with a matching category only 9% of the times.
    While this is a suboptimal result, it is useful to reason on the above results and the
    availability of the 'category' information in each document.
    First question: did we choose a good metric to measure 'aboutness' of related content we
    retrieved with the above approach ? Categories attached to Wikipedia pages are usually
    of good quality, the Ledgewood Circle page’s categories are Transportation in Morris
    County and Traffic circles in New Jersey. A category like Traffic circles would have also
    been appropriate, but more generic. So the level of detail in the choice of relevant
    categories attached to such articles can vary and therefore influence the accuracy
    estimates we calculated above. Another thing to analyze is whether the categories are
    'keywords' as taken from the text or not, in the case of Wikipedia this is not, but in
    general this might not always be the case. We can think about extending the way we
    measured accuracy by including not just categories a document belongs to, but also to
    important words or concepts that are mentioned in the text. For example the Ledgewood
    Circle page contains a section about a controversy that arose back in the '90s with respect
    to the tree that was planted in the middle of the traffic circle. Such information is not
    represented in any way in the categories. If we would be able to extract concepts
    discussed in a page we could add them as additional categories (e.g. in this case it could
    be a generic 'Controversies' one). You can also think to this as tagging each document
    with a set of generic labels: these can be categories, concepts mentioned in the text,
    important words, etc. The bottom line is that our accuracy measure is as good as the
    labels / categories attached to documents. On the other hand the way we build and
    leverage categories can have a significant impact on our evaluations.
    Second question: did we use this metric appropriately ? We extracted the categories of
    the input document and the related content and see if there’s any category which belongs
    to both. The Ledgewood Circle page doesn’t have the Traffic circle category, however its
    category Traffic circles in New Jersey can be thought as a sub category of a more generic
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>175
    Traffic circle one. Extending this reasoning to all the existing categories in Wikipedia,
    we could imagine to build a tree whose nodes are categories, the deeper a node is, the
    more specific/fine grained its category will be.
    Figure 6.6 Building a taxonomy out of Wikipedia categories
    So in our experiment above, we can change the rule for matching categories from 'at least
    one category should be shared between both the input and related content' into 'at least
    one category should be shared between both the input and related content or one of the
    categories of a certain document should be a specification of another category in the
    other document'. If we know more about what kind of relations exist between categories
    (and labels in general) we can use that information too. DBPedia can be used as one such
    source of information about relations existing between pages. Imagine we have our
    algorithm returning the New Jersey page as related to Ledgewood Circle. The main thing
    they have in common is that Ledgewood Circle is located in the state of New Jersey,
    specifically in the Roxbury Township. If such an information is available, it is a great
    'link' you can navigate to measure the relevance of related content. For example you
    could think of marking as relevant the related documents that have any relation with the
    input document, or only mark it as relevant when the documents are linked by any of a
    subset of the existing relations.
    We mentioned the DBPedia project before. It records a lot of such relations existing
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>176
    between pages from Wikipedia. You can think to it as a graph whose nodes are pages and
    arcs are relations (with a name). We can see the mentioned relations between Ledgewood
    Circle and New Jersey using RelFinder 9.
    Footnote 9mwww.visualdataweb.org/relfinder
    Figure 6.7 Navigating relations between 'Ledgewood Circle' and 'New Jersey' pages in
    DBPedia
    Having a good hierarchical taxonomy for categories is very important when using them
    to measure the accuracy of the results from MoreLikeThis and other related content
    algorithms. On the other hand, information about categories and their relations is often
    not available in practice, in such cases methods based on unsupervised learning can help
    to find out whether two documents are similar. Let’s think about algorithms to learn
    vector representations of texts like word2vec (for words) or paragraph vectors (for
    sequences of words): when you plot them on a graph you’ll have that similar words /
    documents will be placed nearby. In that case, you can group closest vectors together in
    order to form clusters (there are several different ways of doing that, but we won’t cover
    them here). In that case you can consider related words / documents if they belong to the
    same cluster. In the next section we’ll have a look at one of the more straightforward
    usages of document embeddings: finding similar content
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>177
    6.3.3 Similar content retrieval with Paragraph Vectors
    Paragraph vectors learns a fixed (distributed) vector representation for each sequence of
    words fed into its neural network architectures. You can think of feeding one entire
    document into the network, or portions of it, like sections of an article or paragraphs or
    even sentences. It’s up to you to define the granularity. For example if you feed it with
    entire documents, you can ask the network to return you what’s the most similar
    document the network has already seen. Each of the ingested document (and generated
    vectors) is identified by a label.
    Let’s get back at the problem we faced in the first section, find related content for a
    search engine on Wikipedia pages. In the previous section we used Lucene’s
    MoreLikeThis tool to try to extract the most important terms and then use them as a query
    to fetch the related content, unfortunately we got low accuracy rates. The main reasons
    for that are:
    first: the way most important terms were extracted by MoreLikeThis is ok, but could be
    improved
    but most importantly: if you look at the set of important terms of a document, you may
    sometimes not recognize what kind of document they came from
    Let’s pick again our 'dear' Ledgewood Circle page, according to MLT the most
    important terms are :
    record govern left depart west onto intersect 1997 wish move cite turn township signal 10 lane travel
    By no means it would be possible to say that these terms come from the Ledgewood
    Circle page, therefore we can’t expect very accurate related content suggestions. With
    document embeddings there is no explicit information one can look into (and that’s a
    general problem in deep learning, it’s not so simple to understand what these black boxes
    do). Paragraph vectors' neural network just adjusts each document vector values itself
    during training as we explained in chapter 5.
    Let’s fetch related content by finding the nearest vectors to the one representing the input
    document, using cosine similarity. In order to do this we first run a user entered query,
    for example 'Ledgewood Circle', which will return some search results. For each such
    result we extract its vector representation and look to its nearest neighbours in the
    embeddings space. You can think to that as navigating on a graph or map which has all
    documents plotted according to their semantic similarity. You get until the point which
    represents 'Ledgewood Circle' and then find the nearest points and see which documents
    they represent.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>178
    Figure 6.8 Paragraph vectors for Ledgewood Circle and its neighbours, compared with
    music related paragraph vectors
    Similarly to what we did for ranking, we first feed the paragraph vector network with the
    indexed data.
    File dump = new File('/path/to/wikipedia-dump.xml');
    WikipediaImport wikipediaImport = new WikipediaImport(dump, languageCode, true);
    wikipediaImport.importWikipedia(writer, ft);
    IndexReader reader = DirectoryReader.open(writer);
    FieldValuesLabelAwareIterator iterator = new FieldValuesLabelAwareIterator(reader, fieldName);
    ParagraphVectors paragraphVectors = new ParagraphVectors.Builder()
    .iterate(iterator)
    .build();
    paragraphVectors.fit();
    Once that is done we can use DL4J’s built in nearestLabels method to find the document
    vectors that are closest to the vector of 'Ledgewood Circle'. Internally that still uses
    cosine similarity to measure how close two vectors are.
    TopDocs hits = searcher.search(query, 10);
    for (int i = 0; i < hits.scoreDocs.length; i++) {
    ScoreDoc scoreDoc = hits.scoreDocs[i];
    Document doc = searcher.doc(scoreDoc.doc);
    String label = 'doc_' + scoreDoc.doc;
    INDArray labelVector = paragraphVectors.getLookupTable().vector(label);
    Collection<String> docIds = paragraphVectors.nearestLabels(labelVector, topN);
    for (String docId : docIds) {
    int docId = Integer.parseInt(docId.substring(4));
    Document document = reader.document(docId);
    System.out.println(document.get('title'));
    }
    }
    run the original query
    for each result build a 'label'
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>179
    fetch the document embedding for the search result
    find the labels of the nearest vectors to the search result one
    for each nearest vector, parse its label and fetch the corresponding Lucene
    Document
    We obtain the following results:
    Listing 6.3 Nearest documents to 'Ledgewood Circle' using paragraph vectors
    Texas State Highway 186
    Texas State Highway Loop 150
    Farm to Market Road 1000
    Jersey Shore, Pine Creek and Buffalo Railway
    Farm to Market Road 999
    Just from looking into this simple example it seems the results are better than the ones
    provided by MLT. In fact we do not have off topic results, they all relate to transportation
    (while we had the Modal dispersion page, which refers to optics, returned by MLT).
    To confirm our good feelings, we do the same we did for measuring MoreLikeThis
    effectiveness by calculating the average accuracy of this method. In order to make a fair
    comparison we use the same approach of checking if any of the categories of the search
    result (e.g. Ledgewood Circle) also appears in the related content categories. Using the
    same random generated queries used when evaluating MLT, we obtain the following
    average accuracy with paragraph vectors.
    paragraph vectors average accuracy : 0.37
    The best average accuracy for MLT was 0.09, 0.37 is much better. Finding similar
    documents with close semantics is one of the key advantages of using document
    embeddings, that’s also why they are so useful in natural language processing and search.
    As we’ve seen they can be used in various ways, we’ve leveraged them for ranking and
    for similar content retrieval. Paragraph vectors is not the only way you can learn
    document embeddings though. We have used averaged word embeddings in the previous
    chapter, but researchers keep working on better and more advanced ways of extracting
    word and document embeddings.
    In chapter 3 and 4 we have met a deep neural network architecture called encoder
    decoder (or sequence to sequence) model. You may remember that such model is
    composed by an encoder LSTM network and a decoder LSTM network. The encoder
    transforms an input sequence of words into a fixed length dense vector as output, this
    output is the input to the decoder which turns it back into a sequence of words as the final
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>180
    output. We’ve used such an architecture for producing alternative query representations
    and for helping users type a query. In this case we are instead interested in using the
    output of the encoder network, the so called 'thought vector'.
    Figure 6.9 Encoder Decoder model
    The reason why we call it thought vector is that it is meant to be a compressed
    representation of the input text sequence which, when decoded correctly, generates a
    desirable output sequence. Seq2seq models, as we’ll see in the next chapter, are also used
    for machine translation; they in fact transform a sentence in an input language into a
    translated output sequence. What we want to do is extract such thought vectors for the
    input sequences (documents, sentneces, etc.) and use them the same way we used
    paragraph vectors to measure similarity between documents.
    So first thing we need to do is hook into the training phase so that we can 'save' the
    embeddings as they are generated one step at a time. In order to do that we will place
    them in a WeightLookupTable which is the entity responsible for holding word vectors in
    Word2Vec and paragraph vectors in ParagraphVectors object respectively. With DL4J
    you can hook into the training phase with a TrainingListener capturing the forward pass,
    as the thought vector is generated by the Encoder LSTM. We extract the input vector,
    transform it back into a sequence by retrieving words one at a time from the original
    corpus. Then we extract the thought vector and put the sequence with its thought vector
    into the WeightLookupTable.
    Listing 6.4 Extracting thought vectors during Encoder - Decoder training
    public class ThoughtVectorsListener implements TrainingListener {
    @Override
    public void onForwardPass(Model model, Map<String, INDArray> activations) {
    INDArray input = activations.get(inputLayerName);
    INDArray thoughtVector = activations.get(thoughtVectorLayerName);
    for (int i = 0; i < input.size(0); i++) {
    for (int j = 0; j < input.size(1); j++) {
    int size = input.size(2);
    String[] words = new String[size];
    for (int s = 0; s < size; s++) {
    words[s] = revDict.get(input.getDouble(i, j, s));
    }
    String sequence = Joiner.on(' ').join(words);
    lookupTable.putVector(sequence, thoughtVector.tensorAlongDimension(i, j));
    }
    }
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>181
    }
    }
    fetch the network input (a sequence of words transformed into vectors) from the
    input layer
    fetch the thought vector from the thought vector layer
    rebuild the sequence one word at a time from the input vector
    merge the words together in a sequence (as a String)
    record the thought vector associated with the input text sequence
    With these vectors we can reach same level of accuracy with respet to paragraph vectors,
    the difference stands in the fact that we can decide how to influence them. These thought
    vectors are generated as an intermediate product of encoder and decoder LSTM
    networks. We can decide what to put on the encoder input and what to put on the decoder
    output in the training phase. So if we put documents belonging to the same category at
    the edges of the network we will have that the generated thought vectors will learn to
    output documents whose categories are the same. Therefore we can have a much higher
    accuracy with that.
    If we take the Encoder Decoder LSTM we defined in chapter 3 and 4 and train it with
    documents belonging to the same category, we will get an average accuracy of 0.77. That
    is much higher even than paragraph vectors!
    6.4 Summary
    Paragraph vectors models provide distributed representations for sentences and
    documents at configurable granularity (sentence, paragraph, document)
    Ranking functions based on paragraph vectors can be more effective than 'old school'
    statistical models and word embeddings based ones.
    Paragraph vectors can be used to effectively retrieve related content based on document
    semantics
    Thought vectors can be extracted from seq2seq models to retrieve related content based
    on document semantics
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>182
    7
    Searching across languages
    This chapter covers:
    introduction to cross language information retrieval
    overview of statistical machine translation
    sequence to sequence models for machine translation
    word / document embeddings for machine translation
    comparison of effectiveness of different MT methods for search
    applications
    In the first part of the book we have got some basic understanding about what search
    engines and deep neural networks are and how they can fit together to build smarter
    search engines. In the second part of the book we have dived more into technical details
    about major deep neural networks applications for search engines, mostly leveraging
    recurrent neural networks and word / document embeddings.
    In the third part of the book we are going to attack more advanced topics and challenges
    by extending the applications of neural search to searching text in multiple languges and
    images. Finally we will focus on what makes the difference in a lot of production
    senarios: performance, be it plain speed in training / predicting or accuracy of results.
    In this chapter we will see how to build a search engine that can automatically translate
    queries in order to serve content to users speaking in different languages. We will see
    how this can be useful in various context, from the common 'web search' case to more
    specific and life changing cases where it’s very important not to miss some search results
    because of the language barrier.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>183
    7.1 Serving multi language speaking users
    In many scenarios presented in the past chapters we have focused on vertical search
    engines (search engines that are specific for an often small and well defined domain, e.g.
    a search engine for movie reviews). In this chapter we are focusing on the challenge of
    retrieving useful information for users speaking in different languages, there’s no better
    fit than web search (searching over data from everywhere on the World Wide Web). We
    use web search on an every day basis with search engines like Google search, Bing,
    Baidu. Although a lot of online content is written in languages spoken by vast majorities
    (like English), there are a lot of users out there that need to retrieve information and hope
    to find them by using their native language.
    You may wonder at this point what’s the main point here. If you have a Wikipedia page
    written in Italian, that will be surely indexed by e.g. Google Search and you will be able
    to search for it by writing a query on Google Search in Italian.
    Figure 7.1 Searching for 'rete neurale', Italian for 'neural network'
    However when searching, especially for tech related topics, it’s might be useful to write
    queries in English because the amount of information that is available in English for tech
    stuff is often larger than for other languages. So a user writes a query in English to
    maximize the recall, to have as many relevant results as possible. However for such users
    search results written in their own native language may be preferred.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>184
    Figure 7.2 Searching for 'artificial neural network', getting results in Italian too
    In the above image we see that a query written in English also generated an Italian result
    (on the right). Presumably when a query is performed by a logged in user, the search
    engine looks up its native language to possibly include results in such language in
    addition to the ones that simply 'match' the English query.
    How is that useful for the user ? Think about reading your preferred book in your native
    language as opposed to reading it in a second language you studied at school. Even
    though you might be able to understand the contents, a book written in your native
    language might be easier to follow, for example you might be able to better grasp some
    subtle or hard parts better. The same applies here, the Wikipedia entry for 'Artificial
    Neural Network' exists in many different languages but the search engine chooses not to
    show only the English one that matched the original query, instead it highlights the one in
    the native language of the user that entered the query.
    Tasks like the one above can be done by incorporating Machine Translation tools into
    your search engine. In Machine Translation a program is able to translate a sentence
    from an input language into the corresponding version in a target language.
    In the rest of this chapter we’ll see how machine translation tools can help in the above
    scenario to perform text translation at query time, resulting in an improved recall and
    precision for the search engine queries.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>185
    7.1.1 Translating documents vs queries
    Imagine having to build a search engine with capabilities similar to the ones briefly
    outlined in the previous section for a no profit entity that supports refugees around the
    world in administrative or legal stuff. A search engine for such an organization would
    help refugees find appropriate documentation, for example, to fill asylym requests. Each
    and every country around the world would likely require different documents and forms
    to be filled and signed, also depending on the country the applicant comes from. Users of
    such a platform may be speaking their native language but not the language of their
    hosting country. So, in case a refugee from Iceland is seeking for asylym in Brazil, he or
    she would need to retrieve the documents that might be written in Portuguese. If such
    users don’t know Portuguese how would they know what to put in the search query for
    the information they look for ?
    The case above is particularly relevant, however we can also assume that as users we
    want to be able to retrieve content in our mother tongue, whenever possible.
    There are two straightforward ways of doing this by leveraging machine translation:
    you have machine translation programs translating queries in order to find matches in
    more than one language
    you have content created in one language only and you have machine translation
    programs creating translated copies of the same document so that queries can directly
    match the translated ones
    The above options are not mutually exclusive, you can either have one or the other or
    even both. What fits best really depends on the use case. Let’s think about customer
    reviews in sites like Amazon or Airbnb: such reviews are very often written in the
    reviewers native language, so for the purpose of easy consumption of search results, it
    might be good to translate those reviews when they reach the user. Another good case for
    translating search results is the ones concerncing question answering systems. In question
    answering you have an information retrieval system where the user specifies its intent in
    the form of a question written in natural language (e.g. Who was elected president of
    U.S.A. in 2009 ?) and the system replies with an answer, a piece of text hopefully
    informative with respect to the question (e.g. Barack Obama).
    On the other hand for web search, as discussed in the previous section, it might be good
    to translate the query to get results in different languages because that allows more
    choices for the end users. Once that is done an important decision will have to be made
    with respect to ranking: how do we rank results coming from the translated query ?
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>186
    In the case of a refugee searching in Icelandic for documents written in Portuguese the
    user will search for 'pólitísk hæli' (Icelandic version of 'politic asylum'), the query will
    be translated into Portuguese ('asilo politico'). In such use cases both results from the
    original and the translated are retrieved. For the specific use case of a user being an
    asylum seeker, the documents coming from the translated query are more important
    because those are the ones that such a user will need to fill and submit to the local
    authorities.
    In web search that might not be always the case. Let’s get back to the example of the
    Wikipedia page for Artificial Neural Networks: the English version of the page has much
    more information than the Italian version of the page. Depending on different factors,
    like for example users interests and preferred topics, the search engine might decide to
    rank the translated page lower than the original one because it’s less informative. If a
    deep learning researcher performs a web search for 'artificial neural networks' the
    Italian version of the Artificial Neural Network page would not be useful to him / her
    because the amount of information when compared to the original English page is lower.
    If instead the user is considered a newbie on the topic, reading a page in its native
    language would surely help grasping the topic. While a lot depends on the use case, if we
    decide to use machine translation in a search engine, it’s a good idea to rank the
    additional results the same or more than the 'normal' results.
    In the rest of this chapter we’ll focus on translating queries rather than translating
    documents, the principles remain quite similar when translating either short or long
    pieces of text. On the other hand working with very short (e.g. a search query) or very
    long texts (e.g. a long article) is usually harder than working with single sentences from
    the technical perspective.
    7.1.2 Cross language search
    We’ll now have a quick look at how to incorporate machine translation into a search
    engine to translate user queries. In web search the machine translation task is usually
    performed inside the search engine, nothing is said to the user about that. For the other
    use case we mentioned users might want to specify the desired language of search results;
    in fact an asylum seeker would know the desired language for the legal documents
    required to ask for political asylum; on the contrary this information might be not
    available to the search system.
    Going forward we assume we have a set of machine translation tools that can translate
    from the language of the user query to other languages and that our search engine
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>187
    contains documents in many different languages, a common setup for cross language
    information retrieval for web search. The mentioned tools for performing machine
    translation can be implemented in manby different ways; as we go on in the chapter we’ll
    see a few different methods for machine translation. However it’s common for such tools
    to be able to translate text from one source language to another target language. So
    imagine we have a query written in Icelandic, as in the example mentioned before, and
    we have 3 models that can translate from Icelandic to English, from English to Icelandic
    and from Italian to English. The search engine needs to be able to pick up the right tool
    for translating the query. If we pick the Italian to English tool, we can have no translation
    or, even worse, a bad translation coming out from the model. This might cause retrieval
    of unwanted results, which of course would be bad. Even in the case that the
    unappropriate models just give no translation, that would still require CPU and memory
    resources and therefore might negatively impact performance without giving any useful
    outcome.
    To mitigate such issues it’s a good practice to place a language detector program on top
    of the machine translation models. A language detector receiving an input text will output
    the language of the input sequence. You can think to it as a text classifier whose output
    classes are language codes (e.g. en, it, ic, pt, etc.). With the language detector giving us
    the user query language, we can pick up the right machine translation models to translate
    the query. The output text of all the used machine translation models will be used as an
    additional query to be sent to the search engine together with the original one, you can
    think to it as using a boolean operator OR between the original and translated version of
    the query (e.g. pólitísk hæli OR political asylum). See an example flow of using machine
    translation at query time below.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>188
    Figure 7.3 Query translation flow
    Let’s have a look at how cross language search can be implemented on top of Apache
    Lucene. For now we keep the machine translation part a bit abstract on purpose. We’ll go
    over different types of MT models in the subsequent sections discovering advantages and
    weaknesses of each of them. In particular focusing on why most of research and industry
    has switched from the Statistical Machine Translation (based on statistical analysis of
    words and phrases probability distributions) to the so called Neural Machine Translation
    (based on the usage of neural networks).
    7.1.3 Querying in multiple languages on top of Lucene
    Let’s build up on the example of asylum seekers. For example as an Italian refugee in the
    US, I need to fill some legal documents for that. So I type a query in Italian looking for
    documents to enter in the US. That’s what the search engine should do.
    > q: documenti per entrare negli Stati Uniti
    > detected language 'ita' for query 'documenti per entrare negli Stati Uniti'
    > found 1 translation
    > t: documents to enter in the US
    > 'documenti per entrare negli Stati Uniti' was parsed as:
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>189
    '(text:documenti text:per text:entrare text:negli text:Stati text:Uniti)'
    OR
    '(text:documents text:to text:enter text:in text:the text:US)'
    input query
    language detection output
    translated query
    enhanced query containing both the original and translated queries separated by a
    boolean OR clause
    As you might guess, the 'magic' happens during the parsing of the user entered query.
    Here’s a simplified sequence of operations performed by the query parser:
    the query parser reads the input query
    the query parser passes the input query to the language detector
    the language detector determines the language of the input query
    the quary parser picks up machine translation models which can translate the identified
    language into other languages
    each selected machine translation model translates the input query into another language
    the query parser aggregates the input and the translated text in OR clauses of a boolean
    query
    We extend a Lucene QueryParser whose main method #parse transforms a String into a
    Lucene Query object.
    Listing 7.1 Create a BooleanQuery containing the original query
    @Override
    public Query parse(String query) throws ParseException {
    BooleanQuery.Builder builder = new BooleanQuery.Builder();
    builder.add(new BooleanClause(super.parse(query), BooleanClause.Occur.SHOULD));
    ...
    }
    create a boolean query in Lucene
    parse and add the original user query to the boolean query as an OR clause
    Then the input query language is extracted by a language detector tool. There are many
    different ways that can be done, for now we won’t focus on that too much. We’ll use the
    LanguageDetector tool from the Apache OpenNLP project 1.
    Footnote 1mopennlp.apache.org
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>190
    Listing 7.2 Detecting language of the query
    Language language = languageDetector.predictLanguage(query);
    String languageString = language.getLang();
    perform language detection
    get the language code (e.g. en, it, etc.)
    Here we assume we already have the models to perform machine translation loaded, e.g.
    in a Map whose key is the language code (e.g. en for English, it for Italian, etc.) and
    whose value is a Collection of TranslatorTools. For the moment we don’t care too much
    how TranslatorTool is implemented, we’ll focus on this in the next sections.
    Listing 7.3 Picking the correct TranslatorTools
    private Map<String,Collection<TranslatorTool>> perLanguageTools;
    @Override
    public Query parse(String query) throws ParseException {
    ...
    Collection<TranslatorTool> tools = perLanguageTools.get(languageString);
    ...
    }
    get the tools that are capable of translating from the detected language into other
    languages
    Now that we have the machine translation tools loaded, we can use them to create
    additional boolean clauses to be added to the final query.
    for (TranslatorTool tt : tools) {
    Collection<Translation> translations = tt.translate(query);
    for (Translation translation : translations) {
    String translationString = translation.getTranslationString();
    builder.add(new BooleanClause(super.parse(translationString), BooleanClause.Occur.SHOULD));
    }
    }
    return builder.build();
    perform translation of the input query
    iterate over all the possible translations of the input query
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>191
    get the translation text (each translation is composed by the text and its score,
    representing the quality of the translation)
    parse and add the translated query to the boolean query to be returned
    finalize the boolean query building
    With the above we are all set with a query parser which allows to create queries in
    multiple languages. The missing part is to implement the TranslatorTool interface the
    best possible way. In order to do that we’ll start a quick journey into different ways of
    addressing the machine translation task. We’ll start with using a Statistical Machine
    Translation tool and then move to methods based on neural networks; this will help us
    understand the main challenges of translating text but also how the use of neural
    networks based models provides generally better machine translation models.
    7.2 Statistical machine translation
    Statistical machine translation (or SMT) uses statistical approaches to predict what target
    word or sentence is the most probable translation of an input word or sentence. So a SMT
    program should be able to answer the question: 'what’s the most probable English
    translation of the word 'hombre'?'. To do that we train a statistical model over a parallel
    corpus. A parallel corpus is a collection of text fragments (it can be documents or
    sentences or even words) where each content comes in two versions: one is the source
    language (for example Spanish) and the other is the target language (for example
    English).
    s: a man with a suitcase
    t: un hombre con una maleta
    A statistical model is a model that can calculate the probabiluty of source and target text
    fragments. A correctly trained statistical model for machine translation will answer the
    question above providing the translation together with its probability:
    hombre -> man (0.333)
    The probability of a translated text fragment will help us decide whether the translation
    can be considered good or not and therefore whether or not using it for search. A SMT
    model will evaluate the probability of many possible translations and only return the one
    with the highest probability. If ask the SMT model to output all the probabilities for the
    example query 'hombre', we would be able to see high probabilities for sound translations
    and low probabilities estimated for unrelated translations, as in the sample output below.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>192
    man
    (0.333)
    husband (0.238)
    love
    (0.123)
    ...
    woman
    (0.003)
    truck
    (0.001)
    ...
    Under the hood the SMT model will calculate the probabilities of each possible
    translation and 'record' the translation having the best probability. Such an algorithm in
    pseudo code can be seen below.
    f = 'hombre'
    for (each e in target language)
    p(e|f) = (p(f|e) * p(e)) / p(f)
    if (p(e|f) > pe~)
    e~ = e
    pe~ = p(e|f)
    e~ = best translation, the one with highest probability
    pe~ = the probability of the best translation
    calculate the probability of the current target word given the soure word 'hombre',
    see also the Bayes theorem en.wikipedia.org/wiki/Bayes%27_theorem
    if the probability is higher than the current max probability we have a new best
    translation
    record the best translation
    record the best translation probability
    As you can see the algorithm is not complex, the only missing piece is how to calculate
    probabilities like p(e) and p(f|e). In information theory and statistics p(f|e) is the
    conditional probability of e given f, generally speaking you can think to it as the
    probability of the event e occurring as a consequence of event f. In our case 'events' are
    pieces of texts! Without going too deep into statistics, we can assume that word
    probabilities rely on counting frequencies of words. So for example p(man) would be
    equals to the number of times the word man appears in the parallel corpus. Similarly we
    can assume p(hombre|man) is equals to the number of times the word man appears in a
    sentence in the target language which is paired with a sentence in Spanish containing
    hombre. Let’s look at the three parallel sentences below, two of them contain man in the
    source language and hombre in the target sentence, one of them only contains man in the
    source sentence, but not hombre in the target one.
    s: a man with a suitcase
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>193
    t: un hombre con una maleta
    s: a man with a ball
    t: un hombre con una pelota
    s: a working man
    t: un senor trabajando
    In this case p(hombre|man) is equals 2. To make another example, within the above
    parallel sentences p(senor|man) is equals to 1 because the third parallel sentence contains
    man in the source sentence and senor in the target sentence. In summary you can think
    that hombre gets translated to man because, among the many possible alternatives, man is
    the most frequent English word when a Spanish sentence contains hombre.
    We’ve learned some of the basics about statistical machine translation. We’ll get to know
    some of the challenges that make this task harder than it could seem from this
    introduction. Those are important to know because neural machine translation is less
    affected by such problems, and therefore it helps understand the rationale behind the
    current switch from statistical to neural machine translation.
    7.2.1 Alignment
    In the previous section we have learned that a statistical model is build in order to
    translate text. This translation happens via estimation of probabilities based on the
    frequency of words. In practice though there are other factors at play. For example, the co
    occurrence of two words f and e in two source and target sentences doesn’t mean one is
    the translation of the other. In fact if you think to the previously mentioned parallel
    sentences, the words a and hombre co occurr with more frequency than hombre and man.
    So p(hombre|a) = 3, p(hombre|man) = 2 Does that mean that a is the English for hombre
    ? Of course not!
    s: a man with a suitcase
    t: un hombre con una maleta
    s: a man with a ball
    t: un hombre con una pelota
    s: a working man
    t: un senor trabajando
    We can easily notice that man and hombre, when co occurring, are always equally
    aligned. This information is important when deciding whether the right translation for
    hombre is a or man. At the same time translated words are not always perfectly aligned.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>194
    Let’s take the third parallel sentence. We can notice that the right translatino for man is
    senor in that context. However man is in third position in the source sentence while senor
    is placed in the second position in the target sentence.
    s: a working man
    t: un senor trabajando
    The task of dealing with words placed at possibly different position in source and target
    sentences is called word alignment and plays an important role in the effectiveness of
    SMT. What SMT models usually do is to define an alignment function which maps, for
    example, a Spanish target word at position i to an English source word at position j. The
    mapping for the above sentence would transform positions according to the following
    indices: {1 1, 2 3, 3 2}
    s: a working man
    t: un senor trabajando
    a and un are at the same position
    man and senor are back shifted of one position
    Another example where word alignment plays an important role is where there’s no one
    to one mapping between words in different languages. This is especially true with
    languages that do not originate from the same root languages. Let’s take another example
    of an English to Spanish parallel sentence.
    s: I live in the USA
    t: vivo en Estados Unidos
    There are two 'special' cases here :
    the words 'I live' in English is translated to the single word 'vivo' in Spanish
    the word 'USA' in English is translated to two words 'Estados Unidos' in Spanish
    The word alingment function will need to also take care of these cases.
    s: I live in the USA
    t: vivo en Estados Unidos
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>195
    7.2.2 Phrase-based Translation
    So far we have discussed how to translate single words. However, like in many other
    areas of natural language processing, translating a single word is hard without knowing
    the context. Phrase based translation aims to reduce the amount of error due to the
    information lack we have when translating single words. Generally speaking performing
    phrase based translation requires more data to train a good statistical model, however it
    can handle longer sentences better and it’s often more accurate that word based statistical
    models. All the things we learned for word based SMT models apply to phrase based
    models, the only difference is that the translation units are not words anymore, but rather
    phrases.
    When a phrase based model receives an input text, it breaks it into phrases. In fact
    phrases are the fundamental units of phrase SMT models. Each phrase is translated
    singularly, after that the per phrase translations are reordered using a phrase alignment
    function.
    Until the success of neural models for machine translation, phrase (and hierarchical)
    SMT models where the de facto standard for machine translation, used in many tools like
    Google Translate.
    7.3 Working with parallel corpora
    You should have realised that one of the most important things in machine learning is
    having a lot of good quality data. Machine translation models are usually trained on
    parallel corpora, (text) datasets that are provided in two languages so that a words,
    sentences, etc. in the source language can be mapped into words, sentences, etc. in the
    target language.
    A very useful resource for all people interested in machine translation is the Open
    Parallel Corpus 3, also known as OPUS. Lots of parallel resources are provided in the
    OPUS project, you can select the source and target languages and you will be shown a
    list of parallel corpora in different formats. Each parallel corpus is usually provided in
    different XML formats, or dedicated machine translation formats like the one from the
    Moses 4 project; sometimes also translation dictionaries with word frequencies are also
    available.
    Footnote 3mopus.nlpl.eu/
    Footnote 4mwww.statmt.org/moses/
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>196
    In this context let’s setup a small tool to parse the Translation Memory eXchange (or
    TMX) format 5. Although the TMX specification is not new, a lot of the existing parallel
    corpora is available in TMX on the OPUS project so that would be useful when we get to
    train our first neural machine translation model.
    Footnote 5men.wikipedia.org/wiki/Translation_Memory_eXchange
    A sample from a TMX file for translating from English to Italian would look like this:
    <?xml version='1.0' encoding='UTF-8' ?>
    <tmx version='1.4'>
    <header creationdate='Wed Jul 30 13:12:22 2014'
    srclang='en'
    adminlang='en'
    o-tmf='unknown'
    segtype='sentence'
    creationtool='Uplug'
    creationtoolversion='unknown'
    datatype='PlainText' />
    <body>
    ...
    <tu>
    <tuv xml:lang='en'><seg>It contained a bookcase: I soon possessed myself
    of a volume, taking care that it should be one stored with pictures.</seg></tuv>
    <tuv xml:lang='it'><seg>Vi era una biblioteca e io m'impossessai di un
    libro, cercando che fosse ornato d'incisioni.</seg></tuv>
    </tu>
    ...
    </body>
    </tmx>
    In the end what we’re interested in is to simply get the content of the tuv/seg XML nodes.
    What we want is to collect parallel sentences where we can drain source and target text.
    So we create a ParallelSentence class :
    Listing 7.4 ParallelSentence class
    public class ParallelSentence {
    private final String source;
    private final String target;
    public ParallelSentence(String source, String target) {
    this.source = source;
    this.target = target;
    }
    public String getSource() {
    return source;
    }
    public String getTarget() {
    return target;
    }
    }
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>197
    The TMX file format has a many XML nodes tu, one per parallel sentence. Each tu node
    has two child elements tuv: one for the source sentence and one for the target sentence.
    Each such node has a seg node containing the actual text.
    Let’s create a TMXParser class to extract a Collection of parallel sentences by such TMX
    files.
    TMXParser tmxParser = new TMXParser(Paths.get('/path/to/it-en-file.tmx').toFile(),
    'it', 'en');
    Collection<ParallelSentence> parse = tmxParser.parse();
    for (ParallelSentence ps : parse) {
    String source = ps.getSource();
    String target = ps.getTarget();
    ...
    }
    The TMXParser will look inside all tu, tuv and seg nodes and build the above Collection.
    public TMXParser(final File tmxFile, String sourceCode, String targetCode) {
    ...
    }
    public Collection<ParallelSentence> parse() throws IOException,
    XMLStreamException {
    try (final InputStream stream = new FileInputStream(tmxFile)) {
    final XMLEventReader reader = factory.createXMLEventReader(stream);
    while (reader.hasNext()) {
    final XMLEvent event = reader.nextEvent();
    if (event.isStartElement() && event.asStartElement().getName()
    .getLocalPart().equals('tu')) {
    parse(reader);
    }
    }
    }
    return parallelSentenceCollection;
    }
    create a parser on a TMX File, specifying source and target language
    read the file
    create an XMLEventReader, an utility class that reads emits events every time it
    reads XML elements
    iterate over each XML events (nodes, attributes, etc.)
    intercept tu nodes
    parse the tu node and read the contained parallel sentences
    We won’t dig too much into the code for extracting the ParallelSentences because
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>198
    parsing XMLs is not the primary focus here, for completion the important part of the
    parseEvent method follows:
    if (event.isEndElement() &&
    event.asEndElement().getName().getLocalPart().equals('tu')) {
    if (source != null && target != null) {
    ParallelSentence sentence = new ParallelSentence(source, target);
    parallelSentenceCollection.add(sentence);
    }
    return;
    }
    if (event.isStartElement()) {
    final StartElement element = event.asStartElement();
    final String elementName = element.getName().getLocalPart();
    switch (elementName) {
    case 'tuv':
    Iterator attributes = element.getAttributes();
    while(attributes.hasNext()) {
    Attribute next = (Attribute) attributes.next();
    code = next.getValue();
    }
    break;
    case 'seg':
    if (sourceCode.equals(code)) {
    source = reader.getElementText();
    } else if (targetCode.equals(code)) {
    target = reader.getElementText();
    }
    break;
    }
    }
    closing tu element, the ParallelSentence is ready
    read the language code from the tuv element
    read the text from the seg element
    With the generated parallel sentences we can train a machine translation model, be it a
    statistical one, as described in the previous sections or neural, as we’ll see in the next
    sections.
    7.4 Neural machine translation
    With all the background about statistical machine translation and parallel corpora, we are
    now ready to learn about why and how neural networks are used in the context of
    machine translation applied to search.
    Imagine you are the engineer that has the task of building a search engine for a non profit
    organization that helps refugees from all around the world gather information about
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>199
    required legal documentation for each country. You will need several machine translation
    models for as many language pairs (e.g. Spanish to English, Swahili to English, English
    to Spanish, etc.) as possible. You want to cover as many languages as possible.
    Training statistical models based on explicit probability estimation, like the word or
    phrase based SMT models discussed above may be very time consuming because of
    amount of 'manual' work that such approaches usually take. Let’s just mention word
    alignment, this would require quite a lot of work to be done for each of the language
    pairs.
    When the first neural machine translation models were introduced, one of their most
    intriguing features was that they required a very low tuning effort. When Ilya Sutskever
    presented the work he and his co-authors did on encoder decoder architecture for neural
    machine translation 6 he stated: 'we use minimum innovation for maximum results' 7.
    That turned out to be one of the best qualities of this type of models. They are very
    flexible for mapping sequences to sequences in different domains, not just for machine
    translation. For example we have uses them to perform natural language text generation
    for query expansion.
    Footnote 6marxiv.org/abs/1409.3215
    Footnote 7mwww.youtube.com/watch?v=-uyXE7dY5H0
    They simply took a deep Long Short Memory Network ending up in a big vector, the so
    called thought vector we already encountered in chapter 3, and then kept feeding the
    sequence to another decoder LSTM that generated the translated sequence.
    Over time different 'flavours' of NMT models have been proposed but the main idea of
    encoder - decoder networks stays as a milestone as it was the first model fully based on
    neural networks to beat SMT models in a machine translation task.
    We have used seq2seq encoder decoder models for performing query expansion in
    chapter 3 and its thought vectors for related content retrieval in chapter 6, however we
    will now get a bit deeper into how such model works and how the sequences flow inside
    and outside of it.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>200
    7.4.1 Encoder Decoder models
    On a higher level the encoder LSTM reads and encodes a sequence of the source text into
    a fixed-length vector, the thought vector. A decoder LSTM then outputs a transalted
    version of the source sentence from the encoded vector. The encoder–decoder system is
    trained to maximize the probability of a correct translation given a source sentence. So, to
    some extent, these encoder decoder networks, like many other deep learning based
    models, is a statistical model too! The difference with respect 'traditional' SMT is that
    NMT models learn to maximize the correctness of a generated translation via neural
    networks and they do that in an end to end fashion. For example there is no need of
    dedicated tools for word alignment, what an encoder decoder network needs is just a
    huge collection of source / target sentence pairs.
    The key features of encoder decoder models are that:
    they are easy to setup and understand, the model is quite intuitive
    they can handle variable length input and output sequences
    they produce input sequence embeddings that can be leveraged in different ways
    they can be used for sequence to sequence mapping tasks in different domains
    they are an 'end to end' tool, as explained above
    We’ll now get a bit deeper into Encoder-Decoder LSTM models.
    Figure 7.4 An Encoder Decoder model
    Let’s explode the above graph to better understand what’s in each box and how they
    work together.
    The encoder is made up of a recurrent neural network (RNN), usually a Long Short
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>201
    Memory Network (or LSTM) or other alternatives like Gated Recurrent Units 8, which
    we don’t expand on here. You should remember the main difference between a feed
    forward and a recurrent neural network is that the latter has recurrent layers which allow
    to easily work with unbounded sequences of inputs while keeping the size of the input
    layer fixed. The encoder RNN is usually deep, so it has more than one hidden recurrent
    layer. Same as we have seen when we introduced recurrent neural networks in chapter 3,
    we can add more hidden layers if we notice the translation quality is poor even when a lot
    of training data is provided. In general a number of recurrent layers between 2 and 5 is
    enough for training sets in the order of magnitude of tens of GBs. The output of the
    encoder network is the thought vector which corresponds to the last time step of the last
    hidden layer of the encoder network. In case the encoder has 4 hidden layers, the last
    time step of the fourth layer will represent the thought vector.
    Footnote 8msee Kyunghyun Cho’s et al. well known NMT paper: arxiv.org/abs/1406.1078
    For simplicity let’s think about translating a sentence having four words written by an
    Italian user who is looking for information about being able to enter the UK with an
    Italian identity card. The source sentence could be something like 'carta id per gb'. The
    encoder network will be fed with one word of the sentence at each time step. After four
    time steps the encoder network should have been fed with all the four words in the input
    sentence, as per picture below.
    Figure 7.5 An encoder network with 4 hidden recurrent layers
    Note that, in practice, the input sequence is often reversed because it turned out that the
    network can usually give better results. When learning about word2vec in chapter 2 we
    have seen that words are often transformed into one hot encoded vectors to be used
    within a neural network. Back then we then learned word embeddings as an output of the
    word2vec algorithm. In the encoder network we do something similar using a so called
    embedding layer. So we transform the input words into hot encoded vectors,
    consequently the input layer of the network will have its dimension equals to the size of
    the vocabulary of words in the collection of source sentences. Remember in fact that an
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>202
    hot encoded vector for a certain word, e.g. gb, is a vector having only one 1 at the vector
    index assigned to that word, and zeros in all the remaining positions. Before the recurrent
    layer the hot encoded vector is transformed into a word embedding of a layer of lower
    dimension than the input one. Such layer is called the embedding layer and its output is a
    vector representation of the word (a word embedding) similar to the one obtained using
    word2vec.
    So if we look closer into the encoder network layers we should see a stack similar to the
    following:
    Figure 7.6 Encoder network layers (up to second hidden recurrent layer) with a dictionary
    of 10 words
    In the above image we illustrate an input layer of 10 neurons, which implies that the
    source language only contains 10 words, in reality such input layer might contain tens of
    thousands of neurons. The embedding layer reduces the input word size and generate a
    vector whose values are not just zeros and ones, but real values. The embedding layer
    output vector is then passed over to the recurrent layers.
    After processing the last word in the input sequence, a special token (e.g. <EOS> : end
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>203
    of sentence) is passed to the network to signal that the input has finished, and decoding
    should start. This eases the handling of variable length input sequences, because
    decoding won’t start until the end of sentence token is received.
    The decoding part is specular to the encoding part. The only difference is that the decoder
    receives both the fixed length vector and one source word at each time step.
    Figure 7.7 A decoder network with 4 hidden recurrent layers
    No embedding layer is used in the decoder. The probability values in the output layer of
    the decoder network are used to sample a word from the dictionary at each time step.
    Let’s jump to see how to setup such an encoder decoder LSTM with DL4J and see it in
    action.
    ENCODER DECODER FOR MT IN DL4J
    DL4J allows declaring the architecture of your neural network via a so called
    computational graph. That’s a very common paradigm in deep learning framework as
    you can find similar patterns being used in other popular DL tools like TensorFlow,
    Keras and others. A computational graph for a neural network allows to declare which
    layers exist and how they are connected to one another. Think about the encoder network
    layers we defined in the picture in the previous section. We have an input layer, an
    embedding layer and two recurrent (LSTM) layers.
    Listing 7.5 Encoder network computational graph
    ComputationGraphConfiguration.GraphBuilder graphBuilder = builder.graphBuilder()
    ...
    .addInputs('inputLine', ...)
    .setInputTypes(InputType.recurrent(dict.size()), ...)
    .addLayer('embeddingEncoder',
    new EmbeddingLayer.Builder()
    .nIn(dict.size())
    .nOut(EMBEDDING_WIDTH)
    .build(),
    'inputLine')
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>204
    .addLayer('encoder',
    new LSTM.Builder()
    .nIn(EMBEDDING_WIDTH)
    .nOut(HIDDEN_LAYER_WIDTH)
    .activation(Activation.TANH)
    .build(),
    'embeddingEncoder')
    .addLayer('encoder2',
    new LSTM.Builder()
    .nIn(HIDDEN_LAYER_WIDTH)
    .nOut(HIDDEN_LAYER_WIDTH)
    .activation(Activation.TANH)
    .build(),
    'encoder');
    ...
    specify an input type for a recurrent neural network
    create an embedding layer
    the embedding layer expects a number of inputs equals to size of the word
    dictionary
    the output embedding vector width
    the embedding layer takes an input labelled as 'inputLine'
    add the encoder layer
    the first layer of the encoder is a LSTM layer
    use a tanh function in the LSTM layers
    the second layer of the encoder (another LSTM layer)
    The decoder part will look like follows:
    ...
    .addLayer('decoder',
    new LSTM.Builder()
    .nIn(dict.size() + HIDDEN_LAYER_WIDTH)
    .nOut(HIDDEN_LAYER_WIDTH)
    .activation(Activation.TANH)
    .build(),
    'merge')
    .addLayer('decoder2',
    new LSTM.Builder()
    .nIn(HIDDEN_LAYER_WIDTH)
    .nOut(HIDDEN_LAYER_WIDTH)
    .activation(Activation.TANH)
    .build(),
    'decoder')
    .addLayer('output',
    new RnnOutputLayer.Builder()
    .nIn(HIDDEN_LAYER_WIDTH)
    .nOut(dict.size())
    .activation(Activation.SOFTMAX)
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>205
    .lossFunction(LossFunctions.LossFunction.MCXENT)
    .build(),
    'decoder2')
    .setOutputs('output');
    also the decoder recurrent layers are based on LSTMs
    a normal RNN output layer
    the output is a probability distribution generated by the Softmax activation
    the cost function used is multi class cross entropy
    The graph will look slightly more complex than you might expect because the decoding
    part needs to take :
    the thought vector at each time step
    and the source language word at each decoding time step
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>206
    Figure 7.8 Encoder Decoder model with two LSTMs layers per side
    With the computational graph built above, we are ready to train the network with our
    parallel corpus. In order to do that we build a ParallelCorpusProcessor that allows to
    process a parallel corpus, e.g. in the form of a TMX file downloaded from the OPUS
    project. Such processor extracts the source and target sentences, builds the dictionary of
    words. Then it will be used to provide the input and output sequences required for the
    training of the encoder decoder model.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>207
    File tmxFile = new File('/path/to/file.tmx');
    ParallelCorpusProcessor corpusProcessor = new ParallelCorpusProcessor(tmxFile, 'it', 'en');
    corpusProcessor.process();
    Map<String, Double> dictionary = corpusProcessor.getDict();
    Collection<ParallelSentence> sentences = corpusProcessor.getSentences();
    the TMX file containing the parallel corpus
    the processor will parse the TMX file and extract source and target sentences based
    on the language codes (e.g. 'it' for the source, 'en' for the target)
    process the corpus
    retrieve the corpus dictionary
    retrieve the parallel sentences
    The dictionary will then be used to setup the network because the dictionary size defines
    the number of inputs (for hot encoded vectors). The dictionary is in this case a Map
    whose keys are the words and whose value is a number to the word when feeding it into
    the embedding layer. The sentences and the dictionary will be required to build an
    iterator over the parallel sentences. A DataSetIterator over the parallel corpus will be
    then used to train the network across different epochs.
    ComputationalGraph graph = createGraph(dictionary.getSize());
    ParallelCorpusIterator parallelCorpusIterator = new ParallelCorpusIterator(corpusProcessor);
    for (int epoch = 0; epoch < EPOCHS; epoch++) {
    while (parallelCorpusIterator.hasNext()) {
    MultiDataSet multiDataSet = parallelCorpusIterator.next();
    graph.fit(multiDataSet);
    }
    }
    build the network using the computational graph
    build the iterator over the parallel corpus
    iterate over the corpus
    extract a batch of input and output sequences
    train the network over the current batch
    The network will start learning to generate English sequences from Italian sequences.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>208
    Figure 7.9 Encoder decoder network training graph
    The translation performed by the network will consist of a feed forward pass for all the
    words in the input sequence across the encoder and decoder networks.
    We let the encoder network implement the TranslatorTool API, the output method
    performs the feed forward pass in the neural network. That gives the translated version of
    the source sentence.
    Listing 7.6 Implementing the TranslatorTool API
    @Override
    public Collection<Translation> translate(String text) {
    double score = 0d;
    String string = Joiner.on(' ').join(output(text, score));
    Translation translation = new Translation(string, score);
    return Collections.singletonList(translation);
    }
    The output method takes care of transforming the text sequence into a vector and then
    passing it along the encoder and decoder networks. The text vector will be fed into the
    network using the word indices that were generated by the ParallelCorpusProcessor. So
    we transform a String into a List<Double> which is the ordered list of word indices
    corresponding to each token in the source sequence.
    Collection<String> tokens = corpusProcessor.tokenizeLine(text);
    List<Double> rowIn = corpusProcessor.wordsToIndexes(tokens);
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>209
    We now prepare the actual vectors to be used as input to both the encoder (the input
    vector) and the decoder (the decode vector) and perform a feed forward pass separately
    for the encoder and decoder network.
    Listing 7.7 Encoder feed forward pass
    net.rnnClearPreviousState();
    Collections.reverse(rowIn);
    Double[] array = rowIn.toArray(new Double[0]);
    INDArray input = Nd4j.create(ArrayUtils.toPrimitive(array), new int[] {1, 1, rowIn.size()});
    int size = corpusProcessor.getDict().size();
    double[] decodeArr = new double[size];
    decodeArr[2] = 1;
    INDArray decode = Nd4j.create(decodeArr, new int[] {1, size, 1});
    net.feedForward(new INDArray[] {input, decode}, false, false);
    The decoder feed forward pass is slightly more complex because it expects to use the
    thought vector generated by the encoder pass and the source sequence token vectors. So
    that at each timestep the decoder performs a translation given the thought vector and a
    source sentence token vector.
    Listing 7.8 Decoder feed forward pass
    Collection<String> result = new LinkedList<>();
    GravesLSTM decoder = (GravesLSTM) net.getLayer('decoder');
    Layer output = net.getLayer('output');
    GraphVertex mergeVertex = net.getVertex('merge');
    INDArray thoughtVector = mergeVertex.getInputs()[1];
    for (int row = 0; row < rowIn.size(); row++) {
    mergeVertex.setInputs(decode, thoughtVector);
    INDArray merged = mergeVertex.doForward(false);
    INDArray activateDec = decoder.rnnTimeStep(merged);
    INDArray out = output.activate(activateDec, false);
    double idx = sampleFrom(output);
    result.add(corpusProcessor.getRevDict().get(idx));
    double[] newDecodeArr = new double[size];
    newDecodeArr[idx] = 1;
    decode = Nd4j.create(newDecodeArr, new int[] {1, size, 1});
    }
    return result;
    Finally everything is set to start translating queries using the encoder decoder network. In
    practice you will be performing the training phase outside of the search workflow. Once
    training is finished the model will be persisted to disk and then picked up by the query
    parser we defined at the beginning of this chapter.
    ComputationGraph net ...
    File networkFile = new File('/path/to/file2save');
    ModelSerializer.writeModel(net, networkFile, true);
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>210
    The query parser will be created using the encoder decoder network for Italian sentences
    (and the language detector tool).
    File modelFile = new File('/path/to/file2save');
    ComputationGraph net = ModelSerializer.restoreComputationGraph(modelFile);
    net.init();
    TranslatorTool mtNetwork = new MTNetwork(modelFile);
    Map<String, Collection<TranslatorTool>> mappings = new HashMap<>();
    mappings.put('ita', Collections.singleton(mtNetwork));
    LanguageDetector languageDetector = new LanguageDetectorME(new
    LanguageDetectorModel(new FileInputStream('/path/to/langdetect.bin')));
    MTQueryParser MTQueryParser = new MTQueryParser('text', new StandardAnalyzer(),
    languageDetector, mappings);
    The query parser internal logging will tell us how it is translating incoming queries.
    Suppose an Italian user wants to know whether its identity card is valid or not in the UK,
    its query will be written in Italian and translated to English using our encoder decoder
    network.
    > q: validità della carta d'identità in UK
    > detected language 'ita' for query 'validità della carta d'identità in UK'
    > found 1 translation
    > t: identity card validity in the UK
    > 'validità della carta d'identità in UK' was parsed as:
    '(text:validità text:della text:carta text:identità text:in text:UK)'
    OR
    '(text:identity text:card text:validity text:in text:the text:UK)'
    7.5 Summary
    Machine translation can be very useful in the context of search to improve the user
    experience of multi language speaking users
    Statistical models exist that can reach a good translation accuracy, however the amount
    of per language pair tuning required is non trivial
    Neural machine translation models look into ways of learning to translate sequence of
    text into different languages in a less articulated yet more powerful way
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>211
    8
    Content based image search
    This chapter covers:
    an introduction to content based image retrieval
    overview of convolutional neural networks
    search techniques for similar image search
    In the previous chapter we have discovered how to build a search engine that can handle
    content and queries written in multiple languages. So far in the book we have discussed
    several ways neural networks can help users search through text documents: making use
    of word2vec for generating word synonyms, expanding text queries via RNNs, better
    ranking text search results using word / document embeddings, translating text queries
    with encoder / decoder models. Most users make use of search engines by writing text
    queries and consuming (reading) text results. However it’s nowadays very common to
    expect search engines to be 'smarter' and allow us to search using voice (e.g. using
    smartphones microphone) and return us text documents as well as relevant images,
    videos and more.
    Text metadata (title, image content description, tags, etc.) can always be used when
    indexing images and search through them using classical information retrieval techniques
    (along with the ones described in this book). However we do not want to ask someone to
    manually craft descriptions and tags for our images before they get indexed, it would be
    very nice if we would be able to index images as they are and be able to search for them
    without any manual intervention. In this chapter we will look at the image search use
    case to allow users to search through images based on their contents rather than based on
    the text describing them: we will briefly touch traditional ways of extracting manually
    crafted features from images as well as how deep learning techniques are especially good
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>212
    at capturing image features with less manual work. We will then look into how to
    incorporate such techniques in the context of search.
    Back in chapter 1 we gave a brief introduction about deep learning most promising
    aspects; back then we mentioned the example of learning a representation of images that
    can capture image semantics incrementally as the number of layers grows: from pixels to
    objects.
    Figure 8.1 Learning image abstractions incrementally
    In this chapter we’ll finally meet deep neural networks that can learn image
    representations this way. Before jumping to study them and see how they are better than
    other approaches to 'understand' image contents, let’s see why they are needed at all !
    If you have ever fancied composing a postcard by using some royalty free image
    available on the internet then you might already have experienced some problems when
    searching for images relevant to a certain topic of your interest. If you’ve bought a small
    car model for your nephew, you might want to print a postcard of a car to write
    something for him / her. So you go to a search engine for images like Google Images,
    Adobe Stock, etc. and you will type something like 'sport car'. Apart from the search
    results, the important thing to focus on is that users look for images that contain some
    object, or some peculiar feature. You might want a 'red sport car' or a 'vintage sport
    car'. Some search engines for images provide a feature called query by example where
    you upload or take a picture to be used as your input query so that the search engine will
    return similar images to the input one.
    Let’s freeze our running query for a moment and think about how images are produced,
    for example when taken using a camera or by drawing using some graphics software.
    You take a picture and suddenly you have a file stored somewhere which contains binary
    data (0s and 1s). You can think of an image stored in a computer as a grid with a certain
    width and height where each cell in the grid is called a pixel having a certain color. A
    colored pixel can be represented in different ways, as there are several color schemes that
    can be used to describe colors. For the sake of simplicity we’ll pick the most common
    scheme called RGB (Red, Green, Blue) in which each color is made by a mixture of some
    red, some green and some blue. Each such color can have different range values (e.g.
    from 0 to 255) indicating the 'amount' of red, green, blue to be used in each combination
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>213
    (there’s not just one red). Each such value can be represented with 8 binary values (as
    2^8 = 256 so it contains all the possible ranges from 0 to 255). So a RGB image will have
    a grid whose pixels will be made by binary values representing their colors 1.
    Footnote 1mwhile in practice images can have lots of different formats and color schemes, the core problem is
    that images are usually stored as plain binaries, optionally with some metadata that usually doesn’t tell anything
    about their contents
    With this in mind let’s unfreeze our query: how can we match a query for 'sport car'
    when images are just series of bits ?
    In the next sections we’ll see a few different ways we can make queries and images
    match, and learn a few different ways we can find the particular sportcar we want.
    8.1 A look back: text based image retrieval
    Users naturally tend to think about images in terms of what objects they contain (like
    sport car), rather than their bit / RGB values. But shapes and colors are actually better for
    specifying 'the information need,' as in the thing they’re looking for, whether it’s a red
    sport car, a formula 1 sport car or some other kind.
    A 'not so smart' but common approach to mitigating the problem of matching text
    queries with binary images is to add metadata to images during indexing. This allows you
    to do a normal text search with a text query, and it will return images that have metadata
    text attached to them that match the query. Using our sport car query, let’s assume there
    are four images that can match that query. During indexing we can ingest both the image
    data and a small caption describing each image. The image data will be used to return the
    actual image content to the end user (in the search result list), the text description of the
    image is instead indexed for queries and images to match.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>214
    Figure 8.2 Manually captioned sport car images
    If we search for 'sport car', the search engine will return all of the above images. If we
    search for 'black sport car' we’ll only have two of them in the result list (recall that using
    double quotes in queries will force matches on the entire phrase black sport car rather on
    the single words black, sport and car).
    The above approach can be performed in Lucene in a straightforward way. We store the
    image binary as it is but index (the description will not be returned with the search
    results) a manually entered description of the image during indexing.
    Listing 8.1 Ingesting binary image and text descrption
    byte[] bytes = ...
    String description = ...
    Document doc = new Document();
    doc.add(new StoredField('binary', bytes));
    doc.add(new TextField('description', description, Field.Store.NO));
    writer.addDocument(doc);
    writer.commit();
    obtain the image content as a byte[]
    write an image description as a String
    add the image binary content as a stored field
    add the image description as a text field
    index the image document
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>215
    commit the index changes
    At search time a simple text query can be used.
    Listing 8.2 Text based image search
    DirectoryReader reader = DirectoryReader.open(writer);
    IndexSearcher searcher = new IndexSearcher(reader);
    TopDocs topDocs = searcher.search(new PhraseQuery('description',
    'black',
    'sport',
    'car'), 3);
    for (ScoreDoc sd : topDocs.scoreDocs) {
    Document document = reader.document(sd.doc);
    IndexableField binary = document.getField('binary');
    BytesRef imageBinary = binary.binaryValue();
    ...
    }
    open an IndexReader over the index containing the images
    create an IndexSearcher to run the query
    run a query for 'black sport car' on the 'caption' field
    fetch each matching document
    retrieve the 'binary' field
    retrieved the actual image as a binary and do something with it
    This approach can work for a small number of images. But it is nowadays very common
    to have data whose size is in the order of magnitude of millions or billions of documents.
    Even a small online shop for postcards will probably have hundreds or thousands of
    images. In many cases it is not possible to ask persons to take the (not so nice) task of
    looking at each and every image and come with a good descriptive text. Sometimes such
    texts might be found to not be good enough for some search cases 2. To sum it up, this
    approach doesn’t scale and it is only as good as the quality of description: poor
    descriptions will lead to poorly relevant search results.
    Footnote 2min production systems it is not so uncommon to have issues like 'why is the query black sport car
    not returning the black fluo sport car? Please change the description so that it can match such a query'
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>216
    8.2 Understanding images
    As I said, an image can be described in different ways, and the most common is to
    specify the persons, objects, animals and other recognizable objects it contains. So, for
    example 'this is a picture of a man.' Additionally we could mention some descriptive
    details; for example, 'this image shows a tall man.' As you can see with the following
    images, however, these short descriptions are prone to ambiguity. The ambiguity comes
    from the simple fact that one object or entity can be described in many different ways.
    Figure 8.3 Some images described as 'tall man'
    All three images certainly fit the description of 'tall man' that you might use as a text
    query. The image in the middle, however, is different from the others. Yes, it is a picture
    of a 'tall man,' but it is also a picture of a player from the Houston Rockets NBA
    basketball team. So other phrases, including 'basketball player', 'houston rockets
    player', 'basketball player wearing a 35 numbered jersey' describe that image as well. It
    is impossible for a human being tasked with the job of writing short metatags to think of
    every possible way that an image could be described.
    In the same vein, a description like 'basketball player wearing a 35 numbered jersey'
    would not only fit well on the image in the middle of Figure (?). It also applies perfectly
    to the following images, which are of an entirely different player on entirely different
    teams. So in this case, the searcher may be looking for one kind of image, and get another
    kind altogether, even though both have the descriptive metatag, and both woulkd show
    up in the search results.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>217
    Figure 8.4 Some images described as 'basketball player wearing a 35 numbered jersey'
    These simple examples teach us that text is very much prone to ambiguity and
    mismatches because a single entity (a person, an animal, an object, etc.) can be described
    in many different ways. This makes the quality of search results dependent on the way
    the user defines queries and documents. We have seen such problems everywhere in the
    context of search already, that’s one of the reasons why we have synonyms, query
    expansion, etc.
    In contrast, images, visually speaking, are generally less affected by this kind of
    ambiguity. Let’s take the first image described as 'tall man' and imagine that we can find
    images that are visually similar to that one.
    Figure 8.5 Some visually similar images
    An input image allows for a better definition of what’s inside the image, regardless of the
    different ways it can be textually described. At the same time it’s easy to say whether an
    image is not similar to the input one, for example the basketball player image from the
    first set of images is clearly different from the images above in the color and the type of
    dressing of the guy in the picture.
    Using sample images instead of text as input queries (also called query by example) is
    very common in image search platforms where systems try to extract semantic
    information from the images for accurate retrieval rather than having text metadata
    describing each image. So a user expresses its query intent by means of a visual query.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>218
    Same as it happens in text queries, the quality of the query has an impact on the relevance
    of results. Now we have all the pieces we need to start looking into some algorithms to
    extract information from images and represent them in a way that makes it possible to
    run queries that return meaningful results.
    8.2.1 Image representations
    The biggest challenge at this point is how to describe images in a way that makes it
    possible to find similar images. In the example we mentioned we wanted to create a
    postcard for a gift we want to give someone. It would be great if we could take a picture
    of our gift with a camera and use that as our query to the image search engine. That way
    we can have a postcard with a nice looking picture that somehow suggests what’s inside
    the gift box when we will actually give it to the recipient.
    Although images are made by pixels, it is not possible to perform a plain pixel
    comparison. In fact pixel values alone do not provide enough information about what’s
    inside an image. One problem is that a pixel only represents a very tiny portion of the
    image, so it gives no information about its context. A red pixel might be part of a red
    apple or a red car: there’s no way to determine which one it comes from by looking at
    pixels alone. Another problem is that a large, high-quality these days may contain
    millions of pixels, so performing pairwise comparison would hardly be computationally
    efficient. Additionally even two pictures of the same object taken with the same camera
    in the exact same conditions (light, exposure, etc.), but taken from two slightly different
    angles would likely generate very different binary images, pixel wise.
    But we want to take a picture of our gift, the red sport car model, while we are at home
    without caring about light conditions and the exact angle we shoot the photo from. And
    we still want the image search engine to return a nice picture (possibly not a photo) of a
    red sport car, preferably the very same model sport car, maybe on a circuit.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>219
    Figure 8.6 A red sport car photo retrieved by the image search engine based on a picture
    of a red sport car model
    In order to overcome the problem of pixels providing poor information, the most widely
    used technique to [FL: creating a search-worthy picture??] is to extract visual features
    [FROM?] images. These visual features incorporate more (visual) semantics than pixels.
    Such features are then transformed into fixed-sized vectors that are used to represent
    images instead of pixels. The search engine will have to be able to work with such
    features to find similar images in the 'query by example' scenario.
    Visual features can be of different types:
    they can refer to global features like the colors used across the image, or identified
    textures, global / average values around RGB and other kinds of color models (like
    CMYK, HSV, etc.)
    they can refer to local features (extracted from portions of the images) like edges, corners
    and other interesting key points in image cells (as done in methods like scale invariant
    feature transform, speeded up robust features, difference of gaussians, etc.)
    they can be learned end to end as semantic abstractions close to human cognition process
    thanks to the usage of deep neural networks //FL: lots of important info in these 3 bullets!
    Please 'unpack' and expand your explanation, define what yopu mean, give examples,
    etc so it is clearer what you mean.
    The first two sets of features are often referred to as 'hand crafted' features as the
    respective algorithms have been designed and tuned for the purpose based on some
    heuristics [FL: such as?]. Deep learning based algorithms for image representations often
    'just' feed the network layers with pixels coming out with some feature vectors, e.g.
    based on the labels (or classes) each image is tagged with.
    Let’s have a look at some methods to extract both local and global hand crafted features
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>220
    first, then we’ll focus on deep learning based feature learning for images.
    FEATURE EXTRACTION
    Many cameras today allow you to review a picture as soon as it’s been taken. Some of
    them also provide information about the amount of color contained in the picture for each
    of the three RGB channels (red, green, blue). Let’s take as an example a picture of a
    butterfly, as in Figure [Fig number].
    Figure 8.7 A picture of a butterfly
    The camera used to take that picture provides its color histogram, which you can see in
    Figure [Fig number]:
    Figure 8.8 Color histogram for the butterfly picture
    A color histogram is a representation of the distribution of the three color channel
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>221
    possible values (e.g. from 0 to 255) among the pixels. For example if a certain pixel as a
    red channel value of 4 and another pixel has the same value, the color histogram for the
    red channel for that image will have a size of 2 for the value 4 (2 pixels have red channel
    value equals to 4). That applied to all the channels and pixels of a certain image will
    produce the three red, green and blue graphs shown in the image. The color histogram is
    an example of a very simple and intuitive global feature that can be used to describe an
    image.
    We use the open source library LIRE (Lucene Image REtrieval) 3 in order to extract the
    color histogram from an image. LIRE provides a lot of useful tools that are Lucene
    friendly to work with images 4.
    Footnote 3mnote that it’s licensed under GNU GPL 2 license
    Footnote 4mat the time of writing, it doesn’t support yet any deep learning based methods to extract image
    features
    Listing 8.3 Extracting the color histogram from an image with LIRE
    File file = new File(imgPath);
    SimpleColorHistogram simpleColorHistogram = new SimpleColorHistogram();
    BufferedImage bufferedImage = ImageIO.read(file);
    simpleColorHistogram.extract(bufferedImage);
    double[] features = simpleColorHistogram.getFeatureVector();
    the image File
    create a color histogram object
    read the image from the file
    extract the color histogram from the image
    extract the color histogram feature vectors as a double array
    Such a global representation of images has the advantage of being human interpretable
    and usually efficient in terms of performance. However if you think for a moment to the
    fact that the color histogram image representation is bound to color distribution over the
    image (disregarding position), it’s not hard to realise that an image with the same or
    similar color distribution but different contents might easily exist. Consider the previous
    butterfly image and the second image of a butterfly below.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>222
    Figure 8.9 Another picture of a butterfly
    Although the butterfly is the primary subject in both images, they have different color
    schemes, in the former image the main colors are yellow and green while in the second
    one the main colors are blue and yellow. Their histograms will look very much different
    and therefore they won’t be considered similar by the search engine.
    Figure 8.10 Comparing histograms of two butterfly images
    The color histogram scheme is just one of many possible ways of extracting global
    features but in general they suffer the problem that is hard to capture image details. One
    detail that would help immensely is distinguishing background regions of a photo from
    the central image. If the representations for the two butterfly images could somehow
    understand that the regions of the image that contain the butterfly are more important
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>223
    than the background portions. One situation where global features can work well is
    duplicate image detection, where the searcher is looking for an image very similar, if not
    exactly the ome, to the one in hand.
    In contrast to global features, local features can more accurately capture details of
    portions of the images. So, if you want to make a computer detect potentially relevant
    objects (e.g. a butterfly) in an image, a common approach is to start by splitting that
    image into smaller cells, and then look into those cells for relevant shapes or objects.
    Let’s see how this works, using the same butterfly picture, now split into smaller cells
    (Figure number).
    Figure 8.11 Analyzing image cells
    We are assuming here that an algorithm would be able to identify objects in an image
    portion. As you can see from this image, it is very likely that relevant objects are not
    centered so that they appear all within a single cell. Therefore the task of local features
    extraction consists of two parts:
    find interesting points (rather than objects)
    encode interesting points with respect to local region into a descriptor that can be used
    later to match interesting regions //FL: Is the word 'interesting' common in this field?
    'Interesting is a very broad term with lot of meanings. I have been interpreting this to
    mean 'relevant' as in relevant to the search query. //FL: Show a mental model of this
    process—or at least explain what we are going to do—so we understand why we are
    going through the hnext series of steps
    Typical kinds of local features include human understandable visual features like edges
    and corners. However in practice local features like scale invariant feature transform
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>224
    (also known as SIFT) or speeded up robust features (also known as SURF) are used.
    While finding edges is a relatively simple task that can be solved using mathematical
    tools like Fourier, Laplace or Gabor transforms. SIFT or SURF algorithms are more
    complex but more powerful. With SIFT, for example, it is possible to recognize
    important regions in an image so that an object and a rotated version of the same object
    will produce same or similar local features. This means that with SIFT based features
    images that contain the same rotated objects can be recognized as similar.
    Local features, like SIFT, are representations of portions of an image. A single image
    will be associated to several local features. However we need to come with a single
    representation of an image so that:
    the final image representation will contain information about all the interesting local
    points
    efficient comparison can be performed at query time (one feature vectors vs many
    different feature vectors)
    In order to do that local features need to be aggregated into a single representation (a
    feature vector). To do so a common approach is to aggregate local features using Bag of
    Visual Words model (or BoVW). You might recall that we mentioned Bag of Words
    model for text several times in the book. In such a model a document is represented as a
    vector whose size is equals to the number of existing words in all the existing documents.
    Each position in the vector is tight to a certain word, so if there’s a 1 (or a value bigger
    than zero, e.g. calculated using TF-IDF scheme) then the related document contains that
    word, otherwise the value will be zero.
    Recall the sample bag of words representations for some documents we used in chapter
    5.
    termsbernhardbio divehypothesisin
    doc11.280.00.00.0doc21.01.00.00.0
    influence
    intolife mathematicalriemann
    0.0 0.00.01.0 0.01.28
    1.0 1.00.00.0 0.00.0
    In the BoVW model each value of the vector is higher than zero if the image has the local
    feature corresponding to that position. So instead of the words bernhard or bio in the text
    case, the BoVW model will have local-feature1 , local-feature2, etc. Each image will be
    represented according the same principle but using clustered local features instead of
    words.
    features
    local-feature1
    local-feature2
    local-feature3
    local-feature4
    local-feature5
    image10.30.00.00.40.0
    image20.50.70.00.81.0
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>225
    Using local feature extractors like SIFT, each image will come with a number of
    descriptors which might vary depending on image quality, image size and other factors.
    Additionally the amount of local features might become too large.
    For the above and other reasons BoVW model involves an additional preprocessing step
    to identify a fixed amount of local features. Let’s assume that for a dataset of images,
    SIFT extract local features for each image, however some has tens, some hundreds of
    features. In order to create a shared vocabulary of local features, all local features are
    used to collected together a clustering algorithm (e.g. k-means) is performed over all such
    features to extract n centroids. The centroids are the words for the BoVW model.
    If you look at a clear sky on a dark night, , you will see a lot of different stars. Each of
    the stars can be considered a cluster point, our local feature. Now imagine that the
    brightest stars in the sky are the ones that have more stars near them (whereas in reality
    the brightness of a star depends on distance, size, age, radioactivity and other factors).
    Under those conditions, the brightest stars will be the cluster centroids; we can use them
    to represent all the points with some approximation. So instead of billions of stars (local
    features), we only consider tens or hundreds of stars (centroids). That’s what clustering
    algorithms do.
    Figure 8.12 Stars and clusters
    It’s now possible to use LIRE to create image feature vectors using a Bag of Visual
    Words model. First of all we extract local features with SIFT and generate our
    vocabulary of visual words using a clustering algorithm like k-means.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>226
    Listing 8.4 Perform SIFT extraction and clustering.
    for (String imgPath : imgPaths) {
    File file = new File(imgPath);
    SiftExtractor siftExtractor = new SiftExtractor();
    BufferedImage bufferedImage = ImageIO.read(file);
    siftExtractor.extract(bufferedImage);
    List<LocalFeature> localFeatures = siftExtractor.getFeatures();
    for (LocalFeature lf : localFeatures) {
    kMeans.addFeature(lf.getFeatureVector());
    }
    }
    for (int k = 0; k < 15; k++) {
    kMeans.clusteringStep();
    }
    Cluster[] clusters = kMeans.getClusters();
    iterate over all the images
    create a local feature extractor based on SIFT algorithm
    read the image contents
    perform SIFT algorithm on the given image
    extract all the SIFT local features
    add all the SIFT features for the current image as points for clustering
    perform k-means clustering for a predefined number of steps
    extract the generated clusters
    The above code computes all the visual words, as a fixed number of clusters. With the
    visual vocabulary in place, the local features of each image are compared with the cluster
    centroids to calculate the final value of each visual word. This task is performed by the
    Bag of Visual Words model which calculates euclidean distance between SIFT features
    and cluster centroids.
    Listing 8.5 Feature vector generation and indexing with BoVW model
    for (String imgPath : imgPaths) {
    File file = new File(imgPath);
    SiftExtractor siftExtractor = new SiftExtractor();
    BufferedImage bufferedImage = ImageIO.read(file);
    siftExtractor.extract(bufferedImage);
    List<LocalFeature> localFeatures = siftExtractor.getFeatures();
    BOVW bovw = new BOVW();
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>227
    bovw.createVectorRepresentation(localFeatures, clusters);
    double[] featureVector = bovw.getVectorRepresentation();
    }
    iterate again over all the images
    extract again SIFT local features, of course SIFT features could be temporarily
    cached per image in a Map to avoid computing SIFT features twice
    create a BoVW instance
    compute a single vector representation for current image, given local SIFT features
    and centroids
    extract feature vectors
    With the code above we have a single feature vector representation for each image that
    we can leverage in image search. For global features extraction we had used a simple
    color histogram extractor, for local features we have used SIFT in conjunction with Bag
    of Visual Words model. They are just some of several different algorithms that can be
    used to perform explicit feature extraction. For example for global feature extraction
    other alternatives exist like fuzzy color scheme, which is a bit more flexible than the one
    we used. For local feature extraction we mentioned SURF, which is a variant of SIFT
    which is more robust and usually better in terms of speed.
    The main advantage of the color histogram feature extractor is its simplicity and
    intuitiveness, the main advantage of SIFT, SURF and other global feature extractors are
    that they perform good for identifying objects in smaller portions of an image in a scale
    and rotation invariant way.
    In practice what makes the difference in a production system is what gives the best
    guarantees in terms of: accuracy, speed, engineering effort and maintenance required to
    make the whole system work.
    Once we have a feature vector of a fixed dimension representing an image, what makes
    the difference in terms of speed is the indexing and search strategies, which we’ll see
    later in this chapter. Regarding engineering effort, maintenance and accuracy the global
    and local feature extractors we have discussed so far have been overtaken by deep
    learning architectures in the feature extraction task; the central point is that features are
    not 'manually' extracted but rather learned through a deep neural network. In the next
    section we’ll see how that makes feature extraction a straightforward end to end learning
    process, from pixels to feature vectors. Such deep learning generated features are also
    typically better in semantic understanding of visual objects.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>228
    8.3 Deep learning for image representation
    Learning representations of data is what made deep learning so successful over the recent
    years. In particular the first field where deep learning outperformed previous state of the
    art approaches is computer vision. In computer vision, computers are tasked to recognize
    objects in images or videos. This can include a variety of applications, from retina scan,
    to street code violations, optical character recognition, etc. This success is driving the
    researchers and engineers to work on increasingly difficult tasks like, for example,
    driverless cars to be solved via deep learning.
    Some famous outstanding results of deep learning applied to images include LeNet 5, a
    neural network that could recognize handwritten and machine printed digits, AlexNet 6, a
    neural network that could recognize objects in an image. The latter is particularly
    interesting for the image search scenario because it was able to categorize (assign a
    category) to a certain image among 1000 different and very fine grained categories. For
    example it could differentiate between very similar dogs of different canine races.
    Footnote 5myann.lecun.com/exdb/lenet/
    Footnote 6mvision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf
    Figure 8.13 Image of dogs classified by AlexNet
    Both LeNet and AlexNet use a special kind of feed forward (artificial) neural network
    called Convolutional Neural Network (also abbreviated as CNN or ConvNet). Over the
    years convolutional neural networks have been applied not just to images or videos but
    also to sound and text, so they are very flexible to be used for different tasks.
    At the beginning of this chapter we mentioned that we could use deep learning to find
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>229
    increasingly abstract structures in images. Researchers have discovered that this is what
    CNNs do during the training phase. As the number of layers grows, layers closer to the
    input learn raw features like edges and corners, while layers placed towards the end of
    the deep neural network learn features that represent shapes and objects. Going forward
    we’ll learn the architecture of convolutional neural networks and how to train and set up
    them.
    8.3.1 Convolutional neural networks
    Despite their names, the connection between artificial neural networks and how the
    human brain works is not so obvious. Most common neural network architectures have a
    fixed architecture, often neurons are fully connected, whereas the neurons in the brain
    hardly have such fixed (and simple) structures. However convolutional neural networks
    were originally inspired by how the visual cortex in the human brain works. Dedicated
    cells take care of certain portions of the image, passing the information to other cells
    which elaborate such information in a flow similar to the one that we’re going to see for a
    convolutional neural network.
    However a fundamental difference in how CNNs work with respect to other types of
    neural networks is that they don’t handle 'flat signal' inputs (e.g. dense, hot encoded
    vectors, etc.). When we created the color histogram of an image we mentioned the fact
    that images are commonly represented via RGB scheme, so a single pixel is described by
    three different values for the red, green and blue channels. If we extend that to an entire
    image, with many different pixels, we will have that the representation for an image of
    width X and height Y will be made by three different matrices for each of the three RGB
    components, each with Y rows and X columns.
    Instead of a single matrix of words or character vectors a neural network needs to handle
    three matrices for each input image, one per color channel. This poses severe
    performance issues when handling such images with 'conventional' feed forward fully
    connected neural networks. Very small images of size 100x100 would need 100 * 100 *
    3 = 30.000 learnable weights for the first layer only. With a relatively medium sized
    image, for today’s standard, like 1024*768 the first layer would need more than 2
    millions parameters (1024 * 768 * 3 = 2.359.296).
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>230
    Figure 8.14 Building blocks (and flow) of convolutional neural networks
    CONVOLUTIONAL LAYERS
    ConvNets solve the problem of handling by training over large inputs by adopting a
    lightweight design in layers and neurons connections. Not all the neurons in this type of
    layers are always connected to neurons in its preceding layer. Such neurons have a
    receptive field of a certain configurable size which defines the local region of the input
    matrices they are connected to. Therefore some neurons are not connected to the whole
    input region and hence do not have an attached weight. Such layers called convolutional
    layers are the main building block of CNNs (together with pooling layers).
    Figure 8.15 Convolutional layer
    Convolutaional layers have a configurable depth (4 in the above image), have a number
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>231
    of filters, plus some other configuration hyperparameters. The filters of a convolutional
    layer contain the parameters (weights) that are learned by the network via
    backpropagation during training. Filters can be imagined as a small window over the
    entire image that changes the input pixels it is currently 'seeing', the filter is slided over
    the entire image so that it is applied to all of the input values. This sliding filtering is the
    convolution operation that gives the name to this type of layer (and to the network of
    course). A filter of 5 x 5 has 25 weights, so it sees 25 pixels at a time. Mathematically
    speaking the filter computes the dot product between the 25 values of the pixels and the
    25 weights of the filter. A convolutional layer will receive an input image like 100 x 100
    x 3, this is also called an input volume since it has 3 dimensions. If such layer has 10
    filters, the output of such a layer will be a volume of 100 x 100 x 10 values. The 10
    generated 100 x 100 matrices (one for each filter) are called activation maps. When
    sliding the filter over the input values it moves by one single value / pixel at a time.
    However sometimes the filter can slide by 2 or 3 values at a time (e.g. on the width axis)
    to reduce the number of generated outputs. This moving size parameter is generally
    called stride. When sliding by one value at a time we have stride = 1, when sliding by
    two values stride is equals to 2, etc. CNNs also reduce the computational burden of
    training with large input volumes by adopting a way to control the number of weights to
    be learned. If you look at the above convolutional layer image, think about all the
    neurons having a certain depth (e.g. depth = 2), they will share the same weights. This
    technique is called parameter sharing.
    In the end the main difference between convolutional layers and 'normal' fully
    connected neural network layers stands in the fact that convolutional neurons are only
    connected to a local region of the input and that some neurons in a convolutaional layer
    share parameters.
    POOLING LAYERS
    Pooling layers responsibility is to downsample the input volume. This means reducing
    the input size while trying to maintain the most important information. This has the
    advantage of reducing the computation complexity and the number of parameters to be
    learned for successive layers (e.g. other convolutional layers). Pooling layers are not
    associated with weights to be learned, they just look at portions of the input volume and
    extract one or more values depending on the chosen function. Common functions are
    max or average. Like convolutional layers, also pooling layers have configurable
    receptive field size and stride. A receptive field size of 2 and stride of 2 will with a max
    function will take as input 4 values from the input volume, and output the maximum
    value of among the 4 input values.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>232
    CNNS TRAINING
    We have learned about the main building blocks of CNNs. Let’s stack them together in
    order to create an actual convolutional neural network and see how such a network is
    trained. Remember that our main goal so far is to extract feature vectors that capture the
    notion of semantically similar images.
    A typical CNN architecture usually involves at least one (or more) convolutional layers,
    followed by
    a dense fully connected layer, this will hold the feature vectors for the images
    an output layers containing class scores for each of the existing classes an image can be
    tagged with
    In fact a convolutional neural network is usually trained in a supervised way using
    training examples whose input is an image and expected outputs are a set of classes the
    image belongs to.
    Let’s look at a known dataset that has been used quite a lot in computer vision research.
    The Cifar dataset 7 contains thousands of images labelled with 10 categories.
    Footnote 7mwww.cs.toronto.edu/~kriz/cifar.html
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>233
    Figure 8.16 Some examples from the Cifar dataset
    Images from the Cifar dataset are color images of 32 x 32 pixel size (very small). The
    first layer will therefore receive inputs of 32 * 32 * 3 = 3072 values.
    Let’s create a simple CNN with two convolution + pooling layers, one dense layer and
    the output layer.
    Figure 8.17 A simple CNN with 2 convolution + pooling layers
    We expect the network to produce an evaluation of the likelihood that the input image
    belongs to any of the 10 categories. The output can be thought as something below. The
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>234
    green bars are the one that correspond to correct labelling from the network, the red ones
    indicate inaccurate predictions 8.
    Footnote 8mthe image below was generated using
    cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html
    Figure 8.18 Testing a CNN on the Cifar dataset
    As we see a CNN training no feature engineering has to be performed. The feature
    vectors can be drained from the final dense layer end to end. You 'just' need lots of
    images with labels. The above architecture is just a simple example of a convolutional
    neural network. A lot of things can be changed in the fundamental design and in the
    many hyperparameters. For example, adding more convolution layers has shown to help
    in improving accuracy. The size of the receptive field as long as the depth of
    convolutional layers or pooling operations (max, average, etc.) are all powerful handles
    that can be tweaked to improve accuracy.
    SETTING UP A CNN IN DL4J
    The above CNN can be implemented in Deeplearning4J quite easily. DL4J comes with
    an utility class to iterate and train over the Cifar dataset so we leverage that in order to
    train our convolutional neural network.
    Listing 8.6 Setting up a CNN for Cifar dataset in DL4J
    int height = 32;
    int width = 32;
    int channels = 3;
    int numSamples = 50000;
    int batchSize = 100;
    int epochs = 10;
    MultiLayerNetwork model = getSimpleCifarCNN();
    CifarDataSetIterator dsi = new CifarDataSetIterator(batchSize, numSamples, new int[]
    {height, width, channels}, false, true);
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>235
    for (int i = 0; i < epochs; ++i) {
    model.fit(dsi);
    }
    cf.saveModel(model, 'simpleCifarModel.json');
    height of input images
    width of input images
    number of image channels to be used
    number of training examples to drain from the Cifar dataset
    size of mini batch
    number of epochs to train the network for
    setup the network architecture
    create an iterator over the Cifar dataset
    train the network
    save the model for later use
    The model architecture is defined by the method getSimpleCifarCNN described below.
    Listing 8.7 CNN configuration
    public MultiLayerNetwork getSimpleCifarCNN() {
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .list()
    .layer(0, new ConvolutionLayer.Builder(new int[]{4, 4}, new int[]{1, 1},
    new int[]{0, 0}).name('cnn1').convolutionMode(ConvolutionMode.Same)
    .nIn(3).nOut(64).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU)
    .layer(1, new SubsamplingLayer.Builder(PoolingType.MAX,
    new int[]{2,2}).name('maxpool1').build())
    .layer(2, new ConvolutionLayer.Builder(new int[]{4,4}, new int[] {1,1},
    new int[] {0,0}).name('cnn2').convolutionMode(ConvolutionMode.Same)
    .nOut(96).weightInit(WeightInit.XAVIER_UNIFORM).activation(Activation.RELU).build())
    .layer(3, new SubsamplingLayer.Builder(PoolingType.MAX,
    new int[]{2,2}).name('maxpool2').build())
    .layer(4, new DenseLayer.Builder().name('ffn1').nOut(1024).build())
    .layer(14, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
    .name('output').nOut(numLabels).activation(Activation.SOFTMAX).build())
    .backprop(true).pretrain(false)
    .setInputType(InputType.convolutional(height, width, channels))
    .build();
    MultiLayerNetwork model = new MultiLayerNetwork(conf);
    model.init();
    return model;
    }
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>236
    Figure 8.19 Resulting model from DL4J UI
    Once the CNN has finished training we are ready to leverage the outputs of the network.
    If we think back to the color histogram or bag of visual words models, back then we
    obtained a feature vector for each image. In the case of a CNN we have more than that. In
    fact we have that the dense layer towards near the output layer contains the feature
    vectors we can use to compare images; however we have also a trained CNN that we can
    use to tag new images. For that to be useful we need to have that the data used for
    training is similar to the data we want to use in our search engine. The labels used for
    training the network will need to fit well with the categories of images we want to index.
    We can now extract feature vectors and classification scores for each image, usually
    iterating over a different set of images.
    Listing 8.8 Extracting feature vectors
    DatasetIterator iterator = ...
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>237
    while (iterator.hasNext()) {
    DataSet batch = iterator.next(batchSize);
    for (int k = 0; k < batchSize; k++) {
    DataSet dataSet = batch.get(k);
    List<INDArray> activations = model.feedForward(dataSet.getFeatureMatrix(), false);
    INDArray imageRepresentation = activations.get(activations.size() - 2);
    INDArray classification = activations.get(activations.size() - 1);
    ...
    }
    }
    obtain the iterator over the images to be processed
    iterate over the dataset
    iterate over each batch
    iterate over each image from the current batch
    perform a feed forward pass, without training, with the current image (pixels as
    input)
    extract the image representation which is stored in the dense layer before the final
    output layer
    extract the classification scores for the current image
    process (e.g. store) the feature vector image representation
    We are now ready to learn about how we can efficiently index and search over the feature
    vectors extracted by the convolutional neural network (although this apply generally also
    for any feature vector, regardless of how it was extracted).
    8.4 Image search
    Recall the example we used at the beginning of the chapter, we want to ingest the a
    picture of our gift taken with our smartphone camera and eventually get a nice picture
    drawn from a professional to be used as a gift card. In order to do that we need to :
    feed the input image to our CNN
    extract the generated feature vectors
    use the feature vectors to make a query to find similar images in the search engine
    We have seen how to perform the first two steps in the previous section, in this section
    we’ll learn how to perform the query efficiently.
    An obvious way to perform the query would be to perform a comparison between the
    input image feature vectors and the feature vectors of all the images stored in the search
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>238
    engine. Imagine to put the feature vectors extracted from a CNN like the one from the
    previous section on a graph, similar images will have their vectors placed very close.
    This is in the end the same line of thought applied to word or document embeddings. So
    one could compute the distance between the feature vector from the input image and each
    of all the other feature vectors and return the images having the top 10 closest feature
    vectors. From the computation perspective this would hardly scale as the time taken to
    perform a query would grow linearly with the number of images in the search engine.
    The process above is an exact nearest neighbour algorithm, in practice this can be also
    approximated.
    Let’s get the CNN feature vectors and index them in Lucene as points. Recent Lucene
    versions have support for n-dimensional points (another way to see a vector) based on
    k-d Tree algorithm 9.
    Footnote 9men.wikipedia.org/wiki/K-d_tree
    We can index images with a feature vector using a FloatPoint.
    Listing 8.9 Indexing a feature vector as a point
    List<INDArray> activations = cnnModel.feedForward(currentImage, false);
    INDArray imageRepresentation = activations.get(activations.size() - 2);
    float[] aFloat = imageRepresentation.data().asFloat();
    doc.add(new FloatPoint('features', floats));
    obtain the feature vector generated by the CNN
    convert it to a float array
    index the feature vector as a Lucene FloatPoint
    At search time we can use a nearest neighbour algorithm.
    Listing 8.10 Nearest neigbour search
    List<INDArray> inputActivations = cnnModel.feedForward(inputImage, false);
    float[] inputFeatures = inputActivations.get(activations.size() - 2).data().asFloat();
    TopFieldDocs docs = FloatPointNearestNeighbor.nearest(searcher,
    'features', 10, inputFeatures);
    ScoreDoc[] scoreDocs = docs.scoreDocs;
    for (ScoreDoc sd : scoreDocs) {
    System.out.println('-->' + reader.document(sd.doc).getField('caption'));
    }
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>239
    pass the input image to the CNN
    extract the input image features
    perform a nearest neighbour search returning the top 10 results
    iterate over the results
    We now have completed the flow from extracting features, to indexing and finally
    searching through images.
    8.5 Summary
    Searching through binary content like images requires learning a representation which
    can capture visual semantics that can be compared across different images
    Traditional techniques for feature extraction have limits and require significant
    engineering efforts
    Convolutional neural networks are at the core of the recent rise of deep learning as they
    can learn image representation abstractions (edges, shapes, objects) incrementally during
    network training
    CNNs can be used to extract feature vectors from images that can be leveraged to
    perform similar images search
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>240
    9
    Peaking into performance
    This chapter covers:
    considerations and advice for setting up deep learning models
    optimizing performance and deployment when setting up search
    systems to work with deep neural networks
    How to get real life neural search systems to work with data streams
    If you have read the previous eight chapters of this book, I hope you have gained a deep
    understanding of deep learning and how it can improve search. At this point, you should
    be set to make the most out of deep learning in setting up successful search engine
    systems for your users. Perhaps along the way, however, you may have wondered about
    actually applying these ideas to real-life production systems. More than once, you may
    have wondered:
    how such tidy approaches would get applied in practice in a production scenario
    whether adding these deep learning algorithms would pose serious impacts on the time
    and space constraints of your systems
    just how big that impact might be, and on which parts or processes (e.g. searching vs
    indexing, or…)
    In this chapter, I want to address these practical concerns and discuss the considerations
    you’ll need to think about as you apply deep learning and neural networks to your search
    engine. We’ll look at the performance bits in the search engine when search engines and
    neural networks work side by side. And I’ll provide you with some options that can be
    applied in practice.
    In the previous chapters we have approached several different search problems deep
    learning can help solve. If you think about the application of word2vec model for
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>241
    synonym expansion or recurrent neural networks to expand queries, you might recall that
    the data flew in and out of neural networks and in and out of search engines. We can
    consider the search engine and the neural network two separate components in a real life
    software architecture. A neural network needs to be trained for predicting accurate
    outputs, at the same time the search engine requires data to be ingested so that users
    search for it. At the same time to leverage deep learning to have more effective search
    results we need the neural network to be effective. These are somewhat conflicting
    requirements:
    Should training happen before indexing ?
    Or should indexing happen beforehand ?
    Can we combine those feeding tasks ?
    How to handle updates to the data ?
    There are a few alternative deployment options. We’ll discuss them in this chapter.
    Also we have briefly touched on time and space constraints with respect to neural
    networks training and their outputs. For example, word and document embeddings, are
    dense vectors: long sequences of real valued numbers. For the search engine to be able to
    quickly leverage them, they would rather be stored inside it. While Lucene and other
    search engines are very good at efficiently storing text:
    Should we adopt dedicated strategies for storing such vectors ?
    How can we do that efficiently ?
    Does using embeddings (e.g. in ranking) pose any serious issues in the time taken to
    return search results ?
    How much additional space can we accept to be taken by indexed embeddings ?
    How much additional time can we accept to be taken when indexing embeddings ?
    In this chapter we are going to answer some of these questions, as we look at the
    considerations we need to take into account when launching real life deployments of
    search engines leveraging neural networks.
    9.1 Performance and the promises of deep learning
    New deep learning architectures keep being published continuously to solve several
    different tasks, we have faced some of them along the course of the chapters, for
    example: generating text (chapter 3 and 4), translating text from one language into
    another (chapter 7), classifying and representing images based on their contents (chapter
    8) and others.
    Not just entire architectures, but also new types of activation functions, cost functions,
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>242
    backpropagation algorithm optimizations, weights initialiation schemes and others.
    The deep learning concepts introduced in this book can apply to recent past, current and
    (hopefully) newer neural networks architectures. As a responsible for search engine
    infrastructure you will probably look for approaches that researchers demonstrated to
    work best for a specific task (also known as state of the art). Think for example to the
    machine translation or image search cases. For example, as of today, state of the art for
    machine translation is represented by sequence to sequence models, like encoder-decoder
    networks with attention 1. So you would want to implement those state of the art models
    and expect them to give you the nice results that you can read on the related research
    papers. In those cases the first challenge is to reproduce the model described in the paper
    and then to make it work effectively on your data and infrastructure. To do so, :
    Footnote 1msee recent research that even discards RNNs
    papers.nips.cc/paper/7181-attention-is-all-you-need.pdf
    the neural network will need to provide accurate results
    the neural network will need to provide results quickly
    software and hardware need to be adequate to the computational load time and space
    wise (and training, remember, is costly)
    In the next section we will run through the entire process of taking a neural network
    model to solve a specific task and see what are common steps you may have to take
    along the way to solve the above problems.
    9.1.1 From model design to production
    In the past chapter we have seen convolutional neural networks in action to learn to
    classify images. Once training had finished we used the network to extract feature vectors
    to be indexed and searched by the search engine. We didn’t look too much into the
    accuracy of the neural network classifications.
    We’ll now track some numbers in terms of accuracy and training / prediction time in the
    road to build a good neural network model to use in conjunction with our search engine.
    Let’s get back to the Cifar dataset we used in chapter 8 to learn to classify images and see
    how to gradually adjust the neural network model in our way to improve accuracy while
    keeping reasonable training timings. We’ll do that step by step as you would do it in your
    own project. In real life you will run a few experiments and evaluations before actually
    indexing feature vectors. In fact indexing is usually costly as well with real life data.
    Cifar is just about a few tens of thousands of images, but many live deployments will
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>243
    have to index hundreds of thousands, millions or billions of images / documents. If you
    index 100 milion images with their feature vectors, you don’t want to do that again as
    you realise that the user experience is not so good because the feature vectors didn’t
    accurately reflect image contents. So you’ll usually perform some evaluation and then
    index images with feature vectors.
    Let’s start with a CNN similar to one of the first CNN based architectures that reached
    good results for categorizing images: the LeNet architecture 2. This is a simple CNN
    similar to the one we set up in chapter 8, but with slightly different sets for convolution
    depth, receptive field size, stride and dense layer dimensionality.
    Footnote 2myann.lecun.com/exdb/lenet/
    Figure 9.1 LeNet model
    The model contains two sequences of convolutional layer followed by a max pooling
    layer and a fully connected layer. The filters are 5 x 5 sized, the first convolutional layer
    depth is 28, the second convolutional layer is 10. The dense layer is 500 sized. Max
    pooling layers have stride equals to 2.
    Listing 9.1 LeNet kind of model
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .list()
    .layer(0, new ConvolutionLayer.Builder(new int[]{5, 5}, new int[]{1, 1},
    new int[]{0, 0}).convolutionMode(ConvolutionMode.Same)
    .nIn(3).nOut(28).activation(Activation.RELU).build())
    .layer(1, new SubsamplingLayer.Builder(PoolingType.MAX, new int[]{2,2}).build())
    .layer(2, new ConvolutionLayer.Builder(new int[]{5,5}, new int[] {1,1},
    new int[] {0,0}).convolutionMode(ConvolutionMode.Same)
    .nOut(10).activation(Activation.RELU).build())
    .layer(3, new SubsamplingLayer.Builder(PoolingType.MAX, new int[]{2,2}).build())
    .layer(4, new DenseLayer.Builder().nOut(500).build())
    .layer(5, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>244
    .nOut(numLabels).activation(Activation.SOFTMAX).build())
    .backprop(true)
    .pretrain(false)
    .setInputType(InputType.convolutional(height, width, channels))
    .build();
    This model is quite old so we do not expect it to perform too well. However it’s a good
    practice to start with smaller models first and see how far it gets.
    We train over just 2000 examples from the Cifar dataset at first, to have a quick
    turnaround about the model parameters. In fact if the model starts diverging earlier we
    can avoid load huge training sets before discovering it.
    Listing 9.2 Training over 2000 samples from the Cifar dataset
    int height = 32;
    int width = 32;
    int channels = 3;
    int numSamples = 2000;
    int batchSize = 100;
    boolean preProcessCifar = false;
    CifarDataSetIterator dsi = new CifarDataSetIterator(batchSize, numSamples,
    new int[] {height, width, channels}, preProcessCifar, true);
    MultiLayerNetwork model = new MultiLayerNetwork(conf);
    model.init();
    for (int i = 0; i < epochs; ++i) {
    model.fit(dsi);
    }
    we only use 2000 random samples from the Cifar dataset
    To monitor how well the neural network learns to categorize the images we monitor the
    training process with the DL4J UI.
    Figure 9.2 LeNet training
    In the best possible case we would see the score steadily going down towards 0, however
    in this case it goes down very slowly without ever reaching a point close to 0. You should
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>245
    recall that the score is a measure of the amount of error the neural network commits when
    trying to predict the classes for each input image. So, with these stats, we do not expect it
    to perform very well. In order to perform the evaluation of the accuracy of predictions for
    a machine learning model it is always a good practice to separate the training set from the
    test set. During training the model could overfit the data and therefore give good
    accuracy on the training set while unable to generalize well over slightly different data.
    A separate iterator over a different set of images can be created and passed to DL4J
    evaluation tools.
    Listing 9.3 Model evaluation with DL4J
    CifarDataSetIterator cifarEvaluationData = new CifarDataSetIterator(batchSize,
    1000, new int[] {height, width, channels}, preProcessCifar, false);
    Evaluation eval = new Evaluation(cifarEval.getLabels());
    while(cifarEvaluationData.hasNext()) {
    DataSet testDS = cifarEvaluationData.next(batchSize);
    INDArray output = model.output(testDS.getFeatureMatrix());
    eval.eval(testDS.getLabels(), output);
    }
    System.out.println(eval.stats());
    create a test set iterator
    instantiate the evaluation tool
    iterate over the test dataset
    fetch the next mini batch of data (100 examples in this case)
    perform prediction over the current batch
    perform evaluation using the actual output and the Cifar output labels
    print the statistics on the standard output
    The evaluation stats include metrics like accuracy, precision, recall and F1-score as well
    as the confusion matrix. The F1-score is a measure whose value ranges between 0 and 1
    which takes into account precision and recall.
    Listing 9.4 Test output
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.2310
    Precision:
    0.2207
    Recall:
    0.2255
    F1 Score:
    0.2133
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>246
    Precision, recall & F1: macro-averaged (equally weighted avg. of 10 classes)
    =========================Confusion Matrix=========================
    0 1 2 3 4 5 6 7 8 9
    -------------------------------
    31 9 4 10 2 3 6 3 26 9 | 0 = airplane
    6 19 0 7 6 6 4 0 16 25 | 1 = automobile
    18 8 6 14 8 6 15 4 12 9 | 2 = bird
    11 14 1 28 14 5 8 6 3 13 | 3 = cat
    8 5 3 14 15 5 15 7 5 13 | 4 = deer
    9 5 5 21 18 8 8 1 3 8 | 5 = dog
    8 9 7 12 21 4 29 7 5 10 | 6 = frog
    11 11 8 13 8 4 6 10 11 20 | 7 = horse
    18 6 1 9 4 1 2 2 47 16 | 8 = ship
    12 12 2 8 6 3 2 3 23 38 | 9 = truck
    In the confusion matrix above we can see that for the class airplane, in the first row, 31
    samples have been correctly assigned to the airplane class, however about the same
    number of predictions (26) assigned a wrong class of a ship to an airplane image. Ideally
    a confusion matrix would contain high values on the right diagonal and low values
    everywhere else.
    Changing the numSamples value to 5000 and performing training and evaluation again
    we expect better results.
    Listing 9.5 Test output with more samples
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.3100
    Precision:
    0.3017
    Recall:
    0.3061
    F1 Score:
    0.3010
    Precision, recall & F1: macro-averaged (equally weighted avg. of 10 classes)
    =========================Confusion Matrix=========================
    0 1 2 3 4 5 6 7 8 9
    -------------------------------
    38 2 6 3 5 1 1 9 25 13 | 0 = airplane
    4 34 3 2 4 4 6 4 14 14 | 1 = automobile
    15 4 12 7 15 9 16 8 10 4 | 2 = bird
    7 4 4 26 16 11 15 13 1 6 | 3 = cat
    4 2 10 9 24 7 13 5 8 8 | 4 = deer
    7 5 5 19 9 14 11 7 3 6 | 5 = dog
    3 8 10 9 22 5 40 12 1 2 | 6 = frog
    4 8 6 13 12 2 9 29 2 17 | 7 = horse
    17 5 2 8 4 2 0 4 51 13 | 8 = ship
    7 13 3 4 4 3 2 10 21 42 | 9 = truck
    The F1 score went up by 9% (0.30 vs 0.21), which is a big step forward, however getting
    good results only about 30% of the times would not be appropriate for usage in
    production.
    For a better understanding of how good this model could work, we would need to train it
    with much more data. We have more than 50.000 images. We should split such a dataset
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>247
    in a way that most of it is used for training, but there are still many images available to
    perform evaluation. So let’s do a full training with 50000 images and evaluation with
    10000 images.
    Apart from the fact that machine learning models in general (but deep learning ones in
    particular) usually need lots of training examples to produce good results, there are other
    reasons not to base model architecture on small training iterations. You might recall that
    neural network train by backpropagation algorithm (possibly with some variations
    depending on the specific architecture, like backpropagation through time for recurrent
    neural networks). The backpropagation algorithm aims at reducing the error committing
    by the network in predictions by adjusting the weights so that the overall error rate gets
    lower and lower. At some point it will find a set of weights (e.g. the weights attached to
    connections between neurons in different layers) with the lowest possible error. However
    this might take a lot depending on features of the data used for training like:
    the diversity in training examples : some text is written in very formal language, some
    text is written in slang; or some images relate to pictures taken with the daylight and
    some others were taken at night
    the 'noise' in the training examples : some text has typos or grammatical errors; or some
    images are of poor quality or contain watermarks or other types of noise which make
    training harder
    The capability of a neural network training to converge to a good set of weights also
    highly depends on their tuning parameters, like the learning rate : we already mentioned
    it, but it’s worth repeating that it’s a fundamental handle to get right. A too high learning
    rate would make training fail, a too low one would make training take too much to
    converge to a good set of weights (or eventually never get there).
    Let’s have a look at the loss of the same neural network but with different learning rates.
    Figure 9.3 Loss plotted for the same neural networks but trained with different learning
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>248
    rates
    You can clearly see that both converge over time to the very same set of weights. The
    blue line relates to a high learning rate while the red one refers to a low learning rate.
    Consider what would happen if we stopped training within the green or grey time boxes.
    If we were to choose after a small number of iterations (green box) we would surely
    exclude the high learning rate because that would seem to make the loss grow instead of
    get lower. If we were to choose at time corresponding to the right edge of the grey box,
    we would instead discard the learning rate related to the red line because that would seem
    to keep the same score or even grow at some point.
    It’s therefore a good idea to come with a few possible architectures with reasonable
    parameter settings, do some quick experiments but then also run full training and
    evaluation on an extensive test set. An important thing to note down at this point, before
    using the full dataset, is the time taken by training with respect to the available hardware
    and requirements of a production scenario.
    The first iteration of training for 10 epochs with 2000 images took 3 minutes on a
    'normal' laptop, 5000 images 10 epochs training took 7 minutes. These are acceptable
    timings for short turnaround experiments. Training over the full dataset for several
    epochs might take hours, or anyway an amount of time that would rather be better used if
    we knew in advance what to change. Using more weights to be learned is likely to
    require training to take more time and resources. For example a common mistake is to
    add layers or increase the size of layers as much as possible. Adding more layers can help
    when the network doesn’t have enough training power to fit over lots of different training
    examples, you have that when the number of weights is way lower than the number of
    examples and the neural networks is having a hard time to find at converging to a good
    set of weights (e.g. the score doesn’t go below a certain value).
    In the previous code we have trained a relatively lightweight CNN with 5000 examples,
    let’s see what happens if we change the convolutional layers to just be deeper (96 and
    256 depth each). The training time for 5000 examples would increase from 10 minutes to
    1 hour, with the following evaluation stats:
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.3011
    Precision:
    0.3211
    Recall:
    0.2843
    F1 Score:
    0.3016
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>249
    In this case it was not worth adding more power to the network, or not so much. Working
    with deep neural networks in production surely requires some experience but it is not
    magic. The number of weights to be learned is an important thing to be taken care of. The
    number of data points from the training set should always be less than the number of
    weights. Possible consequences of not following this rule is overfitting or difficulties in
    converging.
    Let’s instead do some reasoning on the data, we have tiny images of 32 x 32 pixels.
    CNNs learn feature over time with convolutional layers while downsampling with
    pooling layers. Maybe it could help to just give a few more weights to the initial
    convolution layer but let the pooling layer have a stride of 2 instead of 1. We expect
    slightly better results but less time taken to train the network.
    Listing 9.6 Test results
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.3170
    Precision:
    0.3069
    Recall:
    0.3297
    F1 Score:
    0.3316
    Training finishes in 5 minutes instead of 7, thanks to the change to the pooling layer. This
    might seem small but would surely make a difference when training over the entire
    dataset, and we got a small improvements in the quality of results.
    Let’s run the current settings over 50.000 images for training and 10.000 for evaluation.
    We expect better evaluation metrics results and lower score at the end of training.
    Listing 9.7 Evaluation over 10.000 images with the CNN trained over 50.000
    examples
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.4263
    Precision:
    0.4213
    Recall:
    0.4263
    F1 Score:
    0.4131
    Precision, recall & F1: macro-averaged (equally weighted avg. of 10 classes)
    =========================Confusion Matrix=========================
    0
    1
    2
    3
    4
    5
    6
    7
    8
    9
    ------------------------------------------
    459 60 39 40 14 24 41 49 191 83 | 0 = airplane
    29 592
    3 30
    3 12 47 34 50 200 | 1 = automobile
    92 50 123 81 165 89 229 97 46 28 | 2 = bird
    19 34 40 247 48 200 216 103 19 74 | 3 = cat
    44 21 58 83 284 60 263 128 33 26 | 4 = deer
    11 22 69 189 63 337 158 100 29 22 | 5 = dog
    3 26 20 90 66 32 661 52
    9 41 | 6 = frog
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>250
    24
    122
    38
    92
    27
    12
    86
    25
    72
    6
    69 107 494 18 65
    21 23 26 546 127
    | 7 = horse
    | 8 = ship
    Figure 9.4 CNN full training loss graph
    After training on almost the whole training set we got a 0.41 f1-score after almost 3 hours
    (always on a normal laptop). We can’t be satisfied by the accuracy of the model, it would
    still make errors 59% of the times.
    In this cases it’s useful to look at the loss curve. The curve is going down and maybe it
    would keep going down if we just had more data. Unfortunately for this case we don’t,
    unless we accept to use a smaller test set.
    One thing we can look into when we have such curves is whether we have a wrong size
    for the batch parameter. A batch or mini-batch is a collection of training examples that
    are put together and fed to the neural network as a single 'batched' input. For example
    instead of feeding one image at a time, so an input volume (a set of stacked matrices) at a
    time, we squash a number of input volumes together. This has usually two consequences
    training is faster
    the training is less prone to overfitting
    If the mini batch parameter would be set to 1 we would see a curve that, especially in the
    first iterations, would float up and down quite heavily. On the other hand if we have a
    mini batch that is too big, the network might not be able to 'learn' about specific patterns
    and features that rarely occur in inputs.
    It is possible that the reason for a quite flat loss curve is related to a batch size which is
    too high for that specific data (100). Let’s then get it down to 48. In order to see whether
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>251
    it can make a difference it is useful to do quick tests on small portions of the dataset,
    which will have to be proved later by a full dataset training if we get encouraging results.
    So we set the batch parameter to 48, train on 5000 examples and perform evaluation on
    1000 images. We expect a less smooth curve which brings to a lower loss and hope for
    better accuracy.
    Figure 9.5 Training with 48 sized batches
    Reducing the batch size helped indeed as we reached a loss close to the minimum much
    faster than we did when we had the batch size set to 100. Trainining however took more
    time, it took 9 minutes instead of the previous timing of 7 minutes. A difference of 2
    minutes can be noticed on a larger scale, however it might be acceptable if the training
    time pays with a significantly better f1-score.
    Listing 9.8 Accuracy with batch set to 48
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.3274
    Precision:
    0.3403
    Recall:
    0.3324
    F1 Score:
    0.3218
    We got a better f1-score when training with 5000 examples than we got with 100 sized
    batch. We got a f1-score of 0.30 then while we get a f1-score of 0.32. Reducing the batch
    size seems to be a good idea that we have to prove with a full training. We won’t
    compare the f1-score of a small training set like this with the f1-score reached when
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>252
    training over 50.000 images as that wouldn’t be fair and could mislead (and frustrate) our
    efforts. But, can we do better with an even smaller batch size ? Let’s set the batch size to
    24 and see what we get.
    Figure 9.6 Training with 24 sized batches
    The curve is much more sharpened, we got a loss close to the one with batch set to 48.
    We get a higher f1-score of 0.33, but raining took 13 minutes instead of 9.
    Listing 9.9 Accuracy with batch set to 24
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.3601
    Precision:
    0.3512
    Recall:
    0.3551
    F1 Score:
    0.3340
    At this point we have to take a decision: can we afford a more costly training in terms of
    time and computational resources (which could mean more money, e.g. if you are
    running training in production over cloud services) to get better numbers. A good
    practice is to save the different models we generate together with their evaluation metrics
    and training times so that we can pick them up in a later stage when we need to make
    decisions.
    With a smaller batch size, the neural network should be able to better handle more
    diverse inputs, however the curve is very sharp. If we take the latest model and train it
    with 50.000 examples we got the following evaluation results after 5 hours of training on
    a laptop:
    Listing 9.10 Evaluation with batch set to 24 on 50.000 examples
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>253
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.5301
    Precision:
    0.5213
    Recall:
    0.5095
    F1 Score:
    0.5153
    We improved the former 0.41 f1-score of 10% to reach a not so bad 0.51. However this is
    not yet something we would ship to our end users. With such number if the user would
    look for images similar to a deer could get only 5 deers, the remaining images would
    picture cat, dogs or even trucks and ships!
    We tried using more deep convolutional layers, but it didn’t help. We have seen that the
    accuracy grows with the amount of data used. Batch size has proved to be an important
    parameter to get right even during prototyping phase to reach better results, however
    changes in batch size have impacts on the training time.
    However there is still a number of things to be looked into:
    train for more epochs
    check weights and bias initialization
    look into regularization options
    we can change the way the neural network updates its weights during backpropagation,
    the so called updater algorithm
    we have mentioned adding layers could be an option, we need to figure out if that could
    help in this case
    Let’s have a look at all these options. An epoch of training is a full round of training on
    all the available training examples from the training set. We have used 10 epochs so that
    the neural network would see the same input batches 10 times. The rationale behind that
    is that the network should be able to get the right weights for those inputs with a higher
    probability if it 'sees' them multiple times. Low numbers like 5, 10 or 30 are common
    during the development phase where the network is designed. However we may change
    that for training our final model. If we increase the number of epochs but don’t see any
    significant improvement the network probably can’t do more with the current setup for
    that data therefore we need to change something else. Changing the number of epochs
    from 10 to 20 in our case gives the current results:
    Listing 9.11 Evaluation when training over 20 epochs
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.3700
    Precision:
    0.3710
    Recall:
    0.3646
    F1 Score:
    0.3565
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>254
    Training took 28 minutes.
    Figure 9.7 Loss curve for 20 epochs
    Think about the neural network before it receives any input. All the neurons have
    activation functions and connections. As the network will start receiving inputs and
    backpropagate output error, it will start changing the weights attached to their
    connections. A surprisingly effective change you can make to your neural network is the
    way those weights are initialized. This might sound surprising but a lot of research has
    shown that weight initialization has a big impact on the effectiveness of training 3.
    Footnote 3msee proceedings.mlr.press/v9/glorot10a/glorot10a.pdf and arxiv.org/abs/1502.01852
    Simple things we could do to initialize weights are to set them all to zero or set them to
    random numbers. A few chapters back we have shown how the learning algorithm
    (backpropagation) makes the network weights change and this can be seen visually as
    moving a point on an error surface. A point on such a surface represents a set of weights,
    the minimum height point represents the point where the weights make the network
    commit as low error as possible.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>255
    Figure 9.8 Error surface with some points of interest
    The black point in the image above represents a set of weights with high error, the white
    point represent a set of weights with an average error, the green point represents the point
    with lowest possible error. The backpropagation algorithm will hopefully make the
    network weights move from a start position somewhere to the green point. Now think
    about the weight initialization, it will be responsible to set the starting point for the
    algorithm when looking for the best set of weights (the green point). With a weight
    initialization to 0 the network weights might be at the white point, not so bad and not so
    good. With a random initialization you might get luck and place them near the green
    point (but it is very unlikely) or set the weights somewhere far from the green point, e.g.
    on the black point. This starting position influence the capability of backpropagation to
    ever reach the green point (it may take too much) or at least to make it longer and harder.
    So a good initialization for the neural network weights is crucial for its successful
    training.
    A good commonly used weight initialization is called Xavier initialization. Basically it
    initializes the weights of the neural network by drawing them from a distribution with
    zero mean and a specific variance for each neuron. That initial weight depends on the
    number of neurons who have outgoing connections to that specific neuron. We can set
    that in DL4J in a specific layer with the following code:
    Listing 9.12 Setting the weight initialization
    .layer(2, new ConvolutionLayer.Builder(new int[]{5,5}, new int[] {1,1}, new int[] {0,0})
    .convolutionMode(ConvolutionMode.Same)
    .nOut(10)
    .weightInit(WeightInit.XAVIER_UNIFORM)
    .activation(Activation.RELU)
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>256
    initialize the weights of the given layer using Xavier distribution
    When we lowered the number inputs to put in a single batch we have noticed the loss
    curve becoming less and less smoother. This is because with less batches the learning
    algorithm is more prone to overfitting.
    Figure 9.9 Sharpening loss curve with smaller batch sizes
    It's often useful to introduce regularization methods in your neural network training
    algorithm. This would help because of the small batch size for sure, but that's a good
    practice in general. The amount of regularization to use depends on the use case.
    Listing 9.13 Regularization
    MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
    .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer)
    .l1(1.0e-4d).l2(5.0e-4d)
    With regularization and weight initializations in place let’s perform another round of
    training for 10 epochs on 5000 images.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>257
    Figure 9.10 Tuning best
    Training takes 16 minutes, however we can already see that the loss is decreasing much
    faster and to a lower value than previous settings. We therefore expect a higher f1-score.
    Listing 9.14 Final results
    ========================Evaluation Metrics========================
    # of classes:
    10
    Accuracy:
    0.4454
    Precision:
    0.4602
    Recall:
    0.4417
    F1 Score:
    0.4438
    As expected we got a quite high f1-score with relatively few training examples. Having
    noticed improvements with a higher number of epochs, we can try to increase it to 20 as
    we did before.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>258
    Figure 9.11 Tuning best for 20 epochs
    While training time increases to 19 minutes, the curve looks more or less similar and,
    surprisingly, the f1-score remains unchanged.
    Listing 9.15 Final results for 20 epochs
    ========================Evaluation Metrics========================
    # of classes:
    11
    Accuracy:
    0.4435
    Precision:
    0.4624
    Recall:
    0.4395
    F1 Score:
    0.4411
    There are a few possible reasons for that: the first one is that we might need more data.
    Let’s evaluate the accuracy of the last settings using the whole 50.000 images dataset.
    Listing 9.16 Whole dataset training evaluation
    ========================Evaluation Metrics========================
    # of classes:
    11
    Accuracy:
    0.5998
    Precision:
    0.6213
    Recall:
    0.5998
    F1 Score:
    0.5933
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>259
    Figure 9.12 Whole dataset training loss curve
    Reaching good numbers is not always easy and might require several iterations of the
    process just described. Looking into recent research is always a good idea to find out
    whether better solutions exist for the various aspects of neural networks. To some extent
    tuning neural network can look like a art; the fact is that experience helps but getting to
    know the maths and dynamics of learning is the key point to come with effective models
    and settings.
    9.2 Indexes and neurons working together
    In the previous section we have gone through an end to end process to set up and tweak a
    deep neural network to achieve the best results in terms of accuracy. We have also briefly
    noted the time taken each time to train the whole network. With that set however only
    half of the problem is solved. In fact our goal is to leverage such deep learning models in
    the context of search to provide more meaningul search results to end users. The question
    is now how to use and update those deep learning models together with search engines.
    Let’s assume for a moment that we have a pretrained model that perfectly fits the data to
    be indexed. In the given example we index text documents and want to use a pretrained
    sequence to sequence model to extract thought vectors to be used in the ranking function
    by the search engine. The straightforward solution is to establish a document indexing
    pipeline where the document text is first sent to the seq2seq model, the feature vectors
    are extracted and indexed together with the document text into the search engine.
    You can see that the actions and responsibilities of the neural network and the search
    engine are heavily interleaved.
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>260
    Figure 9.13 Neural network and search engine interactions, indexing time
    At search time the seq2seq model is used again to extract thought embeddings from the
    query. The ranking functions then performs scoring leveraging the query and document
    embeddings (previously stored in the index).
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>261
    Figure 9.14 Neural network and search engine interactions, search time
    If you look at the graphs above everything might sound reasonable. However the neural
    network might introduce some overhead both for indexing and search:
    neural network prediction time : how much time does the neural network take to extract
    thought vectors for documents at indexing time ? how much time does the neural network
    take to extract thought vectors for queries at search time ?
    search engine index size : how much space do generated embeddings take in addition to
    storage space taken by text documents ?
    In general the most critical part for performance is the query / search phase. You can’t
    expect users to accept waiting times in the order of seconds just because your ranking
    function return better results. In most cases users won’t ever know what’s behind the
    search box. They expect it to be fast, reliable and give good results. In the previous
    section we have addressed the accuracy of results while noting down the training times.
    Now care needs to be taken to track the time taken by the network to compute a full feed
    forward pass from the input to the layer we pick the network output from.
    In the case of an encoder - decoder network, the feed forward pass of the encoder side of
    the network would only be needed to extract thought vectors. The decoder side of the
    network will only be used only in case we want to also use the input text to perform
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>262
    training using a target output (if we have it).
    The overhead in indexing also has to be taken under control. In a static scenario where
    we ingest a set of documents, even if it’s huge, that might not be that important because
    we could accept an aggregate overhead of 1 or 2 hours if it’s just once. Reindexing or
    high volume concurrent indexing might instead be problematic. Reindexing means taking
    the whole corpus of documents in the search engine and re index them all from scratch.
    This is usually done because of some change in the configuration of text analysis
    pipelines or some added document processor to extract more metadata. For example let’s
    take a simple search engine based on plain Lucene with no query expansion capability. If
    we want to use word2vec model to expand synonyms at indexing time, we would need to
    take all the existing documents and re index them so that the resulting inverted index will
    also contain the words / synonyms extracted by word2vec. The bigger the index, the
    higher the impact of reindexing will be.
    Concurrency is another aspect, can neural network deal with concurrent inputs ? This is
    really an implementation detail and might depend on the specific technology used to
    implement your model, but it has to be taken into account both at indexing (multiple
    parallel indexing processes) and search time (multiple users searching at the same time).
    Embeddings, and dense vectors in general, can have many dimensions. Efficient storage
    of them is still an open problem. In real life the choices might be limited by the
    capabilities of the search engine technology used. In Lucene for example dense vectors
    can be indexed as :
    binaries : every vector is stored as it was an unqualified binary, all the embedding
    processing is done upon fetching of the binary
    n dimensional points: every vector is stored as a point with many dimensions (one for
    each vector dimension), basic geometric and nearest neighbour queries can be performed
    text : it might sound weird at first but, with appropriate design, vectors can be indexed
    and searched over as text units 4.
    Footnote 4msee arxiv.org/pdf/1706.00957.pdf
    Other libraries like Vespa 5 or search platforms like Apache Solr 6 or Elasticsearch 7 can
    offer more / different options.
    Footnote 5mvespa.ai
    Footnote 6mlucene.apache.org/solr
    Footnote 7mwww.elastic.co/products/elasticsearch
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>263
    9.3 Working with streams of data
    We have used static datasets across all the different examples in the book. Static datasets
    are great for illustrative purposes, as they make it easier to focus on a peculiar set of data.
    Also, when building a search engine, it is very common to start with a first set of
    documents (text and/or images) that you want to index. So, all good for starting, but as a
    search engine goes into production and starts being used, there will most likely arise new
    documents that it need to be ingested.
    Let’s consider an application that allows users to search for popular posts from social
    networks for various different topics. You might start with a first set of posts downloaded
    or bought somewhere, but as the focus is on popular posts, you need to keep ingesting
    data as the trends change over time. A similar such application would work on news
    rather than on social network posts. You can download a news corpus like the NYT
    Annotated Corpus 8 but still every day there are many different articles your application
    will need to ingest so that users can search for them.
    Footnote 8mcatalog.ldc.upenn.edu/LDC2008T19
    These days, a common architecture to address incoming flows of data is called streaming
    architecture. In a streaming architecture, data flows in continuously from one or more
    sources and gets transformed by different functions stacked in a pipeline. Data can get
    transformed, aggregated, dropped at any time and finally reach a sink . A sink can be
    thought as the final stage of the pipeline, a persistence system like a database or a search
    engine index.
    In our example a streaming architecture could continuously ingest posts from social
    networks and index them into the search engine. An application working with the
    indexed data would read the index and expose search features to end users. However we
    want to do more: in this section we are interested in all aspects of performance of search.
    In particular we’ll leverage a streaming architecture to stream posts, extract document
    embeddings using different models, index them in Lucene and evaluate different ranking
    models leveraging such embeddings. This would help us choose the best model for our
    production application.
    In order to set up the streaming architecture we use Apache Flink 9, a framework and
    distributed processing engine for computations over data streams. Our Flink streaming
    pipeline will do the following :
    Footnote 9mflink.apache.org
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>264
    stream posts from the Twitter social network 10 that contain certain keywords
    Footnote 10mtwitter.com
    extract each tweet text, language, user, etc.
    extract document embeddings using paragraph vectors and word2vec averaged vectors
    index each tweet with its text, language, user and different document embeddings
    run predefined queries on all of the indexed data using different ranking models (classic
    and neural)
    write outputs on a CSV file that can be analyzed at a later stage for assessing the quality
    of search results
    The output file will tell us how the different ranking models reacted to changing data
    with respect to fixed queries. For example, this will give us some valuable information
    with respect to how well the different ranking models adapt to the new posts. If a ranking
    model will keep giving the same results despite the changing data, that will probably
    won’t be the best option for an application that aims at capturing 'trending' data.
    First thing we define a stream over data coming from Twitter.
    Listing 9.17 Defining a stream of data over Twitter with Apache Flink
    final StreamExecutionEnvironment env =
    StreamExecutionEnvironment.getExecutionEnvironment();
    Properties props = new Properties();
    props.load(StreamingTweetIngestAndLearnApp.class.
    getResourceAsStream('/twitter.properties'));
    TwitterSource twitterSource = new TwitterSource(props);
    String[] tags = {'neural search', 'natural language processing', 'lucene',
    'deep learning', 'word embeddings', 'manning'};
    twitterSource.setCustomEndpointInitializer(new FilterEndpoint(tags));
    DataStream<Tweet> twitterStream = env.addSource(twitterSource)
    .flatMap(new TweetJsonConverter());
    define an execution environment
    load security credentials for accessing Twitter
    create a new Flink source for Twitter data
    define the keywords to be used to fetch posts from Twitter (only tweets containing
    those keywords will be ingested)
    add the keyword filter to the twitter source
    create a stream over twitter data
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>265
    start by converting raw text into a json format for Tweets
    The whole piece of code performs the required configuration to start ingesting tweets that
    contain the keywords neural search, natural language processing, lucene, deep learning,
    word embeddings and manning.
    At this point we will define a series of functions to work on the tweets. At this point we
    also focus on implementation details regarding performance. Does it make sense to run
    our predefined queries every time a new tweet comes in ? Perhaps it’s better to do it
    when we have more data, e.g. 20 tweets, that could influence scoring. For this reason we
    define a so called count window function. This function will only pass the data to the next
    function when he has received 20 tweets.
    Listing 9.18 Streaming data manipulation pipeline
    Path outputPath = new Path('/path/to/data.csv');
    OutputFormat<Tuple2<String, String>> format = new CsvOutputFormat<>(outputPath);
    DataStreamSink<Tuple2<String, String>> tweetSearchStream =
    twitterStream
    .countWindowAll(batchSize)
    .apply(new ModelAndIndexUpdateFunction())
    .map(new MultiRetrieverFunction())
    .map(new ResultTransformer()).countWindowAll(1)
    .apply(new TupleEvictorFunction())
    .writeUsingOutputFormat(format);
    env.execute();
    the output CSV file
    define a count window over the streaming data
    update models, extract features and update index
    run predefined queries
    transform the output in a way that is suitable for composing a CSV
    Let’s look into the ModelAndIndexUpdateFunction.
    Listing 9.19 ModelAndIndexUpdateFunction
    public class ModelAndIndexUpdateFunction implements AllWindowFunction<Tweet,
    Long, GlobalWindow> {
    @Override
    public void apply(GlobalWindow globalWindow, Iterable<Tweet> iterable, Collector<Long>
    collector) throws Exception {
    ParagraphVectors paragraphVectors = Utils.fetchVectors();
    CustomWriter writer = new CustomWriter();
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>266
    for (Tweet tweet : iterable) {
    Document document = new Document();
    document.add(new StringField('id', tweet.getId(), Field.Store.YES));
    document.add(new StringField('lang', tweet.getLanguage(), Field.Store.YES));
    document.add(new StringField('user', tweet.getUser(), Field.Store.YES));
    document.add(new TextField('text', tweet.getText(), Field.Store.YES));
    if (tweet.getText() != null && tweet.getText().trim().length() > 0) {
    try {
    INDArray paragraphVector = paragraphVectors.inferVector(tweet.getText());
    document.add(new BinaryDocValuesField('pv',
    new BytesRef(paragraphVector.data().asBytes())));
    INDArray averageWordVectors =
    averageWordVectors(paragraphVectors.getTokenizerFactory().
    create(tweet.getText()).getTokens(), paragraphVectors.lookupTable());
    document.add(new BinaryDocValuesField('wv',
    new BytesRef(averageWordVectors.data().asBytes())));
    } catch (Exception e) {
    ...
    }
    }
    writer.addDocument(document);
    }
    long commit = writer.commit();
    writer.close();
    collector.collect(commit);
    }
    }
    The MultiRetrieverFunction will contain just some very basic Lucene search code to run
    the query neural networks deep learning search over the entire index with different
    ranking functions.
    The output aggregated in the CSV file will contain a line for each ranking model many
    times. Let’s have a look at two consecutive iterations.
    ...
    classic,Amazing what neural networks can do.
    // Computational Protein Design with Deep Learning Neural Networks
    | Scientifi… https://t.co/K6fL4YE8eX
    language model dirichlet,Amazing what neural networks can do.
    // Computational Protein Design with Deep Learning Neural Networks
    | Scientifi… https://t.co/K6fL4YE8eX
    bm25,Amazing what neural networks can do.
    // Computational Protein Design with Deep Learning Neural Networks
    | Scientifi… https://t.co/K6fL4YE8eX
    average wv ranking,Amazing what neural networks can do.
    // Computational Protein Design with Deep Learning Neural Networks
    | Scientifi… https://t.co/K6fL4YE8eX
    document embedding ranking,Amazing what neural networks can do.
    // Computational Protein Design with Deep Learning Neural Networks
    | Scientifi… https://t.co/K6fL4YE8eX
    ...
    classic,Amazing what neural networks can do.
    // Computational Protein Design with Deep Learning Neural Networks
    | Scientifi… https://t.co/K6fL4YE8eX
    language model dirichlet,Amazing what neural networks can do.
    // Computational Protein Design with Deep Learning Neural Networks
    | Scientifi… https://t.co/K6fL4YE8eX
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>267
    bm25,Amazing what neural networks can do.
    // Computational Protein Design with Deep Learning Neural Networks
    | Scientifi… https://t.co/K6fL4YE8eX
    average wv ranking,The Connection Between Text Mining and Deep Learning
    https://t.co/kZpCVNvdrW
    document embedding ranking,All-optical machine learning using diffractive
    deep neural networks: https://t.co/GAwpSlqHd0
    ...
    We have used common Lucene similarities like classic (vector space model), bm25 and
    language model dirichlet. Also we have used the similarities we have defined in chapter
    5 and 6 using average word vectors and paragraph vectors respectively. One good thing
    we can observe is that the non neural ranking models didn’t change their top most search
    result, while the ones relying on embeddings both better adapted to the new data
    ingested.
    This kind of applications can be very useful both to keep up with high loads of data to be
    indexed into the search engine, evaluate best models, but also to carefully orchestrate
    how neural networks and search engine can work together.
    9.4 Looking forward
    We started this book by wondering whether we could use deep neural networks as a
    smart assistant to help us provide better search tools. Along the flow of the chapters we
    have touched several different areas of common search engines where deep learning has
    some good potential to help our users find what they’re looking for.
    Hopefully you got more and more interested about the topic as we’ve touched slightly
    more complex aspects and algorithms while advancing through the chapters. The hope is
    that this book has given you some tools and practical advices to be used immediately
    while also inspiring you about what can be done even better, what remains unsolved and
    you want to dive in. While writing this book a lot of new deep learning papers were
    published, some even related to search. New activation functions have been found out to
    be useful, new models have been proposed with promising results. You’re encouraged
    not to stop here and keep thinking about what you and your users need and how to get
    there with creativity.
    Speaking of deep learning applied to information retrieval we’ve just started surfacing
    this area. We wanted you to learn about the foundations of neural search and now be
    ready to learn and do more by yourself.
    Have fun!
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search
    Licensed to Myriam Pauillac <myriam.pauillac@anaphore.eu>268
    9.5 Summary
    training deep learning models is not always straightforward; tuning and adjustments for
    real life scenarios are often needed
    search engines and neural networks are often two different systems which interact both at
    indexing and search time. It is essential to monitor their performance in order to keep the
    overall user experience good in terms of response times
    real life deployments, like the common streaming scenario, need to account for load,
    concurrency and quality evaluation for the best possible search solution to be used
    ©Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and
    other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.
    https://forums.manning.com/forums/deep-learning-for-search"

}